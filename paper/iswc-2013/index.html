<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML+RDFa 1.0//EN' 
	'http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd'>
<html version='XHTML+RDFa 1.0'>
<head>
    <title>ISWC2013 Proceeding Papers</title>
<style type="text/css">
.on { 
  background: green;
  cursor: pointer;
  border: 2px solid black;
  border-right-width: 15px;
  padding: 0 5px;
  border-radius: 5px; 	
}

.off {
  background: red;
  cursor: pointer;
  border: 2px solid black;
  border-right-width: 2px;
  border-left-width: 15px;
  padding: 0 5px;
  border-radius: 5px; 	
}
</style>    
<script type="text/javascript">
<!--
	function togglestyle(el){
	    if(el.className == "on") {
	    	el.className="off";
	    } else {
	    	el.className="on";
	    }
	}
	function toggle_visibility(cl){
	   var els = document.getElementsByClassName(cl);
	   for(var i=0; i<els.length; ++i){
	      var s = els[i].style;
	      s.display = s.display==='none' ? 'block' : 'none';
	   };
	}
//-->
</script>    
</head>
<body>


<h2>ISWC2013 Proceeding Papers</h2>
<input type="button" class="off" onclick="togglestyle(this); toggle_visibility('abstract');">show/hide abstracts</input>

<div style="margin:10px;padding-left:10px">
Link to official proceedings at Springer
<div style="padding-left:5px">
<a href="http://www.springer.com/computer/ai/book/978-3-642-41334-6">12th International Semantic Web Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part I</a></div>
<div style="padding-left:5px">
<a href="http://www.springer.com/computer/ai/book/978-3-642-41337-7">12th International Semantic Web Conference, Sydney, NSW, Australia, October 21-25, 2013, Proceedings, Part II</a>
</div>
</div>

<div class="content" 
	xmlns:dcterms='http://purl.org/dc/terms/' 
	xmlns:rdfs='http://www.w3.org/2000/01/rdf-schema#' 
	xmlns:foaf='http://xmlns.com/foaf/0.1/' 
	xmlns:swrc='http://swrc.ontoware.org/ontology#' 
>

<h3 >Research Track Paper</h3>

<div class="paper">
<ul>
<li resource="#TRM_%E2%80%93_Learning_Dependencies_between_Text_and_Structure_with_Topical_Relational_Models" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180001-trm-learning-dependencies-between-text-and-structure-with-topical-relational-models.pdf"><span class="title" property="rdfs:label">TRM – Learning Dependencies between Text and Structure with Topical Relational Models</span></a>,
<span class="authors" property="swrc:listAuthor">Veli Bicer, Thanh Tran, Yongtao Ma</span>,
<span property="swrc:pages">1-16</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Text-rich structured data become more and more ubiquitous on the Web and on the enterprise databases by encoding heterogeneous structural information between entities such as people, locations, or organizations and the associated textual information. For analyzing this type of data, existing topic modeling approaches, which are highly tailored toward document collections, require manually-deﬁned regularization terms to exploit and to bias the topic learning towards structure information. We propose an approach, called Topical Relational Model, as a principled approach for automatically learning topics from both textual and structure information. Using a topic model, we can show that our approach is eﬀective in exploiting heterogeneous structure information, outperforming a state-of-the-art approach that requires manually-tuned regularization.</div>


</li>
<li resource="#A_Confidentiality_Model_for_Ontologies" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180017-a-confidentiality-model-for-ontologies.pdf"><span class="title" property="rdfs:label">A Confidentiality Model for Ontologies</span></a>,
<span class="authors" property="swrc:listAuthor">Piero Bonatti, Luigi Sauro</span>,
<span property="swrc:pages">17-32</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We illustrate several novel attacks to the conﬁdentiality of knowledge bases (KB). Then we introduce a new conﬁdentiality model, sensitive enough to detect those attacks, and a method for constructing secure KB views. We identify safe approximations of the background knowledge exploited in the attacks; they can be used to reduce the complexity of constructing secure KB views.</div>


</li>
<li resource="#Pattern_Based_Knowledge_Base_Enrichment" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180033-pattern-based-knowledge-base-enrichment.pdf"><span class="title" property="rdfs:label">Pattern Based Knowledge Base Enrichment</span></a>,
<span class="authors" property="swrc:listAuthor">Lorenz Bühmann, Jens Lehmann</span>,
<span property="swrc:pages">33-48</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Although an increasing number of RDF knowledge bases are published, many of those consist primarily of instance data and lack sophisticated schemata. Having such schemata allows more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the eﬀort required to create them. In this article, we propose a semi-automatic schemata construction approach addressing this problem: First, the frequency of axiom patterns in existing knowledge bases is discovered. Afterwards, those patterns are converted to SPARQL based pattern detection algorithms, which allow to enrich knowledge base schemata. We argue that we present the ﬁrst scalable knowledge base enrichment approach based on real schema usage patterns. The approach is evaluated on a large set of knowledge bases with a quantitative and qualitative result analysis.</div>


</li>
<li resource="#Controlled_Query_Evaluation_over_OWL_2_RL_Ontologies" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180049-controlled-query-evaluation-over-owl-2-rl-ontologies.pdf"><span class="title" property="rdfs:label">Controlled Query Evaluation over OWL 2 RL Ontologies</span></a>,
<span class="authors" property="swrc:listAuthor">Bernardo Cuenca Grau, Evgeny Kharlamov, Egor V. Kostylev, Dmitriy Zheleznyakov</span>,
<span property="swrc:pages">49-64</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We study conﬁdentiality enforcement in ontology-based information systems where ontologies are expressed in OWL 2 RL, a proﬁle of OWL 2 that is becoming increasingly popular in Semantic Web applications. We formalise a natural adaptation of the Controlled Query Evaluation (CQE) framework to ontologies. Our goal is to provide CQE algorithms that (i) ensure conﬁdentiality of sensitive information; (ii) are eﬃciently implementable by means of RDF triple store technologies; and (iii) ensure maximality of the answers returned by the system to user queries (thus restricting access to information as little as possible). We formally show that these requirements are in conﬂict and cannot be satisﬁed without imposing restrictions on ontologies. We propose a fragment of OWL 2 RL for which all three requirements can be satisﬁed. For the identiﬁed fragment, we design a CQE algorithm that has the same computational complexity as standard query answering and can be implemented by relying on state-of-the-art triple stores.</div>


</li>
<li resource="#Completeness_Statements_about_RDF_Data_Sources_and_Their_Use_for_Query_Answering" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180065-completeness-statements-about-rdf-data-sources-and-their-use-for-query-answering.pdf"><span class="title" property="rdfs:label">Completeness Statements about RDF Data Sources and Their Use for Query Answering</span></a>,
<span class="authors" property="swrc:listAuthor">Fariz Darari, Werner Nutt, Giuseppe Pirrò, Simon Razniewski</span>,
<span property="swrc:pages">65-80</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">With thousands of RDF data sources available on the Web covering disparate and possibly overlapping knowledge domains, the problem of providing high-level descriptions (in the form of metadata) of their content becomes crucial. In this paper we introduce a theoretical framework for describing data sources in terms of their completeness. We show how existing data sources can be described with completeness statements expressed in RDF. We then focus on the problem of the completeness of query answering over plain and RDFS data sources augmented with completeness statements. Finally, we present an extension of the completeness framework for federated data sources.</div>


</li>
<li resource="#Empirical_Study_of_Logic-Based_Modules%3A_Cheap_Is_Cheerful" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180081-empirical-study-of-logic-based-modules-cheap-is-cheerful.pdf"><span class="title" property="rdfs:label">Empirical Study of Logic-Based Modules: Cheap Is Cheerful</span></a>,
<span class="authors" property="swrc:listAuthor">Chiara Del Vescovo, Pavel Klinov, Bijan Parsia, Ulrike Sattler, Thomas Schneider, Dmitry Tsarkov</span>,
<span property="swrc:pages">81-96</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">For ontology reuse and integration, a number of approaches have been devised that aim at identifying modules, i.e., suitably small sets of “relevant” axioms from ontologies. Here we consider three logically sound notions of modules: MEX modules, only applicable to inexpressive ontologies; modules based on semantic locality, a sound approximation of the ﬁrst; and modules based on syntactic locality, a sound approximation of the second (and thus the ﬁrst), widely used since these modules can be extracted from OWL DL ontologies in time polynomial in the size of the ontology. In this paper we investigate the quality of both approximations over a large corpus of ontologies, using our own implementation of semantic locality, which is the ﬁrst to our knowledge. In particular, we show with statistical signiﬁcance that, in most cases, there is no diﬀerence between the two module notions based on locality; where they diﬀer, the additional axioms can either be easily ruled out or their number is relatively small. We classify the axioms that explain the rare diﬀerences into four kinds of “culprits” and discuss which of those can be avoided by extending the deﬁnition of syntactic locality. Finally, we show that diﬀerences between MEX and locality-based modules occur for a minority of ontologies from our corpus and largely aﬀect (approximations of ) expressive ontologies – this conclusion relies on a much larger and more diverse sample than existing comparisons between MEX and syntactic locality-based modules.</div>


</li>
<li resource="#The_Logic_of_Extensional_RDFS" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180097-the-logic-of-extensional-rdfs.pdf"><span class="title" property="rdfs:label">The Logic of Extensional RDFS</span></a>,
<span class="authors" property="swrc:listAuthor">Enrico Franconi, Claudio Gutierrez, Alessandro Mosca, Giuseppe Pirrò, Riccardo Rosati</span>,
<span property="swrc:pages">97-112</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The normative version of RDF Schema (RDFS) gives non-standard (intensional) interpretations to some standard notions such as classes and properties, thus departing from standard set-based semantics. In this paper we develop a standard set-based (extensional) semantics for the RDFS vocabulary while preserving the simplicity and computational complexity of deduction of the intensional version. This result can positively impact current implementations, as reasoning in RDFS can be implemented following common set-based intuitions and be compatible with OWL extensions.</div>


</li>
<li resource="#Indented_Tree_or_Graph%3F_A_Usability_Study_of_Ontology_Visualization_Techniques_in_the_Context_of_Class_Mapping_Evaluation" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180113-indented-tree-or-graph-a-usability-study-of-ontology-visualization-techniques-in-the-context-of-class-mapping-evaluation.pdf"><span class="title" property="rdfs:label">Indented Tree or Graph? A Usability Study of Ontology Visualization Techniques in the Context of Class Mapping Evaluation</span></a>,
<span class="authors" property="swrc:listAuthor">Bo Fu, Natalya F. Noy, Margaret-Anne Storey</span>,
<span property="swrc:pages">113-128</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Research  effort  in  ontology  visualization  has  largely  focused  on  developing new visualization  techniques. At  the  same  time,  researchers have paid  less  attention  to  investigating  the  usability  of  common  visualization  techniques  that many practitioners regularly use to visualize ontological data. In this paper,  we  focus  on  two  popular  ontology  visualization  techniques:  indented  tree  and  graph. We  conduct  a  controlled  usability  study  with  an  emphasis  on  the  effectiveness,  efficiency, workload  and  satisfaction of  these visualization  techniques  in  the  context  of  assisting  users  during  evaluation  of  ontology mappings.  Findings  from  this  study have  revealed both  strengths and weaknesses of each visualization  technique.  In  particular,  while  the  indented  tree  visualization  is  more  organized and familiar to novice users, subjects found the graph visualization to  be  more  controllable  and  intuitive  without  visual  redundancy,  particularly  for  ontologies with multiple inheritance.</div>


</li>
<li resource="#Real-time_RDF_extraction_from_unstructured_data_streams" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180129-real-time-rdf-extraction-from-unstructured-data-streams.pdf"><span class="title" property="rdfs:label">Real-time RDF extraction from unstructured data streams</span></a>,
<span class="authors" property="swrc:listAuthor">Daniel Gerber, Sebastian Hellmann, Lorenz Bühmann, Tommaso Soru, Axel-Cyrille Ngonga Ngomo, Ricardo Usbeck</span>,
<span property="swrc:pages">129-144</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The vision behind the Web of Data is to extend the current document-oriented Web with machine-readable facts and structured data, thus creating a representation of general knowledge. However, most of the Web of Data is limited to being a large compendium of encyclopedic knowledge describing entities. A huge challenge, the timely and massive extraction of RDF facts from unstructured data, has remained open so far. The availability of such knowledge on the Web of Data would provide signiﬁcant beneﬁts to manifold applications including news retrieval, sentiment analysis and business intelligence. In this paper, we address the problem of the actuality of the Web of Data by presenting an approach that allows extracting RDF triples from unstructured data streams. We employ statistical methods in combination with deduplication, disambiguation and unsupervised as well as supervised machine learning techniques to create a knowledge base that reﬂects the content of the input streams. We evaluate a sample of the RDF we generate against a large corpus of news streams and show that we achieve a precision of more than 85%.</div>


</li>
<li resource="#One_License_to_Compose_Them_All%3A_a_deontic_logic_approach_to_data_licensing_on_the_Web_of_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180145-one-license-to-compose-them-all-a-deontic-logic-approach-to-data-licensing-on-the-web-of-data.pdf"><span class="title" property="rdfs:label">One License to Compose Them All: a deontic logic approach to data licensing on the Web of Data</span></a>,
<span class="authors" property="swrc:listAuthor">Guido Governatori, Antonino Rotolo, Serena Villata, Fabien Gandon</span>,
<span property="swrc:pages">145-160</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">In the domain of Linked Open Data a need is emerging for developing automated frameworks able to generate the licensing terms associated to data coming from heterogeneous distributed sources. This paper proposes and evaluates a deontic logic semantics which allows us to deﬁne the deontic components of the licenses, i.e., permissions, obligations, and prohibitions, and generate a composite license compliant with the licensing items of the composed different licenses. Some heuristics are proposed to support the data publisher in choosing the licenses composition strategy which better suits her needs w.r.t. the data she is publishing.</div>


</li>
<li resource="#Federated_Entity_Search_using_On-The-Fly_Consolidation" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180161-federated-entity-search-using-on-the-fly-consolidation.pdf"><span class="title" property="rdfs:label">Federated Entity Search using On-The-Fly Consolidation</span></a>,
<span class="authors" property="swrc:listAuthor">Daniel M. Herzig, Roi Blanco, Peter Mika, Thanh Tran</span>,
<span property="swrc:pages">161-176</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Nowadays, search on the Web goes beyond the retrieval of textual Web sites and increasingly takes advantage of the growing amount of structured data. Of particular interest is entity search, where the units of retrieval are structured entities instead of textual documents. These entities reside in diﬀerent sources, which may provide only limited information about their content and are therefore called “uncooperative”. Further, these sources capture complementary but also redundant information about entities. In this environment of uncooperative data sources, we study the problem of federated entity search, where redundant information about entities is reduced on-the-ﬂy through entity consolidation performed at query time. We propose a novel method for entity consolidation that is based on using language models and completely unsupervised, hence more suitable for this on-the-ﬂy uncooperative setting than state-of-the-art methods that require training data. Further, we apply the same language model technique to deal with the federated search problem of ranking results returned from diﬀerent sources. Particular novel are the mechanisms we propose to incorporate consolidation results into this ranking. We perform experiments using real Web queries and data sources. Our experiments show that our approach for federated entity search with on-the-ﬂy consolidation improves upon the performance of a state-of-the-art preference aggregation baseline and also beneﬁts from consolidation.</div>


</li>
<li resource="#ProSWIP%3A_Property-based_Data_Access_for_Semantic_Web_Interactive_Programming" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180177-proswip-property-based-data-access-for-semantic-web-interactive-programming.pdf"><span class="title" property="rdfs:label">ProSWIP: Property-based Data Access for Semantic Web Interactive Programming</span></a>,
<span class="authors" property="swrc:listAuthor">Silviu Homoceanu, Philipp Wille, Wolf-Tilo Balke</span>,
<span property="swrc:pages">177-192</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The Semantic Web has matured from a mere theoretical vision to a variety of ready-to-use linked open data sources currently available on the Web. Still, with respect to application development , the Web community is just starting to develop new paradigms in which data as the main driver of applications is promoted to first class status. Relying on properties of resources as an indicator for the type, property-based typing is such a paradigm. In this paper, we inspect the feasibility of property-based typing for accessing data from the linked open data cloud. Problems in terms of transparency and quality of the selected data were noticeable. To alleviate these problems, we developed an iterative approach that builds on human feedback.</div>


</li>
<li resource="#Simplified_OWL_Ontology_Editing_for_the_Web%3A_Is_WebProt%C3%A9g%C3%A9_Enough%3F" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180193-simplified-owl-ontology-editing-for-the-web-is-webprot-g-enough-.pdf"><span class="title" property="rdfs:label">Simplified OWL Ontology Editing for the Web: Is WebProtégé Enough?</span></a>,
<span class="authors" property="swrc:listAuthor">Matthew Horridge, Tania Tudorache, Jennifer Vendetti, Csongor Nyulas, Mark Musen, Natasha F. Noy</span>,
<span property="swrc:pages">193-208</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Ontology engineering is a task that is notorious for its diﬃculty. As the group that developed Protégé, the most widely used ontology editor, we are keenly aware of how diﬃcult the users perceive this task to be. In this paper, we present the new version of WebProtégé that we designed with two main goals in mind: (1) create a tool that will be easy to use while still accounting for commonly used OWL constructs; (2) support collaboration and social interaction around distributed ontology editing as part of the core tool design. We designed this new version of the WebProtégé user interface empirically, by analysing the use of OWL constructs in a large corpus of publicly available ontologies. Since the beta release of this new WebProtégé interface in January 2013, our users from around the world have created and uploaded 519 ontologies on our server. In this paper, we describe the key features of the new tool and our empirical design approach. We evaluate language coverage in WebProtégé by assessing how well it covers the OWL constructs that are present in ontologies that users have uploaded to WebProtégé. We evaluate the usability of WebProtégé through a usability survey. Our analysis validates our empirical design, suggests additional language constructors to explore, and demonstrates that an easy-to-use web-based tool that covers most of the frequently used OWL constructs is suﬃcient for many users to start editing their ontologies.</div>


</li>
<li resource="#A_Query_Tool_for_EL_with_Non-monotonic_rules" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180209-a-query-tool-for-el-with-non-monotonic-rules.pdf"><span class="title" property="rdfs:label">A Query Tool for EL with Non-monotonic rules</span></a>,
<span class="authors" property="swrc:listAuthor">Vadim Ivanov, Matthias Knorr, Joao Leite</span>,
<span property="swrc:pages">209-224</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We present the Protégé plug-in NoHR that allows the user to take an E L+⊥ ontology, add a set of non-monotonic (logic programming) rules – suitable e.g. to express defaults and exceptions – and query the combined knowledge base. Our approach uses the well-founded semantics for MKNF knowledge bases as underlying formalism, so no restriction other than DL-safety is imposed on the rules that can be written. The tool itself builds on the procedure SLG(O) and, with the help of OWL 2 EL reasoner ELK, pre-processes the ontology into rules, whose result together with the non-monotonic rules serve as input for the topdown querying engine XSB Prolog. With the resulting plug-in, even queries to very large ontologies, such as SNOMED CT, augmented with a large number of rules, can be processed at an interactive response time after one initial brief pre-processing period. At the same time, our system is able to deal with possible inconsistencies between the rules and an ontology that alone is consistent.</div>


</li>
<li resource="#Incremental_Reasoning_in_OWL_EL_without_Bookkeeping" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180225-incremental-reasoning-in-owl-el-without-bookkeeping.pdf"><span class="title" property="rdfs:label">Incremental Reasoning in OWL EL without Bookkeeping</span></a>,
<span class="authors" property="swrc:listAuthor">Yevgeny Kazakov, Pavel Klinov</span>,
<span property="swrc:pages">225-240</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We describe a method for updating the classiﬁcation of ontologies expressed in the E L family of Description Logics after some axioms have been added or deleted. While incremental classiﬁcation modulo additions is relatively straightforward, handling deletions is more problematic since it requires retracting logical consequences that are no longer valid. Known algorithms address this problem using various forms of bookkeeping to trace the consequences back to premises. But such additional data can consume memory and place an extra burden on the reasoner during application of inferences. In this paper, we present a technique, which avoids this extra cost while being very efﬁcient for small incremental changes in ontologies. The technique is freely available as a part of the open-source E L reasoner ELK and its efﬁciency is demonstrated on naturally occurring and synthetic data.</div>


</li>
<li resource="#Secure_Manipulation_of_Linked_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180241-secure-manipulation-of-linked-data.pdf"><span class="title" property="rdfs:label">Secure Manipulation of Linked Data</span></a>,
<span class="authors" property="swrc:listAuthor">Sabrina Kirrane, Ahmed Abdelrahman, Alessandra Mileo, Stefan Decker</span>,
<span property="swrc:pages">241-256</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">When it comes to publishing data on the web, the level of access control required (if any) is highly dependent on the type of content exposed. Up until now RDF data publishers have focused on exposing and linking public data. With the advent of SPARQL 1.1, the linked data infrastructure can be used, not only as a means of publishing open data but also, as a general mechanism for managing distributed graph data. However, such a decentralised architecture brings with it a number of additional challenges with respect to both data security and integrity. In this paper, we propose a general authorisation framework that can be used to deliver dynamic query results based on user credentials and to cater for the secure manipulation of linked data. Speciﬁcally we describe how graph patterns, propagation rules, conﬂict resolution policies and integrity constraints can together be used to specify and enforce consistent access control policies.</div>


</li>
<li resource="#A_decision_procedure_for_SHOIQ_with_transitive_closure_of_roles" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180257-a-decision-procedure-for-shoiq-with-transitive-closure-of-roles.pdf"><span class="title" property="rdfs:label">A decision procedure for SHOIQ with transitive closure of roles</span></a>,
<span class="authors" property="swrc:listAuthor">Chan Le Duc, Myriam Lamolle, Olivier Curé</span>,
<span property="swrc:pages">257-272</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The Semantic Web makes an extensive use of the OWL DL ontology language, underlied by the SHOIQ description logic, to formalize its resources. In this paper, we propose a decision procedure for this logic extended with the transitive closure of roles in concept axioms, a feature needed in several application domains. The most challenging issue we have to deal with when designing such a decision procedure is to represent inﬁnitely non-tree-shaped models, which are different from those of SHOIQ ontologies. To address this issue, we introduce a new blocking condition for characterizing models which may have an inﬁnite non-tree-shaped part.</div>


</li>
<li resource="#Elastic_and_scalable_processing_of_Linked_Stream_Data_in_the_Cloud" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180273-elastic-and-scalable-processing-of-linked-stream-data-in-the-cloud.pdf"><span class="title" property="rdfs:label">Elastic and scalable processing of Linked Stream Data in the Cloud</span></a>,
<span class="authors" property="swrc:listAuthor">Danh Le Phuoc, Hoan Nguyen Mau Quoc, Chan Le Van, Manfred Hauswirth</span>,
<span property="swrc:pages">273-288</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Linked Stream Data extends the Linked Data paradigm to dynamic data sources. It enables the integration and joint processing of heterogeneous stream data with quasi-static data from the Linked Data Cloud in near-real-time. Several Linked Stream Data processing engines exist but their scalability still needs to be in improved in terms of (static and dynamic) data sizes, number of concurrent queries, stream update frequencies, etc. So far, none of them supports parallel processing in the Cloud, i.e., elastic load proﬁles in a hosted environment. To remedy these limitations, this paper presents an approach for elastically parallelizing the continuous execution of queries over Linked Stream Data. For this, we have developed novel, highly eﬃcient, and scalable parallel algorithms for continuous query operators. Our approach and algorithms are implemented in our CQELS Cloud system and we present extensive evaluations of their superior performance on Amazon EC2 demonstrating their high scalability and excellent elasticity in a real deployment.</div>


</li>
<li resource="#Towards_Constructive_Evidence_of_Data_Flow-oriented_Web_Service_Composition" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180289-towards-constructive-evidence-of-data-flow-oriented-web-service-composition.pdf"><span class="title" property="rdfs:label">Towards Constructive Evidence of Data Flow-oriented Web Service Composition</span></a>,
<span class="authors" property="swrc:listAuthor">Freddy Lecue</span>,
<span property="swrc:pages">289-304</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Automation of service composition is one of the most interesting challenges facing the Semantic Web and the Web of services today. Despite approaches which are able to infer a partial order of services, its data ﬂow remains implicit and difﬁcult to be automatically generated. Enhanced with formal representations, the semantic links between output and input parameters of services can be then exploited to infer their data ﬂow. This work addresses the problem of effectively inferring data ﬂow between services based on their representations. To this end, we introduce the non standard Description Logic reasoning join, aiming to provide a “constructive evidence” of why services can be connected and how non trivial links (many to many parameters) can be inferred in data ﬂow. The preliminary evaluation provides evidence in favor of our approach regarding the completeness of data ﬂow.</div>


</li>
<li resource="#The_Combined_Approach_to_OBDA%3A_Taming_Role_Hierarchies_using_Filters" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180305-the-combined-approach-to-obda-taming-role-hierarchies-using-filters.pdf"><span class="title" property="rdfs:label">The Combined Approach to OBDA: Taming Role Hierarchies using Filters</span></a>,
<span class="authors" property="swrc:listAuthor">Carsten Lutz, Inanc Seylan, David Toman, Frank Wolter</span>,
<span property="swrc:pages">305-320</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The basic idea of the combined approach to query answering in the presence of ontologies is to materialize the consequences of the ontology in the data and then use a limited form of query rewriting to deal with inﬁnite materializations. While this approach is eﬃcient and scalable for ontologies that are formulated in the basic version of the description logic DL-Lite, it incurs an exponential blowup during query rewriting when DL-Lite is extended with the popular role hierarchies. In this paper, we show how to replace the query rewriting with a ﬁltering technique. This is natural from an implementation perspective and allows us to handle role hierarchies without an exponential blowup. We also carry out an experimental evaluation that demonstrates the scalability of this approach.</div>


</li>
<li resource="#A_snapshot_of_the_OWL_Web" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180321-a-snapshot-of-the-owl-web.pdf"><span class="title" property="rdfs:label">A snapshot of the OWL Web</span></a>,
<span class="authors" property="swrc:listAuthor">Nicolas Matentzoglu, Samantha Bail, Bijan Parsia</span>,
<span property="swrc:pages">321-336</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Tool development for and empirical experimentation in OWL ontology engineering require a wide variety of suitable ontologies as input for testing and evaluation purposes and detailed characterisations of real ontologies. Empirical activities often resort to (somewhat arbitrarily) hand curated corpora available on the web, such as the NCBO BioPortal and the TONES Repository, or manually selected sets of well-known ontologies. Findings of surveys and results of benchmarking activities may be biased, even heavily, towards these datasets. Sampling from a large corpus of ontologies, on the other hand, may lead to more representative results. Current large scale repositories and web crawls are mostly uncurated and suﬀer from duplication, small and (for many purposes) uninteresting ontology ﬁles, and contain large numbers of ontology versions, variants, and facets, and therefore do not lend themselves to random sampling. In this paper, we survey ontologies as they exist on the web and describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling, various forms of de-duplications and manual cleaning, which allows random sampling of ontologies for a variety of empirical applications.</div>


</li>
<li resource="#Semantic_Rule_Filtering_for_Web-Scale_Relation_Extraction" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180337-semantic-rule-filtering-for-web-scale-relation-extraction.pdf"><span class="title" property="rdfs:label">Semantic Rule Filtering for Web-Scale Relation Extraction</span></a>,
<span class="authors" property="swrc:listAuthor">Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu, Roberto Navigli, Hans Uszkoreit</span>,
<span property="swrc:pages">337-352</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Web-scale relation extraction is a means for building and extending large repositories of formalized knowledge. This type of automated knowledge building requires a decent level of precision, which is hard to achieve with automatically acquired rule sets learned from unlabeled data by means of distant or minimal supervision. This paper shows how precision of relation extraction can be considerably improved by employing a wide-coverage, general-purpose lexical semantic network, i.e., BabelNet, for effective semantic rule ﬁltering. We apply Word Sense Disambiguation to the content words of the automatically extracted rules. As a result a set of relation-speciﬁc relevant concepts is obtained, and each of these concepts is then used to represent the structured semantics of the corresponding relation. The resulting relation-speciﬁc subgraphs of BabelNet are used as semantic ﬁlters for estimating the adequacy of the extracted rules. For the seven semantic relations tested here, the semantic ﬁlter consistently yields a higher precision at any relative recall value in the high-recall range.</div>


</li>
<li resource="#Semantic_Message_Passing_for_Generating_Linked_Data_from_Tables" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180353-semantic-message-passing-for-generating-linked-data-from-tables.pdf"><span class="title" property="rdfs:label">Semantic Message Passing for Generating Linked Data from Tables</span></a>,
<span class="authors" property="swrc:listAuthor">Varish Mulwad, Tim Finin, Anupam Joshi</span>,
<span property="swrc:pages">353-368</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We describe work on automatically inferring the intended meaning of tables and representing it as RDF linked data, making it available for improving search, interoperability and integration. We present implementation details of a joint inference module that uses knowledge from the linked open data (LOD) cloud to jointly infer the semantics of column headers, table cell values (e.g., strings and numbers) and relations between columns. We also implement a novel Semantic Message Passing algorithm which uses LOD knowledge to improve existing message passing schemes. We evaluate our implemented techniques on tables from the Web and Wikipedia.</div>


</li>
<li resource="#Bringing_Math_to_LOD%3A_A_Semantic_Publishing_Platform_Prototype_for_Scientific_Collections_in_Mathematics" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180369-bringing-math-to-lod-a-semantic-publishing-platform-prototype-for-scientific-collections-in-mathematics.pdf"><span class="title" property="rdfs:label">Bringing Math to LOD: A Semantic Publishing Platform Prototype for Scientific Collections in Mathematics</span></a>,
<span class="authors" property="swrc:listAuthor">Olga Nevzorova, Nikita Zhiltsov, Danila Zaikin, Olga Zhibrik, Alexander Kirillovich, Vladimir Nevzorov, Evgeniy Birialtsev</span>,
<span property="swrc:pages">369-384</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We present our work on developing a software platform for mining mathematical scholarly papers to obtain a Linked Data representation. Currently, the Linking Open Data (LOD) cloud lacks up-to-date and detailed information on professional level mathematics. To our mind, the main reason for that is the absence of appropriate tools that could analyze the underlying semantics in mathematical papers and eﬀectively build their consolidated representation. We have developed a holistic approach to analysis of mathematical documents, including ontology based extraction, conversion of the article body as well as its metadata into RDF, integration with some existing LOD data sets, and semantic search. We argue that the platform may be helpful for enriching user experience on modern online scientiﬁc collections.</div>


</li>
<li resource="#ORCHID_%E2%80%93_Reduction-Ratio-Optimal_Computation_of_Geo-Spatial_Distances_for_Link_Discovery" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180385-orchid-reduction-ratio-optimal-computation-of-geo-spatial-distances-for-link-discovery.pdf"><span class="title" property="rdfs:label">ORCHID – Reduction-Ratio-Optimal Computation of Geo-Spatial Distances for Link Discovery</span></a>,
<span class="authors" property="swrc:listAuthor">Axel-Cyrille Ngonga Ngomo</span>,
<span property="swrc:pages">385-400</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The discovery of links between resources within knowledge bases is of crucial importance to realize the vision of the Semantic Web. Addressing this task is especially challenging when dealing with geo-spatial datasets due to their sheer size and the potential complexity of single geo-spatial objects. Yet, so far, little attention has been paid to the characteristics of geo-spatial data within the context of link discovery. In this paper, we address this gap by presenting Orchid, a reduction-ratio-optimal link discovery approach designed especially for geo-spatial data. Orchid relies on a combination of the Hausdorﬀ and orthodromic metrics to compute the distance between geo-spatial objects. We ﬁrst present two novel approaches for the eﬃcient computation of Hausdorﬀ distances. Then, we present the space tiling approach implemented by Orchid and prove that it is optimal with respect to the reduction ratio that it can achieve. The evaluation of our approaches is carried out on three real datasets of diﬀerent size and complexity. Our results suggest that our approaches to the computation of Hausdorﬀ distances require two orders of magnitude less orthodromic distances computations to compare geographical data. Moreover, they require two orders of magnitude less time than a naive approach to achieve this goal. Finally, our results indicate that Orchid scales to large datasets while outperforming the state of the art signiﬁcantly.</div>


</li>
<li resource="#Simplifying_Description_Logic_Ontologies" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180401-simplifying-description-logic-ontologies.pdf"><span class="title" property="rdfs:label">Simplifying Description Logic Ontologies</span></a>,
<span class="authors" property="swrc:listAuthor">Nadeschda Nikitina, Sven Schewe</span>,
<span property="swrc:pages">401-416</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We discuss the problem of minimizing TBoxes expressed in the light-weight description logic E L, which forms a basis of some large ontologies like SNOMED, Gene Ontology, NCI and Galen. We show that the minimization of TBoxes is intractable (NP-complete). While this looks like a bad news result, we also provide a heuristic technique for minimizing TBoxes. We prove the correctness of the heuristics and show that it provides optimal results for a class of ontologies, which we deﬁne through an acyclicity constraint over a reference relation between equivalence classes of concepts. To establish the feasibility of our approach, we have implemented the algorithm and evaluated its effectiveness on a small suite of benchmarks.</div>


</li>
<li resource="#FedSearch%3A_efficiently_combining_structured_queries_and_full-text_search_in_a_SPARQL_federation" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180417-fedsearch-efficiently-combining-structured-queries-and-full-text-search-in-a-sparql-federation.pdf"><span class="title" property="rdfs:label">FedSearch: efficiently combining structured queries and full-text search in a SPARQL federation</span></a>,
<span class="authors" property="swrc:listAuthor">Andriy Nikolov, Andreas Schwarte, Christian Hütter</span>,
<span property="swrc:pages">417-432</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Combining structured queries with full-text search provides a powerful means to access distributed linked data. However, executing hybrid search queries in a federation of multiple data sources presents a number of challenges due to data source heterogeneity and lack of statistical data about keyword selectivity. To address these challenges, we present FedSearch – a novel hybrid query engine based on the SPARQL federation framework FedX. We extend the SPARQL algebra to incorporate keyword search clauses as ﬁrst-class citizens and apply novel optimization techniques to improve the query processing eﬃciency while maintaining a meaningful ranking of results. By performing on-the-ﬂy adaptation of the query execution plan and intelligent grouping of query clauses, we are able to reduce signiﬁcantly the communication costs making our approach suitable for top-k hybrid search across multiple data sources. In experiments we demonstrate that our optimization techniques can lead to a substantial performance improvement, reducing the execution time of hybrid queries by more than an order of magnitude.</div>


</li>
<li resource="#Getting_Lucky_in_Ontology_Search%3A_A_Data-Driven_Evaluation_Framework_for_Ontology_Ranking" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180433-getting-lucky-in-ontology-search-a-data-driven-evaluation-framework-for-ontology-ranking.pdf"><span class="title" property="rdfs:label">Getting Lucky in Ontology Search: A Data-Driven Evaluation Framework for Ontology Ranking</span></a>,
<span class="authors" property="swrc:listAuthor">Natasha F. Noy, Paul Alexander, Rave Harpaz, Trish Whetzel, Raymond Fergerson, Mark Musen</span>,
<span property="swrc:pages">433-448</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">With hundreds, if not thousands, of ontologies available today in many diﬀerent domains, ontology search and ranking has become an important and timely problem. When a user searches a collection of ontologies for her terms of interest, there are often dozens of ontologies that contain these terms. How does she know which ontology is the most relevant to her search? Our research group hosts BioPortal, a public repository of more than 330 ontologies in the biomedical domain. When a term that a user searches for is available in multiple ontologies, how do we rank the results and how do we measure how well our ranking works? In this paper, we develop an evaluation framework that enables developers to compare and analyze the performance of diﬀerent ontology-ranking methods. Our framework is based on processing search logs and determining how often users select the top link that the search engine oﬀers. We evaluate our framework by analyzing the data on BioPortal searches. We explore several diﬀerent ranking algorithms and measure the eﬀectiveness of each ranking by measuring how often users click on the highest ranked ontology. We collected log data from more than 4,800 BioPortal searches. Our results show that regardless of the ranking, in more than half the searches, users select the ﬁrst link. Thus, it is even more critical to ensure that the ranking is appropriate if we want to have satisﬁed users. Our further analysis demonstrates that ranking ontologies based on page view data signiﬁcantly improves the user experience, with an approximately 26% increase in the number of users who select the highest ranked ontology for the search.</div>


</li>
<li resource="#Exploring_Scholarly_Data_with_Rexplore" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180449-exploring-scholarly-data-with-rexplore.pdf"><span class="title" property="rdfs:label">Exploring Scholarly Data with Rexplore</span></a>,
<span class="authors" property="swrc:listAuthor">Francesco Osborne, Enrico Motta, Paul Mulholland</span>,
<span property="swrc:pages">449-464</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Despite the large number and variety of tools and services available today for exploring scholarly data, current support is still very limited in the context of sensemaking tasks, which go beyond standard search and ranking of authors and publications, and focus instead on i) understanding the dynamics of research areas, ii) relating authors ‘semantically’ (e.g., in terms of common interests or shared academic trajectories), or iii) performing fine-grained academic expert search along multiple dimensions. To address this gap we have developed a novel tool, Rexplore, which integrates statistical analysis, semantic technologies, and visual analytics to provide effective support for exploring and making sense of scholarly data. Here, we describe the main innovative elements of the tool and we present the results from a task-centric empirical evaluation, which shows that Rexplore is highly effective at providing support for the aforementioned sensemaking tasks. In addition, these results are robust both with respect to the background of the users (i.e., expert analysts vs. ‘ordinary’ users) and also with respect to whether the tasks are selected by the evaluators or proposed by the users themselves.</div>


</li>
<li resource="#Personalized_Best_Answer_Computation_in_Graph_Databases" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180465-personalized-best-answer-computation-in-graph-databases.pdf"><span class="title" property="rdfs:label">Personalized Best Answer Computation in Graph Databases</span></a>,
<span class="authors" property="swrc:listAuthor">Michael Ovelgönne, Noseong Park, V.S. Subrahmanian, Elizabeth K. Bowman, Kirk A. Ogaard</span>,
<span property="swrc:pages">465-480</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Though subgraph matching has been extensively studied as a query paradigm in semantic web and social network data environments, a user can get a large number of answers in response to a query. Just like Google does, these answers can be shown to the user in accordance with an importance ranking. In this paper, we present scalable algorithms to ﬁnd the top-K answers to a practically important subset of SPARQL-queries, denoted as importance queries, via a suite of pruning techniques. We test our algorithms on multiple real-world graph data sets, showing that our algorithms are eﬃcient even on networks with up to 6M vertices and 15M edges and far more eﬃcient than popular triple stores.</div>


</li>
<li resource="#Towards_an_automatic_creation_of_localized_versions_of_DBpedia" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180481-towards-an-automatic-creation-of-localized-versions-of-dbpedia.pdf"><span class="title" property="rdfs:label">Towards an automatic creation of localized versions of DBpedia</span></a>,
<span class="authors" property="swrc:listAuthor">Alessio Palmero Aprosio, Claudio Giuliano, Alberto Lavelli</span>,
<span property="swrc:pages">481-496</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">DBpedia is a large-scale knowledge base that exploits Wikipedia as primary data source. The extraction procedure requires to manually map Wikipedia infoboxes into the DBpedia ontology. Thanks to crowdsourcing, a large number of infoboxes has been mapped in the English DBpedia. Consequently, the same procedure has been applied to other languages to create the localized versions of DBpedia. However, the number of accomplished mappings is still small and limited to most frequent infoboxes. Furthermore, mappings need maintenance due to the constant and quick changes of Wikipedia articles. In this paper, we focus on the problem of automatically mapping infobox attributes to properties into the DBpedia ontology for extending the coverage of the existing localized versions or building from scratch versions for languages not covered in the current version. The evaluation has been performed on the Italian mappings. We compared our results with the current mappings on a random sample re-annotated by the authors. We report results comparable to the ones obtained by a human annotator in term of precision, but our approach leads to a signiﬁcant improvement in recall and speed. Speciﬁcally, we mapped 45,978 Wikipedia infobox attributes to DBpedia properties in 14 different languages for which mappings were not yet available. The resource is made available in an open format.</div>


</li>
<li resource="#Type_Inference_on_Noisy_RDF_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180497-type-inference-on-noisy-rdf-data.pdf"><span class="title" property="rdfs:label">Type Inference on Noisy RDF Data</span></a>,
<span class="authors" property="swrc:listAuthor">Heiko Paulheim, Christian Bizer</span>,
<span property="swrc:pages">497-512</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Type information is very valuable in knowledge bases. However, most large open knowledge bases are incomplete with respect to type information, and, at the same time, contain noisy and incorrect data. That makes classic type inference by reasoning diﬃcult. In this paper, we propose the heuristic link-based type inference mechanism SD-Type, which can handle noisy and incorrect data. Instead of leveraging T-box information from the schema, SDType takes the actual use of a schema into account and thus is also robust to misused schema elements.</div>


</li>
<li resource="#What%27s_in_a_%27nym%27%3F_Synonyms_in_Biomedical_Ontology_Matching" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180513-what-s-in-a-nym-synonyms-in-biomedical-ontology-matching.pdf"><span class="title" property="rdfs:label">What's in a 'nym'? Synonyms in Biomedical Ontology Matching</span></a>,
<span class="authors" property="swrc:listAuthor">Catia Pesquita, Cosmin Stroe, Daniel Faria, Emanuel Santos, Isabel Cruz, Francisco Couto</span>,
<span property="swrc:pages">513-528</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">To bring the Life Sciences domain closer to a Semantic Web realization it is fundamental to establish meaningful relations between biomedical ontologies. The successful application of ontology matching techniques is strongly tied to an eﬀective exploration of the complex and diverse biomedical terminology contained in biomedical ontologies. In this paper, we present an overview of the lexical components of several biomedical ontologies and investigate how diﬀerent approaches for their use can impact the performance of ontology matching techniques. We propose novel approaches for exploring the diﬀerent types of synonyms encoded by the ontologies and for extending them based both on internal synonym derivation and on external ontologies. We evaluate our approaches using AgreementMaker, a successful ontology matching platform that implements several lexical matchers, and apply them to a set of four benchmark biomedical ontology matching tasks. Our results demonstrate the impact that an adequate consideration of ontology synonyms can have on matching performance, and validate our novel approach for combining internal and external synonym sources as a competitive and in many cases improved solution for biomedical ontology matching.</div>


</li>
<li resource="#Knowledge_Graph_Identification" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180529-knowledge-graph-identification.pdf"><span class="title" property="rdfs:label">Knowledge Graph Identification</span></a>,
<span class="authors" property="swrc:listAuthor">Jay Pujara, Hui Miao, Lise Getoor, William Cohen</span>,
<span property="swrc:pages">529-544</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Large-scale information processing systems are able to extract massive collections of interrelated facts, but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge. In this paper, we show how uncertain extractions about entities and their relations can be transformed into a know ledge graph. The extractions form an extraction graph and we refer to the task of removing noise, inferring missing information, and determining which candidate facts should be included into a knowledge graph as know ledge graph identiﬁcation. In order to perform this task, we must reason jointly about candidate facts and their associated extraction conﬁdences, identify coreferent entities, and incorporate ontological constraints. Our proposed approach uses probabilistic soft logic (PSL), a recently introduced probabilistic modeling framework which easily scales to millions of facts. We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL pro ject containing over 1M extractions and 70K ontological relations. We show that compared to existing methods, our approach is able to achieve improved AUC and F1 with signiﬁcantly lower running time.</div>


</li>
<li resource="#Ontology-Based_Data_Access%3A_Ontop_of_Databases" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180545-ontology-based-data-access-ontop-of-databases.pdf"><span class="title" property="rdfs:label">Ontology-Based Data Access: Ontop of Databases</span></a>,
<span class="authors" property="swrc:listAuthor">Mariano Rodriguez-Muro, Roman Kontchakov, Michael Zakharyaschev</span>,
<span property="swrc:pages">545-560</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We present the architecture and technologies underpinning the OBDA system Ontop and taking full advantage of storing data in relational databases. We discuss the theoretical foundations of Ontop : the tree-witness query rewriting, T -mappings and optimisations based on database integrity constraints and SQL features. We analyse the performance of Ontop in a series of experiments and demonstrate that, for standard ontologies, queries and data stored in relational databases, Ontop is fast, eﬃcient and produces SQL rewritings of high quality.</div>


</li>
<li resource="#DAW%3A_Duplicate-AWare_Federated_Query_Processing_over_the_Web_of_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180561-daw-duplicate-aware-federated-query-processing-over-the-web-of-data.pdf"><span class="title" property="rdfs:label">DAW: Duplicate-AWare Federated Query Processing over the Web of Data</span></a>,
<span class="authors" property="swrc:listAuthor">Muhammad Saleem, Axel-Cyrille Ngonga Ngomo, Josiane Xavier Parreira, Helena Deus, Manfred Hauswirth</span>,
<span property="swrc:pages">561-576</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Over the last years the Web of Data has developed into a large compendium of interlinked data sets from multiple domains. Due to the decentralised architecture of this compendium, several of these datasets contain duplicated data. Yet, so far, only little attention has been paid to the eﬀect of duplicated data on federated querying. This work presents DAW, a novel duplicate-aware approach to federated querying over the Web of Data. DAW is based on a combination of min-wise independent permutations and compact data summaries. It can be directly combined with existing federated query engines in order to achieve the same query recall values while querying fewer data sources. We extend three well-known federated query processing engines – DARQ, SPLENDID, and FedX – with DAW and compare our extensions with the original approaches. The comparison shows that DAW can greatly reduce the number of queries sent to the endpoints, while keeping high query recall values. Therefore, it can signiﬁcantly improve the performance of federated query processing engines. Moreover, DAW provides a source selection mechanism that maximises the query recall, when the query processing is limited to a subset of the sources.</div>


</li>
<li resource="#On_the_Status_of_Experimental_Research_on_the_Semantic_Web" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180577-on-the-status-of-experimental-research-on-the-semantic-web.pdf"><span class="title" property="rdfs:label">On the Status of Experimental Research on the Semantic Web</span></a>,
<span class="authors" property="swrc:listAuthor">Heiner Stuckenschmidt, Michael Schuhmacher, Johannes Knopp, Christian Meilicke, Ansgar Scherp</span>,
<span property="swrc:pages">577-592</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Experimentation is an important way to validate results of Semantic Web and Computer Science research in general. In this paper, we investigate the development and the current status of experimental work on the Semantic Web. Based on a corpus of 500 papers collected from the International Semantic Web Conferences (ISWC) over the past decade, we analyse the importance and the quality of experimental research conducted and compare it to general Computer Science. We observe that the amount and quality of experiments are steadily increasing over time. Unlike hypothesised, we cannot conﬁrm a statistically signiﬁcant correlation between a paper’s citations and the amount of experimental work reported. Our analysis, however, shows that papers comparing themselves to other systems are more often cited than other papers.</div>


</li>
<li resource="#A_Graph-Based_Approach_to_Learn_Semantic_Descriptions_of_Data_Sources" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180593-a-graph-based-approach-to-learn-semantic-descriptions-of-data-sources.pdf"><span class="title" property="rdfs:label">A Graph-Based Approach to Learn Semantic Descriptions of Data Sources</span></a>,
<span class="authors" property="swrc:listAuthor">Mohsen Taheriyan, Craig Knoblock, Pedro Szekely, José Luis Ambite</span>,
<span property="swrc:pages">593-608</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Semantic models of data sources and services provide support to automate many tasks such as source discovery, data integration, and service composition, but writing these semantic descriptions by hand is a tedious and time-consuming task. Most of the related work focuses on automatic annotation with classes or properties of source attributes or input and output parameters. However, constructing a source model that includes the relationships between the attributes in addition to their semantic types remains a largely unsolved problem. In this paper, we present a graph-based approach to hypothesize a rich semantic description of a new target source from a set of known sources that have been modeled over the same domain ontology. We exploit the domain ontology and the known source models to build a graph that represents the space of plausible source descriptions. Then, we compute the top k candidates and suggest to the user a ranked list of the semantic models for the new source. The approach takes into account user corrections to learn more accurate semantic descriptions of future data sources. Our evaluation shows that our method produces models that are twice as accurate than the models produced using a state of the art system that does not learn from prior models.</div>


</li>
<li resource="#QODI%3A_Query_as_Context_in_Automatic_Data_Integration" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180609-qodi-query-as-context-in-automatic-data-integration.pdf"><span class="title" property="rdfs:label">QODI: Query as Context in Automatic Data Integration</span></a>,
<span class="authors" property="swrc:listAuthor">Aibo Tian, Juan F. Sequeda, Daniel Miranker</span>,
<span property="swrc:pages">609-624</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">QODI is an automatic ontology-based data integration system (OBDI). QODI is distinguished in that the ontology mapping algorithm dynamically determines a partial mapping speciﬁc to the reformulation of each query. The query provides application context not available in the ontologies alone; thereby the system is able to disambiguate mappings for different queries. The mapping algorithm decomposes the query into a set of paths, and compares the set of paths with a similar decomposition of a source ontology. Using test sets from three real world applications, QODI achieves favorable results compared with AgreementMaker, a leading ontology matcher, and an ontology-based implementation of the mapping methods detailed for Clio, the state-of-the-art relational data integration and data exchange system.</div>


</li>
<li resource="#TRank%3A_Ranking_Entity_Types_Using_the_Web_of_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180625-trank-ranking-entity-types-using-the-web-of-data.pdf"><span class="title" property="rdfs:label">TRank: Ranking Entity Types Using the Web of Data</span></a>,
<span class="authors" property="swrc:listAuthor">Alberto Tonon, Michele Catasta, Gianluca Demartini, Philippe Cudré-Mauroux, Karl Aberer</span>,
<span property="swrc:pages">625-640</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Much of Web search and browsing activity is today centered around entities. For this reason, Search Engine Result Pages (SERPs) increasingly contain information about the searched entities such as pictures, short summaries, related entities, and factual information. A key facet that is often displayed on the SERPs and that is instrumental for many applications is the entity type. However, an entity is usually not associated to a single generic type in the background knowledge bases but rather to a set of more speciﬁc types, which may be relevant or not given the document context. For example, one can ﬁnd on the Linked Open Data cloud the fact that Tom Hanks is a person, an actor, and a person from Concord, California. All those types are correct but some may be too general to be interesting (e.g., person), while other may be interesting but already known to the user (e.g., actor), or may be irrelevant given the current browsing context (e.g., person from Concord, California). In this paper, we deﬁne the new task of ranking entity types given an entity and its context. We propose and evaluate new methods to ﬁnd the most relevant entity type based on collection statistics and on the graph structure interconnecting entities and types. An extensive experimental evaluation over several document collections at diﬀerent levels of granularity (e.g., sentences, paragraphs, etc.) and diﬀerent type hierarchies (including DBPedia, Freebase, and schema.org) shows that hierarchy-based approaches provide more accurate results when picking entity types to be displayed to the end-user while still being highly scalable.</div>


</li>
<li resource="#DynamiTE%3A_Parallel_Materialization_of_Dynamic_RDF_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180641-dynamite-parallel-materialization-of-dynamic-rdf-data.pdf"><span class="title" property="rdfs:label">DynamiTE: Parallel Materialization of Dynamic RDF Data</span></a>,
<span class="authors" property="swrc:listAuthor">Jacopo Urbani, Alessandro Margara, Ceriel Jacobs, Frank Van Harmelen, Henri Bal</span>,
<span property="swrc:pages">641-656</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">One of the main advantages of using semantically annotated data is that machines can reason on it, deriving implicit knowledge from explicit information. In this context, materializing every possible implicit derivation from a given input can be computationally expensive, especially when considering large data volumes. Most of the solutions that address this problem rely on the assumption that the information is static, i.e., that it does not change, or changes very infrequently. However, the Web is extremely dynamic: online newspapers, blogs, social networks, etc., are frequently changed so that outdated information is removed and replaced with fresh data. This demands for a materialization that is not only scalable, but also reactive to changes. In this paper, we consider the problem of incremental materialization, that is, how to update the materialized derivations when new data is added or removed. To this purpose, we consider the ρdf RDFS fragment [12], and present a parallel system that implements a number of algorithms to quickly recalculate the derivation. In case new data is added, our system uses a parallel version of the well-known semi-naive evaluation of Datalog. In case of removals, we have implemented two algorithms, one based on previous theoretical work, and another one that is more eﬃcient since it does not require a complete scan of the input. We have evaluated the performance using a prototype system called DynamiTE , which organizes the knowledge bases with a number of indices to facilitate the query process and exploits parallelism to improve the performance. The results show that our methods are indeed capable to recalculate the derivation in a short time, opening the door to reasoning on much more dynamic data than is currently possible.</div>


</li>
<li resource="#Discovering_Missing_Semantic_Relations_between_Entities_in_Wikipedia" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180657-discovering-missing-semantic-relations-between-entities-in-wikipedia.pdf"><span class="title" property="rdfs:label">Discovering Missing Semantic Relations between Entities in Wikipedia</span></a>,
<span class="authors" property="swrc:listAuthor">Mengling Xu, Zhichun Wang, Rongfang Bie, Juanzi Li, Chen Zheng, Wantian Ke, Mingquan Zhou</span>,
<span property="swrc:pages">657-670</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Wikipedia’s infoboxes contain rich structured information of various entities, which have been explored by the DBpedia pro ject to generate large scale Linked Data sets. Among all the infobox attributes, those attributes having hyperlinks in its values identify semantic relations between entities, which are important for creating RDF links between DBpedia’s instances. However, quite a few hyperlinks have not been anotated by editors in infoboxes, which causes lots of relations between entities being missing in Wikipedia. In this paper, we propose an approach for automatically discovering the missing entity links in Wikipedia’s infoboxes, so that the missing semantic relations between entities can be established. Our approach ﬁrst identiﬁes entity mentions in the given infoboxes, and then computes several features to estimate the possibilities that a given attribute value might link to a candidate entity. A learning model is used to obtain the weights of diﬀerent features, and predict the destination entity for each attribute value. We evaluated our approach on the English Wikipedia data, the experimental results show that our approach can eﬀectively ﬁnd the missing relations between entities, and it signiﬁcantly outperforms the baseline methods in terms of both precision and recall.</div>


</li>
<li resource="#Infrastructure_for_Efficient_Exploration_of_Large_Scale_Linked_Data_via_Contextual_Tag_Clouds" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180671-infrastructure-for-efficient-exploration-of-large-scale-linked-data-via-contextual-tag-clouds.pdf"><span class="title" property="rdfs:label">Infrastructure for Efficient Exploration of Large Scale Linked Data via Contextual Tag Clouds</span></a>,
<span class="authors" property="swrc:listAuthor">Xingjian Zhang, Dezhao Song, Sambhawa Priya, Jeff Heflin</span>,
<span property="swrc:pages">671-686</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">In this paper we present the infrastructure of the contextual tag cloud system which can execute large volumes of queries about the number of instances that use particular ontological terms. The contextual tag cloud system is a novel application that helps users explore a large scale RDF dataset: the tags are ontological terms (classes and properties), the context is a set of tags that deﬁnes a subset of instances, and the font sizes reﬂect the number of instances that use each tag. It visualizes the patterns of instances speciﬁed by the context a user constructs. Given a request with a speciﬁc context, the system needs to quickly ﬁnd what other tags the instances in the context use, and how many instances in the context use each tag. The key question we answer in this paper is how to scale to Linked Data; in particular we use a dataset with 1.4 billion triples and over 380,000 tags. This is complicated by the fact that the calculation should, when directed by the user, consider the entailment of taxonomic and/or domain/range axioms in the ontology. We combine a scalable preprocessing approach with a specially-constructed inverted index and use three approaches to prune unnecessary counts for faster intersection computations. We compare our system with a state-of-the-art triple store, examine how pruning rules interact with inference and analyze our design choices.</div>


</li>
<li resource="#Statistical_Knowledge_Patterns%3A_Identifying_Synonymous_Relations_in_Large_Linked_Datasets" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180687-statistical-knowledge-patterns-identifying-synonymous-relations-in-large-linked-datasets.pdf"><span class="title" property="rdfs:label">Statistical Knowledge Patterns: Identifying Synonymous Relations in Large Linked Datasets</span></a>,
<span class="authors" property="swrc:listAuthor">Ziqi Zhang, Anna Lisa Gentile, Eva Blomqvist, Isabelle Augenstein, Fabio Ciravegna</span>,
<span property="swrc:pages">687-702</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The Web of Data is a rich common resource with billions of triples available in thousands of datasets and individual Web documents created by both expert and non-expert ontologists. A common problem is the imprecision in the use of vocabularies: annotators can misunderstand the semantics of a class or property or may not be able to ﬁnd the right objects to annotate with. This decreases the quality of data and may eventually hamper its usability over large scale. This paper describes Statistical Knowledge Patterns (SKP) as a means to address this issue. SKPs encapsulate key information about ontology classes, including synonymous properties in (and across) datasets, and are automatically generated based on statistical data analysis. SKPs can be eﬀectively used to automatically normalise data, and hence increase recall in querying. Both pattern extraction and pattern usage are completely automated. The main beneﬁts of SKPs are that: (1) their structure allows for both accurate query expansion and restriction; (2) they are context dependent, hence they describe the usage and meaning of properties in the context of a particular class; and (3) they can be generated oﬄine, hence the equivalence among relations can be used eﬃciently at run time.</div>


</li>
<li resource="#Complete_Query_Answering_Over_Horn_Ontologies_Using_a_Triple_Store" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82180703-complete-query-answering-over-horn-ontologies-using-a-triple-store.pdf"><span class="title" property="rdfs:label">Complete Query Answering Over Horn Ontologies Using a Triple Store</span></a>,
<span class="authors" property="swrc:listAuthor">Yujiao Zhou, Yavor Nenov, Bernardo Cuenca Grau, Ian Horrocks</span>,
<span property="swrc:pages">703-718</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">In our previous work, we showed how a scalable OWL 2 RL reasoner can be used to compute both lower and upper bound query answers over very large datasets and arbitrary OWL 2 ontologies. However, when these bounds do not coincide, there still remain a number of possible answer tuples whose status is not determined. In this paper, we show how in the case of Horn ontologies one can exploit the lower and upper bounds computed by the RL reasoner to eﬃciently identify a subset of the data and ontology that is large enough to resolve the status of these tuples, yet small enough so that the status can be computed using a fully-ﬂedged OWL 2 reasoner. The resulting hybrid approach has enabled us to compute exact answers to queries over datasets and ontologies where previously only approximate query answering was possible.</div>


</li>

</ul>

<h3 >Semantic Web In Use Track Paper</h3>

<div class="paper">
<ul>
<li resource="#Social_listening_of_City_Scale_Events_using_the_Streaming_Linked_Data_Framework" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190001-social-listening-of-city-scale-events-using-the-streaming-linked-data-framework.pdf"><span class="title" property="rdfs:label">Social listening of City Scale Events using the Streaming Linked Data Framework</span></a>,
<span class="authors" property="swrc:listAuthor">Marco Balduini, Emanuele Della Valle, Daniele Dell'Aglio, Themis Palpanas, Mikalai Tsytsarau, Cristian Confalonieri</span>,
<span property="swrc:pages">1-16</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">City-scale events may easily attract half a million of visitors in hundreds of venues over just a few days. Which are the most attended venues? What do visitors think about them? How do they feel before, during and after the event? These are few of the questions a city-scale event manger would like to see answered in real-time. In this paper, we report on our experience in social listening of two city-scale events (London Olympic Games 2012, and Milano Design Week 2013) using the Streaming Linked Data Framework.</div>


</li>
<li resource="#Deployment_of_RDFa%2C_Microdata%2C_and_Microformats_on_the_Web_%E2%80%93_A_Quantitative_Analysis" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190017-deployment-of-rdfa-microdata-and-microformats-on-the-web-a-quantitative-analysis.pdf"><span class="title" property="rdfs:label">Deployment of RDFa, Microdata, and Microformats on the Web – A Quantitative Analysis</span></a>,
<span class="authors" property="swrc:listAuthor">Christian Bizer, Kai Eckert, Robert Meusel, Hannes Mühleisen, Michael Schuhmacher, Johanna Völker</span>,
<span property="swrc:pages">17-32</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">More and more websites embed structured data describing for instance products, reviews, blog posts, people, organizations, events, and cooking recipes into their HTML pages using markup standards such as Microformats, Microdata and RDFa. This development has accelerated in the last two years as major Web companies, such as Google, Facebook, Yahoo!, and Microsoft, have started to use the embedded data within their applications. In this paper, we analyze the adoption of RDFa, Microdata, and Microformats across the Web. Our study is based on a large public Web crawl dating from early 2012 and consisting of 3 billion HTML pages which originate from over 40 million websites. The analysis reveals the deployment of the different markup standards, the main topical areas of the published data as well as the different vocabularies that are used within each topical area to represent data. What distinguishes our work from earlier studies, published by the large Web companies, is that the analyzed crawl as well as the extracted data are publicly available. This allows our ﬁndings to be veriﬁed and to be used as starting points for further domain-speciﬁc investigations as well as for focused information extraction endeavors.</div>


</li>
<li resource="#Entity_recommendations_in_Web_Search" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190033-entity-recommendations-in-web-search.pdf"><span class="title" property="rdfs:label">Entity recommendations in Web Search</span></a>,
<span class="authors" property="swrc:listAuthor">Roi Blanco, Berkant Barla Cambazoglu, Peter Mika, Nicolas Torzec</span>,
<span property="swrc:pages">33-48</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">While some web search users know exactly what they are looking for, others are willing to explore topics related to an initial interest. Often, the user’s initial interest can be uniquely linked to an entity in a knowledge base. In this case, it is natural to recommend the explicitly linked entities for further exploration. In real world knowledge bases, however, the number of linked entities may be very large and not all related entities may be equally relevant. Thus, there is a need for ranking related entities. In this paper, we describe Spark, a recommendation engine that links a user’s initial query to an entity within a knowledge base and provides a ranking of the related entities. Spark extracts several signals from a variety of data sources, including Yahoo! Web Search, Twitter, and Flickr, using a large cluster of computers running Hadoop. These signals are combined with a machine learned ranking model in order to produce a ﬁnal recommendation of entities to user queries. This system is currently powering Yahoo! Web Search result pages.</div>


</li>
<li resource="#The_Energy_Management_Adviser_at_EDF" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190049-the-energy-management-adviser-at-edf.pdf"><span class="title" property="rdfs:label">The Energy Management Adviser at EDF</span></a>,
<span class="authors" property="swrc:listAuthor">Pierre Chaussecourte, Birte Glimm, Ian Horrocks, Boris Motik, Laurent Pierre</span>,
<span property="swrc:pages">49-64</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The EMA (Energy Management Adviser) aims to produce personalised energy saving advice for EDF’s customers. The advice takes the form of one or more ‘tips’, and personalisation is achieved using semantic technologies: customers are described using RDF, an OWL ontology provides a conceptual model of the relevant domain (housing, environment, and so on) and the diﬀerent kinds of tips, and SPARQL query answering is used to identify relevant tips. The current prototype provides tips to more than 300,000 EDF customers in France at least twice a year. The main challenges for our future work include providing a timely service for all of the 35 million EDF customers in France, simplifying the system’s maintenance, and providing new ways for interacting with customers such as via a Web site.</div>


</li>
<li resource="#Incorporating_Commercial_and_Private_Data_into_an_Open_Linked_Data_Platform_for_Drug_Discovery" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190065-incorporating-commercial-and-private-data-into-an-open-linked-data-platform-for-drug-discovery.pdf"><span class="title" property="rdfs:label">Incorporating Commercial and Private Data into an Open Linked Data Platform for Drug Discovery</span></a>,
<span class="authors" property="swrc:listAuthor">Carole Goble, Alasdair J. G. Gray, Lee Harland, Karen Karapetyan, Antonis Loizou, Ivan Mikhailov, Yrjana Rankka, Stefan Senger, Valery Tkachenko, Antony Williams, Egon Willighagen</span>,
<span property="swrc:pages">65-80</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The Open PHACTS Discovery Platform aims to provide an integrated information space to advance pharmacological research in the area of drug discovery. Eﬀective drug discovery requires comprehensive data coverage, i.e. integrating all available sources of pharmacology data. While many relevant data sources are available on the linked open data cloud, their content needs to be combined with that of commercial datasets and the licensing of these commercial datasets respected when providing access to the data. Additionally, pharmaceutical companies have built up their own extensive private data collections that they require to be included in their pharmacological dataspace. In this paper we discuss the challenges of incorporating private and commercial data into a linked dataspace: focusing on the modelling of these datasets and their interlinking. We also present the graph-based access control mechanism that ensures commercial and private datasets are only available to authorized users.</div>


</li>
<li resource="#When_History_Matters_-_Assessing_Reliability_for_the_Reuse_of_Scientific_Workflows" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190081-when-history-matters-assessing-reliability-for-the-reuse-of-scientific-workflows.pdf"><span class="title" property="rdfs:label">When History Matters - Assessing Reliability for the Reuse of Scientific Workflows</span></a>,
<span class="authors" property="swrc:listAuthor">José Manuel Gómez-Pérez, Esteban García-Cuesta, Aleix Garrido, José Enrique Ruiz</span>,
<span property="swrc:pages">81-96</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Scientiﬁc workﬂows play an important role in computational research as essential artifacts for communicating the methods used to produce research ﬁndings. We are witnessing a growing number of efforts that treat workﬂows as ﬁrst-class artifacts for sharing and exchanging scientiﬁc knowledge, either as part of scholarly articles or as standalone ob jects. However, workﬂows are not born to be reliable, which can seriously damage their reusability and trustworthiness as knowledge exchange instruments. Scientiﬁc workﬂows are commonly sub ject to decay, which consequently undermines their reliability over their lifetime. The reliability of workﬂows can be notably improved by advocating scientists to preserve a minimal set of information that is essential to assist the interpretations of these workﬂows and hence improve their potential for reproducibility and reusability. In this paper we show how, by measuring and monitoring the completeness and stability of scientiﬁc workﬂows over time we are able to provide scientists with a measure of their reliability, supporting the reuse of trustworthy scientiﬁc knowledge.</div>


</li>
<li resource="#Integrating_NLP_using_Linked_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190097-integrating-nlp-using-linked-data.pdf"><span class="title" property="rdfs:label">Integrating NLP using Linked Data</span></a>,
<span class="authors" property="swrc:listAuthor">Sebastian Hellmann, Jens Lehmann, Sören Auer, Martin Brümmer</span>,
<span property="swrc:pages">97-112</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">We are currently observing a plethora of Natural Language Processing tools and services being made available. Each of the tools and services has its particular strengths and weaknesses, but exploiting the strengths and synergistically combining diﬀerent tools is currently an extremely cumbersome and time consuming task. Also, once a particular set of tools is integrated, this integration is not reusable by others. We argue that simplifying the interoperability of diﬀerent NLP tools performing similar but also complementary tasks will facilitate the comparability of results and the creation of sophisticated NLP applications. In this paper, we present the NLP Interchange Format (NIF). NIF is based on a Linked Data enabled URI scheme for identifying elements in (hyper-)texts and an ontology for describing common NLP terms and concepts. In contrast to more centralized solutions such as UIMA and GATE, NIF enables the creation of heterogeneous, distributed and loosely coupled NLP applications, which use the Web as an integration platform. We present several use cases of the second version of the NIF speciﬁcation (NIF 2.0) and the result of a developer study.</div>


</li>
<li resource="#A_Linked-Data-driven_and_Semantically-enabled_Journal_Portal_for_Scientometrics" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190113-a-linked-data-driven-and-semantically-enabled-journal-portal-for-scientometrics.pdf"><span class="title" property="rdfs:label">A Linked-Data-driven and Semantically-enabled Journal Portal for Scientometrics</span></a>,
<span class="authors" property="swrc:listAuthor">Yingjie Hu, Krzysztof Janowicz, Grant Mckenzie, Kunal Sengupta, Pascal Hitzler</span>,
<span property="swrc:pages">113-128</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The Semantic Web journal by IOS Press follows a unique open and transparent process during which each submitted manuscript is available online together with the full history of its successive decision statuses, assigned editors, solicited and voluntary reviewers, their full text reviews, and in many cases also the authors’ response letters. Combined with a highly-customized, Drupal-based journal management system, this provides the journal with semantically rich manuscript time lines and networked data about authors, reviewers, and editors. These data are now exposed using a SPARQL endpoint, an extended Bibo ontology, and a modular Linked Data portal that provides interactive scientometrics based on established and new analysis methods. The portal can be customized for other journals as well.</div>


</li>
<li resource="#Cross-language_Semantic_Retrieval_and_Linking_of_E-gov_Services" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190129-cross-language-semantic-retrieval-and-linking-of-e-gov-services.pdf"><span class="title" property="rdfs:label">Cross-language Semantic Retrieval and Linking of E-gov Services</span></a>,
<span class="authors" property="swrc:listAuthor">Fedelucio Narducci, Matteo Palmonari, Giovanni Semeraro</span>,
<span property="swrc:pages">129-144</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Public administrations are aware of the advantages of sharing Open Government Data in terms of transparency, development of improved services, collaboration between stakeholders, and spurring new economic activities. Initiatives for the publication and interlinking of government service catalogs as Linked Open Data (lod) support the interoperability among European administrations and improve the capability of foreign citizens to access services across Europe. However, linking service catalogs to reference lod catalogs requires a signiﬁcant eﬀort from local administrations, preventing the uptake of interoperable solutions at a large scale. The web application presented in this paper is named CroSeR (Cross-language Service Retriever) and supports public bodies in the process of linking their own service catalogs to the lod cloud. CroSeR supports diﬀerent European languages and adopts a semantic representation of e-gov services based on Wikipedia. CroSeR tries to overcome problems related to the short textual descriptions associated to a service by embodying a semantic annotation algorithm that enriches service labels with emerging Wikipedia concepts related to the service. An experimental evaluation carried-out on e-gov service catalogs in ﬁve diﬀerent languages shows the eﬀectiveness of our model.</div>


</li>
<li resource="#Using_the_past_to_explain_the_present%3A_interlinking_current_affairs_with_archives_via_the_Semantic_Web" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190145-using-the-past-to-explain-the-present-interlinking-current-affairs-with-archives-via-the-semantic-web.pdf"><span class="title" property="rdfs:label">Using the past to explain the present: interlinking current affairs with archives via the Semantic Web</span></a>,
<span class="authors" property="swrc:listAuthor">Yves Raimond, Michael Smethurst, Andrew McParland, Christopher Lowis</span>,
<span property="swrc:pages">145-160</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The BBC has a very large archive of programmes, covering a wide range of topics. This archive holds a signiﬁcant part of the BBC’s institutional memory and is an important part of the cultural history of the United Kingdom and the rest of the world. These programmes, or parts of them, can help provide valuable context and background for current news events. However the BBC’s archive catalogue is not a complete record of everything that was ever broadcast. For example, it excludes the BBC World Service, which has been broadcasting since 1932. This makes the discovery of content within these parts of the archive very diﬃcult. In this paper we describe a system based on Semantic Web technologies which helps us to quickly locate content related to current news events within those parts of the BBC’s archive with little or no pre-existing metadata. This system is driven by automated interlinking of archive content with the Semantic Web, user validations of the resulting data and topic extraction from live BBC News subtitles. The resulting inter-links between live news subtitles and the BBC’s archive are used in a dynamic visualisation enabling users to quickly locate relevant content. This content can then be used by journalists and editors to provide historical context, background information and supporting content around current aﬀairs.</div>


</li>
<li resource="#Publishing_the_Norwegian_Petroleum_Directorate%E2%80%99s_FactPages_as_Semantic_Web_Data" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190161-publishing-the-norwegian-petroleum-directorate-s-factpages-as-semantic-web-data.pdf"><span class="title" property="rdfs:label">Publishing the Norwegian Petroleum Directorate’s FactPages as Semantic Web Data</span></a>,
<span class="authors" property="swrc:listAuthor">Martin G. Skjæveland, Espen H. Lian, Ian Horrocks</span>,
<span property="swrc:pages">161-176</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">This paper motivates, documents and evaluates the process and results of converting the Norwegian Petroleum Directorate’s Fact-Pages, a well-known and diverse set of tabular data, but with little and incomplete schema information, stepwise into other representations where in each step more semantics is added to the dataset. The diﬀerent representations we consider are a regular relational database, a linked open data dataset, and an ontology. For each conversion step we explain and discuss necessary design choices which are due to the speciﬁc shape of the dataset, but also those due to the characteristics and idiosyncrasies of the representation formats. We additionally evaluate the output, performance and cost of querying the diﬀerent formats using questions provided by users of the FactPages.</div>


</li>
<li resource="#Real-time_Urban_Monitoring_in_Dublin_using_Semantic_and_Stream_Technologies" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190177-real-time-urban-monitoring-in-dublin-using-semantic-and-stream-technologies.pdf"><span class="title" property="rdfs:label">Real-time Urban Monitoring in Dublin using Semantic and Stream Technologies</span></a>,
<span class="authors" property="swrc:listAuthor">Simone Tallevi-Diotallevi, Spyros Kotoulas, Luca Foschini, Freddy Lecue, Antonio Corradi</span>,
<span property="swrc:pages">177-192</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Several sources of information, from people, systems, things, are already available in most modern cities. Processing these continuous ﬂows of information and capturing insight poses unique technical challenges that span from response time constraints to data heterogeneity, in terms of format and throughput. To tackle these problems, we focus on a novel prototype to ease real-time monitoring and decision-making processes for the City of Dublin with three main original technical aspects: (i) an extension to SPARQL to support efﬁcient querying of heterogeneous streams; (ii) a query execution framework and runtime environment based on IBM InfoSphere Streams, a high-performance, industrial strength, stream processing engine; (iii) a hybrid RDFS reasoner, optimized for our stream processing execution framework. Our approach has been validated with real data collected on the ﬁeld, as shown in our Dublin City video demonstration. Results indicate that real-time processing of city information streams based on semantic technologies is indeed not only possible, but also efﬁcient, scalable and low-latency.</div>


</li>
<li resource="#Using_Semantic_Web_in_ICD-11%3A_Three_Years_Down_the_Road" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190193-using-semantic-web-in-icd-11-three-years-down-the-road.pdf"><span class="title" property="rdfs:label">Using Semantic Web in ICD-11: Three Years Down the Road</span></a>,
<span class="authors" property="swrc:listAuthor">Tania Tudorache, Csongor I Nyulas, Natasha F. Noy, Mark Musen</span>,
<span property="swrc:pages">193-208</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The World Health Organization is using Semantic Web technologies in the development of the 11th revision of the International Classiﬁcation of Diseases (ICD-11). Health oﬃcials use ICD in all United Nations member countries to compile basic health statistics, to monitor health-related spending, and to inform policy makers. In 2010, we published a paper in the ISWC In Use track reporting on our experience in the ﬁrst six months with building and deploying iCAT, a Semantic Web platform to support the collaborative authoring of ICD-11. Three years since our original publication, 270 domain experts around the world have used iCAT to author more than 45,000 classes, to perform more than 260,000 changes, and to create more than 17,000 links to external medical terminologies. During the last three years, the collaboration processes, modeling and tooling have evolved signiﬁcantly, and we have learned important lessons, which we will report in this paper. We describe the beneﬁts of using semantic technologies as an infrastructure, which proved to be critical in making support for this rapid evolution possible. To our knowledge, this eﬀort is the only real-world pro ject supporting the collaborative authoring of ontologies at this scale, and which, at the same time, has a high visibility and impact for the health care around the world. We believe that the insights that we gained and the lessons that we learned after four years into this large-scale pro ject will be useful to others who need to support similar collaborative pro jects.</div>


</li>
<li resource="#Semantic_Data_and_Models_Sharing_in_systems_Biology%3A_The_Just_Enough_Results_Model_and_the_SEEK_Platform" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190209-semantic-data-and-models-sharing-in-systems-biology-the-just-enough-results-model-and-the-seek-platform.pdf"><span class="title" property="rdfs:label">Semantic Data and Models Sharing in systems Biology: The Just Enough Results Model and the SEEK Platform</span></a>,
<span class="authors" property="swrc:listAuthor">Katherine Wolstencroft, Stuart Owen, Olga Krebs, Quyen Ngyuen, Jacky. L. Snoep, Wolfgang Mueller, Carole Goble</span>,
<span property="swrc:pages">209-224</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Research in Systems Biology involves integrating data and knowledge about the dynamic processes in biological systems in order to understand and model them. Semantic web technologies should be ideal for exploring the complex networks of genes, proteins and metabolites that interact, but much of this data is not natively available to the semantic web. Data is typically collected and stored with free-text annotations in spreadsheets, many of which do not conform to existing metadata standards and are often not publica lly released. Along with initiatives to promote more data sharing, one of the main challenges is therefore to semantically annotate and extract this data so that it is available to the research community. Data annotation and curation are expensive and undervalued tasks that have enormous benefits to the discipline as a whole, but fewer benefits to the individual data producers. By embedding semantic annotation into spreadsheets, however, and automat ically extracting this data into RDF at the time of repository submission, the process of producing standards-compliant data, that is available for semantic web querying, can be achieved without adding additional overheads to laboratory data management. This paper describes these strategies in the context of semantic data management in the SEEK. The SEEK is a web-based resource for sharing and exchanging Systems Biology data and models that is underpinned by the JERM ontology (Just Enough Results Model), which describes the relationships between data, models, protocols and experiments. The SEEK was originally developed for SysMO, a large European Systems Biology consortium studying micro-organisms, but it has since had widespread adoption across European Systems Biology.</div>


</li>
<li resource="#Reasoning_on_crowd-sourced_semantic_annotations_to_facilitate_cataloguing_of_3D_artefacts_in_the_cultural_heritage_domain" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190225-reasoning-on-crowd-sourced-semantic-annotations-to-facilitate-cataloguing-of-3d-artefacts-in-the-cultural-heritage-domain.pdf"><span class="title" property="rdfs:label">Reasoning on crowd-sourced semantic annotations to facilitate cataloguing of 3D artefacts in the cultural heritage domain</span></a>,
<span class="authors" property="swrc:listAuthor">Chih-Hao Yu, Tudor Groza, Jane Hunter</span>,
<span property="swrc:pages">225-240</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">The 3D Semantic Annotation (3DSA) system expedites the classiﬁcation of 3D digital surrogates from the cultural heritage domain, by leveraging crowd-sourced semantic annotations. More speciﬁcally, the 3DSA system generates high-level classiﬁcations of 3D ob jects by applying rule-based reasoning across community-generated annotations and low-level shape and size attributes. This paper describes a particular use of the 3DSA system – cataloguing Greek pottery. It also describes our novel approach to rule-based reasoning that is modelled on concepts inspired from Markov logic networks. Our evaluation of this approach demonstrates its eﬃciency, accuracy and versatility, compared to classical rule-based reasoning.</div>


</li>
<li resource="#Using_Linked_Data_to_evaluate_the_impact_of_Research_and_Development_in_Europe%3A_a_Structural_Equation_Model" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190241-using-linked-data-to-evaluate-the-impact-of-research-and-development-in-europe-a-structural-equation-model.pdf"><span class="title" property="rdfs:label">Using Linked Data to evaluate the impact of Research and Development in Europe: a Structural Equation Model</span></a>,
<span class="authors" property="swrc:listAuthor">Amrapali Zaveri, Joao Ricardo Nickenig Vissoci, Cinzia Daraio, Ricardo Pietrobon</span>,
<span property="swrc:pages">241-256</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Europe has a high impact on the global biomedical literature, having contributed with a growing number of research articles and a signiﬁcant citation impact. However, the impact of research and development generated by European countries on economic, educational and healthcare performance is poorly understood. The recent Linking Open Data (LOD) project has made a lot of data sources publicly available and in human-readable formats. In this paper, we demonstrate the utility of LOD in assessing the impact of Research and Development (R&D) on the economic, education and healthcare performance in Europe. We extract relevant variables from two LOD datasets, namely World Bank and Eurostat. We analyze the data for 20 out of the 27 European countries over a span of 10 years (1999 to 2009). We use a Structural Equation Modeling (SEM) approach to quantify the impact of R&D on the different measures. We perform different exploratory and conﬁrmatory factorial analysis evaluations which gives rise to four latent variables that are included in the model: (i) Research and Development (R&D), (ii) Economic Performance (EcoP), (iii) Educational Performance (EduP), (iv) Healthcare performance (HcareP) of the European countries. Our results indicate the importance of R&D to the overall development of the European educational and healthcare performance (directly) and economic performance (indirectly). The results also shows the practical applicability of LOD to estimate this impact.</div>


</li>

</ul>

<h3 >Evaluation Track Paper</h3>

<div class="paper">
<ul>
<li resource="#Crowdsourcing_Linked_Data_Quality_Assessment" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190257-crowdsourcing-linked-data-quality-assessment.pdf"><span class="title" property="rdfs:label">Crowdsourcing Linked Data Quality Assessment</span></a>,
<span class="authors" property="swrc:listAuthor">Maribel Acosta, Amrapali Zaveri, Elena Simperl, Dimitris Kontokostas, Sören Auer, Jens Lehmann</span>,
<span property="swrc:pages">257-272</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">In this paper we look into the use of crowdsourcing as a means to handle Linked Data quality problems that are challenging to be solved automatically. We analyzed the most common errors encountered in Linked Data sources and classiﬁed them according to the extent to which they are likely to be amenable to a speciﬁc form of crowdsourcing. Based on this analysis, we implemented a quality assessment methodology for Linked Data that leverages the wisdom of the crowds in different ways: (i) a contest targeting an expert crowd of researchers and Linked Data enthusiasts; complemented by (ii) paid microtasks published on Amazon Mechanical Turk. We empirically evaluated how this methodology could efﬁciently spot quality issues in DBpedia. We also investigated how the contributions of the two types of crowds could be optimally integrated into Linked Data curation processes. The results show that the two styles of crowdsourcing are complementary and that crowdsourcing-enabled quality assessment is a promising and affordable way to enhance the quality of Linked Data.</div>


</li>
<li resource="#SPARQL_Web-Querying_Infrastructure%3A_Ready_for_Action%3F" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190273-sparql-web-querying-infrastructure-ready-for-action-.pdf"><span class="title" property="rdfs:label">SPARQL Web-Querying Infrastructure: Ready for Action?</span></a>,
<span class="authors" property="swrc:listAuthor">Carlos Buil-Aranda, Aidan Hogan, Jürgen Umbrich, Pierre-Yves Vandenbussche</span>,
<span property="swrc:pages">273-288</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Hundreds of public SPARQL endpoints have been deployed on the Web, forming a novel decentralised infrastructure for querying billions of structured facts from a variety of sources on a plethora of topics. But is this infrastructure mature enough to support applications? For 427 public SPARQL endpoints registered on the DataHub, we conduct various experiments to test their maturity. Regarding discoverability, we nd that only one-third of endpoints make descriptive meta-data available, making it dicult to locate or learn about their content and capabilities. Regarding interoperability, we nd patchy support for established SPARQL features like ORDER BY as well as (understandably) for new SPARQL 1.1 features. Regarding eciency, we show that the performance of endpoints for generic queries can vary by up to 34 orders of magnitude. Regarding availability, based on a 27-month long monitoring experiment, we show that only 32.2% of public endpoints can be expected to have (monthly) two-nines uptimes of 99100%.</div>


</li>
<li resource="#String_Similarity_Metrics_for_Ontology_Alignment" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190289-string-similarity-metrics-for-ontology-alignment.pdf"><span class="title" property="rdfs:label">String Similarity Metrics for Ontology Alignment</span></a>,
<span class="authors" property="swrc:listAuthor">Michelle Cheatham, Pascal Hitzler</span>,
<span property="swrc:pages">289-304</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Ontology alignment is an important part of enabling the semantic web to reach its full potential. The vast ma jority of ontology alignment systems use one or more string similarity metrics, but often the choice of which metrics to use is not given much attention. In this work we evaluate a wide range of such metrics, along with string pre-processing strategies such as removing stop words and considering synonyms, on diﬀerent types of ontologies. We also present a set of guidelines on when to use which metric. We furthermore show that if optimal string similarity metrics are chosen, those alone can produce alignments that are competitive with the state of the art in ontology alignment systems. Finally, we examine the improvements possible to an existing ontology alignment system using an automated string metric selection strategy based upon the characteristics of the ontologies to be aligned.</div>


</li>
<li resource="#NoSQL_Databases_for_RDF%3A_An_Empirical_Evaluation" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190305-nosql-databases-for-rdf-an-empirical-evaluation.pdf"><span class="title" property="rdfs:label">NoSQL Databases for RDF: An Empirical Evaluation</span></a>,
<span class="authors" property="swrc:listAuthor">Philippe Cudré-Mauroux, Iliya Enchev, Sever Fundatureanu, Paul Groth, Albert Haque, Andreas Harth, Felix Leif Keppmann, Daniel Miranker, Juan Sequeda, Marcin Wylot</span>,
<span property="swrc:pages">305-320</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Processing large volumes of RDF data requires sophisticated tools. In recent years, much eﬀort was spent on optimizing native RDF stores and on repurposing relational query engines for large-scale RDF processing. Concurrently, a number of new data management systems— regrouped under the NoSQL (for “not only SQL”) umbrella—rapidly rose to prominence and represent today a popular alternative to classical databases. Though NoSQL systems are increasingly used to manage RDF data, it is still diﬃcult to grasp their key advantages and drawbacks in this context. This work is, to the best of our knowledge, the ﬁrst systematic attempt at characterizing and comparing NoSQL stores for RDF processing. In the following, we describe four diﬀerent NoSQL stores and compare their key characteristics when running standard RDF benchmarks on a popular cloud infrastructure using both single-machine and distributed deployments.</div>


</li>
<li resource="#On_Correctness_in_RDF_stream_processor_benchmarking" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190321-on-correctness-in-rdf-stream-processor-benchmarking.pdf"><span class="title" property="rdfs:label">On Correctness in RDF stream processor benchmarking</span></a>,
<span class="authors" property="swrc:listAuthor">Daniele Dell'Aglio, Jean-Paul Calbimonte, Marco Balduini, Oscar Corcho, Emanuele Della Valle</span>,
<span property="swrc:pages">321-336</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Two complementary benchmarks have been proposed so far for the evaluation and continuous improvement of RDF stream processors: SRBench and LSBench. They put a special focus on diﬀerent features of the evaluated systems, including coverage of the streaming extensions of SPARQL supported by each processor, query processing throughput, and an early analysis of query evaluation correctness, based on comparing the results obtained by diﬀerent processors for a set of queries. However, none of them has analysed the operational semantics of these processors in order to assess the correctness of query evaluation results. In this paper, we propose a characterization of the operational semantics of RDF stream processors, adapting well-known models used in the stream processing engine community: CQL and SECRET. Through this formalization, we address correctness in RDF stream processor benchmarks, allowing to determine the multiple answers that systems should provide. Finally, we present CSRBench, an extension of SRBench to address query result correctness veriﬁcation using an automatic method.</div>


</li>
<li resource="#Geographica%3A_A_Benchmark_for_Geospatial_RDF_Stores" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190337-geographica-a-benchmark-for-geospatial-rdf-stores.pdf"><span class="title" property="rdfs:label">Geographica: A Benchmark for Geospatial RDF Stores</span></a>,
<span class="authors" property="swrc:listAuthor">George Garbis, Kostis Kyzirakos, Manolis Koubarakis</span>,
<span property="swrc:pages">337-352</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently been deﬁned and corresponding geospatial RDF stores have been implemented. However, there is no widely used benchmark for evaluating geospatial RDF stores which takes into account recent advances to the state of the art in this area. In this paper, we develop a benchmark, called Geographica, which uses both real-world and synthetic data to test the oﬀered functionality and the performance of some prominent geospatial RDF stores.</div>


</li>
<li resource="#Introducing_Statistical_Design_of_Experiments_to_SPARQL_Endpoint_Evaluation" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190353-introducing-statistical-design-of-experiments-to-sparql-endpoint-evaluation.pdf"><span class="title" property="rdfs:label">Introducing Statistical Design of Experiments to SPARQL Endpoint Evaluation</span></a>,
<span class="authors" property="swrc:listAuthor">Kjetil Kjernsmo, John S. Tyssedal</span>,
<span property="swrc:pages">353-368</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">This paper argues that the common practice of benchmarking is inadequate as a scientiﬁc evaluation methodology. It further attempts to introduce the empirical tradition of the physical sciences by using techniques from Statistical Design of Experiments applied to the example of SPARQL endpoint performance evaluation. It does so by studying full as well as fractional factorial experiments designed to evaluate an assertion that some change introduced in a system has improved performance. This paper does not present a ﬁnished experimental design, rather its main focus is didactical, to shift the focus of the community away from benchmarking towards higher scientiﬁc rigor.</div>


</li>
<li resource="#Towards_a_systematic_benchmarking_of_ontology-based_query_rewriting_systems" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190369-towards-a-systematic-benchmarking-of-ontology-based-query-rewriting-systems.pdf"><span class="title" property="rdfs:label">Towards a systematic benchmarking of ontology-based query rewriting systems</span></a>,
<span class="authors" property="swrc:listAuthor">Jose Mora, Oscar Corcho</span>,
<span property="swrc:pages">369-384</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Query rewriting is one of the fundamental steps in ontologybased data access (OBDA) approaches. It takes as inputs an ontology and a query written according to that ontology, and produces as an output a set of queries that should be evaluated to account for the inferences that should be considered for that query and ontology. Diﬀerent query rewriting systems give support to diﬀerent ontology languages with varying expressiveness, and the rewritten queries obtained as an output do also vary in expressiveness. This heterogeneity has traditionally made it diﬃcult to compare diﬀerent approaches, and the area lacks in general commonly agreed benchmarks that could be used not only for such comparisons but also for improving OBDA support. In this paper we compile data, dimensions and measurements that have been used to evaluate some of the most recent systems, we analyse and characterise these assets, and provide a uniﬁed set of them that could be used as a starting point towards a more systematic benchmarking process for such systems. Finally, we apply this initial benchmark with some of the most relevant OBDA approaches in the state of the art.</div>


</li>
<li resource="#Evaluation_measures_for_ontology_matchers_in_supervised_matching_scenarios" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190385-evaluation-measures-for-ontology-matchers-in-supervised-matching-scenarios.pdf"><span class="title" property="rdfs:label">Evaluation measures for ontology matchers in supervised matching scenarios</span></a>,
<span class="authors" property="swrc:listAuthor">Dominique Ritze, Heiko Paulheim, Kai Eckert</span>,
<span property="swrc:pages">385-400</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Precision and Recall, as well as their combination in terms of F-Measure, are widely used measures in computer science and generally applied to evaluate the overall performance of ontology matchers in fully automatic, unsupervised scenarios. In this paper, we investigate the case of supervised matching, where automatically created ontology alignments are veriﬁed by an expert. We motivate and describe this use case and its characteristics and discuss why traditional, F-measure based evaluation measures are not suitable for this use case. Therefore, we investigate several alternative evaluation measures and propose the use of Precision@N curves as a mean to assess different matching systems for supervised matching. We compare the ranking of several state of the art matchers using Precision@N curves to the traditional F-measure based ranking, and discuss means to combine matchers in a way that optimizes the user support in supervised ontology matching.</div>


</li>
<li resource="#Evaluating_and_benchmarking_SPARQL_query_containment_solvers" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190401-evaluating-and-benchmarking-sparql-query-containment-solvers.pdf"><span class="title" property="rdfs:label">Evaluating and benchmarking SPARQL query containment solvers</span></a>,
<span class="authors" property="swrc:listAuthor">Melisachew Wudage Chekol, Jérôme Euzenat, Pierre Genevès, Nabil Layaïda</span>,
<span property="swrc:pages">401-416</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Query containment is the problem of deciding if the answers to a query are included in those of another query for any queried database. This problem is very important for query optimization purposes. In the SPARQL context, it can be equally useful. This problem has recently been investigated theoretically and some query containment solvers are available. Yet, there were no benchmarks to compare theses systems and foster their improvement. In order to experimentally assess implementation strengths and limitations, we provide a rst SPARQL containment test benchmark. It has been designed with respect to both the capabilities of existing solvers and the study of typical queries. Some solvers support optional constructs and cycles, while other solvers support pro jection, union of conjunctive queries and RDF Schemas. No solver currently supports all these features or OWL entailment regimes. The study of query demographics on DBPedia logs shows that the vast ma jority of queries are acyclic and a signicant part of them uses UNION or pro jection. We thus test available solvers on their domain of applicability on three dierent benchmark suites. These experiments show that (i) tested solutions are overall functionally correct, (ii) in spite of its complexity, SPARQL query containment is practicable for acyclic queries, (iii) state-of-the-art solvers are at an early stage both in terms of capability and implementation.</div>


</li>

</ul>

<h3 >Doctoral Consortium Paper</h3>

<div class="paper">
<ul>
<li resource="#Assessing_Content_Value_for_Digital_Publishing_through_Relevance_and_Provenance-based_Trust" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190417-assessing-content-value-for-digital-publishing-through-relevance-and-provenance-based-trust.pdf"><span class="title" property="rdfs:label">Assessing Content Value for Digital Publishing through Relevance and Provenance-based Trust</span></a>,
<span class="authors" property="swrc:listAuthor">Tom De Nies</span>,
<span property="swrc:pages">417-424</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Due to the abundance of content on the Web, content authors and publishers have a pressing need for systems that select content that is valuable for them, is trustworthy and is related to their own work. Additionally, the value of their own work needs to be assessed before it is published, to guarantee high value for the consumer. In this doctoral research, we investigate how to use Semantic Web technologies to automatically assess the value of content that is – or is about to be – digitally published. To achieve this, we propose methods to assess the relevance of content to existing publications, retrieve or reconstruct its provenance, and derive a trust assessment from this provenance. We discuss our evaluation methods, and present some preliminary results.</div>


</li>
<li resource="#The_effects_of_Licensing_on_Open_Data%3A_Computing_a_measure_of_health_for_our_Scholarly_Record" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190425-the-effects-of-licensing-on-open-data-computing-a-measure-of-health-for-our-scholarly-record.pdf"><span class="title" property="rdfs:label">The effects of Licensing on Open Data: Computing a measure of health for our Scholarly Record</span></a>,
<span class="authors" property="swrc:listAuthor">Richard Hosking, Mark Gahegan</span>,
<span property="swrc:pages">425-432</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">As data collections become established in key disciplines, some of the longstanding barriers to data sharing become to dissolve; yet others remain. While metadata and ontologies help overcome the problems of finding and interpreting data, the lack of clarity over licensing remains a real impediment to data reuse. Freedom from legal restriction and uncertainty is essential for the effective sharing, combining and deriving of data from these distributed collections. Reuse and recombination of data will be greatly facilitated by expanding the definition of the semantic web to include the semantics of data licensing. We aim to express licensing terms in a computable manner, within the context of research practice, enabling us to infer the resulting state of rights, obligations and conditions that are inherited by derived and recombined datasets, using a mixed bag of licenses. Building off this we aim to simulate the effects of varying licensing practices within communities, proposing a measure of health of our scholarly record based on compatibility and restrictiveness of the licenses contained therein.</div>


</li>
<li resource="#Utilising_Provenance_to_Enhance_Social_Computation" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190433-utilising-provenance-to-enhance-social-computation.pdf"><span class="title" property="rdfs:label">Utilising Provenance to Enhance Social Computation</span></a>,
<span class="authors" property="swrc:listAuthor">Milan Markovic</span>,
<span property="swrc:pages">433-440</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Many online platforms employ networks of human workers to perform computational tasks that can be diﬃcult for a machine (e.g. reporting travel disruption). Such systems have to make a range of decisions, for example, selection of suitable workers for a task. In this paper we present an approach that utilises Semantic Web technologies and provenance to support such decision-making processes.</div>


</li>
<li resource="#Crowdsourcing_Ontology_Verification" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190441-crowdsourcing-ontology-verification.pdf"><span class="title" property="rdfs:label">Crowdsourcing Ontology Verification</span></a>,
<span class="authors" property="swrc:listAuthor">Jonathan Mortensen</span>,
<span property="swrc:pages">441-448</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">As the scale and complexity of ontologies increases, so too do errors and engineering challenges. It is frequently unclear, however, to what degree extralogical ontology errors negatively affect the application that the ontology underpins. For example, “Shoe SubClassOf Foot” may be correct logically, but not in a human interpretation. Indeed, such errors, not caught by reasoning, are likely to be domain-speciﬁc, and thus identifying salient ontology errors requires consideration of the domain. There are both automated and manual methods that provide ontology quality assurance. Nevertheless, these methods do not readily scale as ontology size increases, and do not necessarily identify the most salient extralogical errors. Recently, crowdsourcing has enabled solutions to complex problems that computers alone cannot solve. For instance, human workers can quickly and more accurately identify objects in images at scale. Crowdsourcing presents an opportunity to develop methods for ontology quality assurance that overcome the current limitations of scalability and applicability. In this work, I aim (1) to determine the effect of extralogical ontology errors in an example domain, (2) to develop a scalable framework for crowdsourcing ontology veriﬁcation that overcomes current ontology Q/A method limitations, and (3) to apply this framework to ontologies in use. I will then evaluate the method itself and also its effect in the context of a speciﬁc domain. As an example domain, I will use biomedicine, which applies many large-scale ontologies. Thus, this work will enable scalable quality assurance for extralogical errors in biomedical ontologies.</div>


</li>
<li resource="#Interactive_Pay_as_you_go_Relational-to-Ontology_Mapping" typeof="swrc:InProceedings">
<a rel="foaf:homepage" href="82190449-interactive-pay-as-you-go-relational-to-ontology-mapping.pdf"><span class="title" property="rdfs:label">Interactive Pay as you go Relational-to-Ontology Mapping</span></a>,
<span class="authors" property="swrc:listAuthor">Christoph Pinkel</span>,
<span property="swrc:pages">449-456</span>
<div property="swrc:abstract" class="abstract" style="display:none;font-style:italic">Ontology Based Data Access (OBDA) enables access to relational data with a complex structure through ontologies as conceptual domain models. To this end, mappings are required. A key aim of OBDA is to facilitate access to data with a complex structure. Ironically, though, in today’s existing OBDA systems mappings typically need to be compiled by hand, which is a complex and labor intensive task. Additionally, existing semi-automatic mapping approaches suﬀer from high human eﬀort for cleaning up results. Fully automatic approaches, on the other side, suﬀer from a lack of precision and/or recall. In setups where the correctness of query results is crucial but the initial human eﬀort must still be kept be small as possible, neither approach is acceptable. This situation calls for a guided, pay as you go feedback process for human mapping validation. We envision a comprehensive suite of methods and techniques that work well with one another in a seamless mapping process and support mapping construction in the context of OBDA. This suite will in part consist of a recombination and adaptation of various existing methods, but will also comprise newly devised algorithms and techniques.</div>


</li>

</ul>

</div>

</div>

</body></html>
