{
    "tracks": [
        {
            "papers": [
                {
                    "page_start": "1", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180001-trm-learning-dependencies-between-text-and-structure-with-topical-relational-models.pdf", 
                    "title": "TRM \u2013 Learning Dependencies between Text and Structure with Topical Relational Models", 
                    "abstract": "Text-rich structured data become more and more ubiquitous on the Web and on the enterprise databases by encoding heterogeneous structural information between entities such as people, locations, or organizations and the associated textual information. For analyzing this type of data, existing topic modeling approaches, which are highly tailored toward document collections, require manually-de\ufb01ned regularization terms to exploit and to bias the topic learning towards structure information. We propose an approach, called Topical Relational Model, as a principled approach for automatically learning topics from both textual and structure information. Using a topic model, we can show that our approach is e\ufb00ective in exploiting heterogeneous structure information, outperforming a state-of-the-art approach that requires manually-tuned regularization.", 
                    "author": "Veli Bicer, Thanh Tran, Yongtao Ma", 
                    "author_latex": "Veli Bicer, Thanh Tran, Yongtao Ma", 
                    "uri": "#TRM_%E2%80%93_Learning_Dependencies_between_Text_and_Structure_with_Topical_Relational_Models", 
                    "authors": [
                        {
                            "name": "Veli Bicer", 
                            "uri": "#Veli_Bicer"
                        }, 
                        {
                            "name": "Thanh Tran", 
                            "uri": "#Thanh_Tran"
                        }, 
                        {
                            "name": "Yongtao Ma", 
                            "uri": "#Yongtao_Ma"
                        }
                    ], 
                    "link_local": "82180001-trm-learning-dependencies-between-text-and-structure-with-topical-relational-models.pdf", 
                    "pages": "1-16"
                }, 
                {
                    "page_start": "17", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180017-a-confidentiality-model-for-ontologies.pdf", 
                    "title": "A Confidentiality Model for Ontologies", 
                    "abstract": "We illustrate several novel attacks to the con\ufb01dentiality of knowledge bases (KB). Then we introduce a new con\ufb01dentiality model, sensitive enough to detect those attacks, and a method for constructing secure KB views. We identify safe approximations of the background knowledge exploited in the attacks; they can be used to reduce the complexity of constructing secure KB views.", 
                    "author": "Piero Bonatti, Luigi Sauro", 
                    "author_latex": "Piero Bonatti, Luigi Sauro", 
                    "uri": "#A_Confidentiality_Model_for_Ontologies", 
                    "authors": [
                        {
                            "name": "Piero Bonatti", 
                            "uri": "#Piero_Bonatti"
                        }, 
                        {
                            "name": "Luigi Sauro", 
                            "uri": "#Luigi_Sauro"
                        }
                    ], 
                    "link_local": "82180017-a-confidentiality-model-for-ontologies.pdf", 
                    "pages": "17-32"
                }, 
                {
                    "page_start": "33", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180033-pattern-based-knowledge-base-enrichment.pdf", 
                    "title": "Pattern Based Knowledge Base Enrichment", 
                    "abstract": "Although an increasing number of RDF knowledge bases are published, many of those consist primarily of instance data and lack sophisticated schemata. Having such schemata allows more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the e\ufb00ort required to create them. In this article, we propose a semi-automatic schemata construction approach addressing this problem: First, the frequency of axiom patterns in existing knowledge bases is discovered. Afterwards, those patterns are converted to SPARQL based pattern detection algorithms, which allow to enrich knowledge base schemata. We argue that we present the \ufb01rst scalable knowledge base enrichment approach based on real schema usage patterns. The approach is evaluated on a large set of knowledge bases with a quantitative and qualitative result analysis.", 
                    "author": "Lorenz B\u00fchmann, Jens Lehmann", 
                    "author_latex": "Lorenz B\\\"uhmann, Jens Lehmann", 
                    "uri": "#Pattern_Based_Knowledge_Base_Enrichment", 
                    "authors": [
                        {
                            "name": "Lorenz B\u00fchmann", 
                            "uri": "#Lorenz_B%C3%BChmann"
                        }, 
                        {
                            "name": "Jens Lehmann", 
                            "uri": "#Jens_Lehmann"
                        }
                    ], 
                    "link_local": "82180033-pattern-based-knowledge-base-enrichment.pdf", 
                    "pages": "33-48"
                }, 
                {
                    "page_start": "49", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180049-controlled-query-evaluation-over-owl-2-rl-ontologies.pdf", 
                    "title": "Controlled Query Evaluation over OWL 2 RL Ontologies", 
                    "abstract": "We study con\ufb01dentiality enforcement in ontology-based information systems where ontologies are expressed in OWL 2 RL, a pro\ufb01le of OWL 2 that is becoming increasingly popular in Semantic Web applications. We formalise a natural adaptation of the Controlled Query Evaluation (CQE) framework to ontologies. Our goal is to provide CQE algorithms that (i) ensure con\ufb01dentiality of sensitive information; (ii) are e\ufb03ciently implementable by means of RDF triple store technologies; and (iii) ensure maximality of the answers returned by the system to user queries (thus restricting access to information as little as possible). We formally show that these requirements are in con\ufb02ict and cannot be satis\ufb01ed without imposing restrictions on ontologies. We propose a fragment of OWL 2 RL for which all three requirements can be satis\ufb01ed. For the identi\ufb01ed fragment, we design a CQE algorithm that has the same computational complexity as standard query answering and can be implemented by relying on state-of-the-art triple stores.", 
                    "author": "Bernardo Cuenca Grau, Evgeny Kharlamov, Egor V. Kostylev, Dmitriy Zheleznyakov", 
                    "author_latex": "Bernardo Cuenca Grau, Evgeny Kharlamov, Egor V. Kostylev, Dmitriy Zheleznyakov", 
                    "uri": "#Controlled_Query_Evaluation_over_OWL_2_RL_Ontologies", 
                    "authors": [
                        {
                            "name": "Bernardo Cuenca Grau", 
                            "uri": "#Bernardo_Cuenca_Grau"
                        }, 
                        {
                            "name": "Evgeny Kharlamov", 
                            "uri": "#Evgeny_Kharlamov"
                        }, 
                        {
                            "name": "Egor V. Kostylev", 
                            "uri": "#Egor_V._Kostylev"
                        }, 
                        {
                            "name": "Dmitriy Zheleznyakov", 
                            "uri": "#Dmitriy_Zheleznyakov"
                        }
                    ], 
                    "link_local": "82180049-controlled-query-evaluation-over-owl-2-rl-ontologies.pdf", 
                    "pages": "49-64"
                }, 
                {
                    "page_start": "65", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180065-completeness-statements-about-rdf-data-sources-and-their-use-for-query-answering.pdf", 
                    "title": "Completeness Statements about RDF Data Sources and Their Use for Query Answering", 
                    "abstract": "With thousands of RDF data sources available on the Web covering disparate and possibly overlapping knowledge domains, the problem of providing high-level descriptions (in the form of metadata) of their content becomes crucial. In this paper we introduce a theoretical framework for describing data sources in terms of their completeness. We show how existing data sources can be described with completeness statements expressed in RDF. We then focus on the problem of the completeness of query answering over plain and RDFS data sources augmented with completeness statements. Finally, we present an extension of the completeness framework for federated data sources.", 
                    "author": "Fariz Darari, Werner Nutt, Giuseppe Pirr\u00f2, Simon Razniewski", 
                    "author_latex": "Fariz Darari, Werner Nutt, Giuseppe Pirr\\`o, Simon Razniewski", 
                    "uri": "#Completeness_Statements_about_RDF_Data_Sources_and_Their_Use_for_Query_Answering", 
                    "authors": [
                        {
                            "name": "Fariz Darari", 
                            "uri": "#Fariz_Darari"
                        }, 
                        {
                            "name": "Werner Nutt", 
                            "uri": "#Werner_Nutt"
                        }, 
                        {
                            "name": "Giuseppe Pirr\u00f2", 
                            "uri": "#Giuseppe_Pirr%C3%B2"
                        }, 
                        {
                            "name": "Simon Razniewski", 
                            "uri": "#Simon_Razniewski"
                        }
                    ], 
                    "link_local": "82180065-completeness-statements-about-rdf-data-sources-and-their-use-for-query-answering.pdf", 
                    "pages": "65-80"
                }, 
                {
                    "page_start": "81", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180081-empirical-study-of-logic-based-modules-cheap-is-cheerful.pdf", 
                    "title": "Empirical Study of Logic-Based Modules: Cheap Is Cheerful", 
                    "abstract": "For ontology reuse and integration, a number of approaches have been devised that aim at identifying modules, i.e., suitably small sets of \u201crelevant\u201d axioms from ontologies. Here we consider three logically sound notions of modules: MEX modules, only applicable to inexpressive ontologies; modules based on semantic locality, a sound approximation of the \ufb01rst; and modules based on syntactic locality, a sound approximation of the second (and thus the \ufb01rst), widely used since these modules can be extracted from OWL DL ontologies in time polynomial in the size of the ontology. In this paper we investigate the quality of both approximations over a large corpus of ontologies, using our own implementation of semantic locality, which is the \ufb01rst to our knowledge. In particular, we show with statistical signi\ufb01cance that, in most cases, there is no di\ufb00erence between the two module notions based on locality; where they di\ufb00er, the additional axioms can either be easily ruled out or their number is relatively small. We classify the axioms that explain the rare di\ufb00erences into four kinds of \u201cculprits\u201d and discuss which of those can be avoided by extending the de\ufb01nition of syntactic locality. Finally, we show that di\ufb00erences between MEX and locality-based modules occur for a minority of ontologies from our corpus and largely a\ufb00ect (approximations of ) expressive ontologies \u2013 this conclusion relies on a much larger and more diverse sample than existing comparisons between MEX and syntactic locality-based modules.", 
                    "author": "Chiara Del Vescovo, Pavel Klinov, Bijan Parsia, Ulrike Sattler, Thomas Schneider, Dmitry Tsarkov", 
                    "author_latex": "Chiara Del Vescovo, Pavel Klinov, Bijan Parsia, Ulrike Sattler, Thomas Schneider, Dmitry Tsarkov", 
                    "uri": "#Empirical_Study_of_Logic-Based_Modules%3A_Cheap_Is_Cheerful", 
                    "authors": [
                        {
                            "name": "Chiara Del Vescovo", 
                            "uri": "#Chiara_Del_Vescovo"
                        }, 
                        {
                            "name": "Pavel Klinov", 
                            "uri": "#Pavel_Klinov"
                        }, 
                        {
                            "name": "Bijan Parsia", 
                            "uri": "#Bijan_Parsia"
                        }, 
                        {
                            "name": "Ulrike Sattler", 
                            "uri": "#Ulrike_Sattler"
                        }, 
                        {
                            "name": "Thomas Schneider", 
                            "uri": "#Thomas_Schneider"
                        }, 
                        {
                            "name": "Dmitry Tsarkov", 
                            "uri": "#Dmitry_Tsarkov"
                        }
                    ], 
                    "link_local": "82180081-empirical-study-of-logic-based-modules-cheap-is-cheerful.pdf", 
                    "pages": "81-96"
                }, 
                {
                    "page_start": "97", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180097-the-logic-of-extensional-rdfs.pdf", 
                    "title": "The Logic of Extensional RDFS", 
                    "abstract": "The normative version of RDF Schema (RDFS) gives non-standard (intensional) interpretations to some standard notions such as classes and properties, thus departing from standard set-based semantics. In this paper we develop a standard set-based (extensional) semantics for the RDFS vocabulary while preserving the simplicity and computational complexity of deduction of the intensional version. This result can positively impact current implementations, as reasoning in RDFS can be implemented following common set-based intuitions and be compatible with OWL extensions.", 
                    "author": "Enrico Franconi, Claudio Gutierrez, Alessandro Mosca, Giuseppe Pirr\u00f2, Riccardo Rosati", 
                    "author_latex": "Enrico Franconi, Claudio Gutierrez, Alessandro Mosca, Giuseppe Pirr\\`o, Riccardo Rosati", 
                    "uri": "#The_Logic_of_Extensional_RDFS", 
                    "authors": [
                        {
                            "name": "Enrico Franconi", 
                            "uri": "#Enrico_Franconi"
                        }, 
                        {
                            "name": "Claudio Gutierrez", 
                            "uri": "#Claudio_Gutierrez"
                        }, 
                        {
                            "name": "Alessandro Mosca", 
                            "uri": "#Alessandro_Mosca"
                        }, 
                        {
                            "name": "Giuseppe Pirr\u00f2", 
                            "uri": "#Giuseppe_Pirr%C3%B2"
                        }, 
                        {
                            "name": "Riccardo Rosati", 
                            "uri": "#Riccardo_Rosati"
                        }
                    ], 
                    "link_local": "82180097-the-logic-of-extensional-rdfs.pdf", 
                    "pages": "97-112"
                }, 
                {
                    "page_start": "113", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180113-indented-tree-or-graph-a-usability-study-of-ontology-visualization-techniques-in-the-context-of-class-mapping-evaluation.pdf", 
                    "title": "Indented Tree or Graph? A Usability Study of Ontology Visualization Techniques in the Context of Class Mapping Evaluation", 
                    "abstract": "Research  effort  in  ontology  visualization  has  largely  focused  on  developing new visualization  techniques. At  the  same  time,  researchers have paid  less  attention  to  investigating  the  usability  of  common  visualization  techniques  that many practitioners regularly use to visualize ontological data. In this paper,  we  focus  on  two  popular  ontology  visualization  techniques:  indented  tree  and  graph. We  conduct  a  controlled  usability  study  with  an  emphasis  on  the  effectiveness,  efficiency, workload  and  satisfaction of  these visualization  techniques  in  the  context  of  assisting  users  during  evaluation  of  ontology mappings.  Findings  from  this  study have  revealed both  strengths and weaknesses of each visualization  technique.  In  particular,  while  the  indented  tree  visualization  is  more  organized and familiar to novice users, subjects found the graph visualization to  be  more  controllable  and  intuitive  without  visual  redundancy,  particularly  for  ontologies with multiple inheritance.", 
                    "author": "Bo Fu, Natalya F. Noy, Margaret-Anne Storey", 
                    "author_latex": "Bo Fu, Natalya F. Noy, Margaret-Anne Storey", 
                    "uri": "#Indented_Tree_or_Graph%3F_A_Usability_Study_of_Ontology_Visualization_Techniques_in_the_Context_of_Class_Mapping_Evaluation", 
                    "authors": [
                        {
                            "name": "Bo Fu", 
                            "uri": "#Bo_Fu"
                        }, 
                        {
                            "name": "Natalya F. Noy", 
                            "uri": "#Natalya_F._Noy"
                        }, 
                        {
                            "name": "Margaret-Anne Storey", 
                            "uri": "#Margaret-Anne_Storey"
                        }
                    ], 
                    "link_local": "82180113-indented-tree-or-graph-a-usability-study-of-ontology-visualization-techniques-in-the-context-of-class-mapping-evaluation.pdf", 
                    "pages": "113-128"
                }, 
                {
                    "page_start": "129", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180129-real-time-rdf-extraction-from-unstructured-data-streams.pdf", 
                    "title": "Real-time RDF extraction from unstructured data streams", 
                    "abstract": "The vision behind the Web of Data is to extend the current document-oriented Web with machine-readable facts and structured data, thus creating a representation of general knowledge. However, most of the Web of Data is limited to being a large compendium of encyclopedic knowledge describing entities. A huge challenge, the timely and massive extraction of RDF facts from unstructured data, has remained open so far. The availability of such knowledge on the Web of Data would provide signi\ufb01cant bene\ufb01ts to manifold applications including news retrieval, sentiment analysis and business intelligence. In this paper, we address the problem of the actuality of the Web of Data by presenting an approach that allows extracting RDF triples from unstructured data streams. We employ statistical methods in combination with deduplication, disambiguation and unsupervised as well as supervised machine learning techniques to create a knowledge base that re\ufb02ects the content of the input streams. We evaluate a sample of the RDF we generate against a large corpus of news streams and show that we achieve a precision of more than 85%.", 
                    "author": "Daniel Gerber, Sebastian Hellmann, Lorenz B\u00fchmann, Tommaso Soru, Axel-Cyrille Ngonga Ngomo, Ricardo Usbeck", 
                    "author_latex": "Daniel Gerber, Sebastian Hellmann, Lorenz B\\\"uhmann, Tommaso Soru, Axel-Cyrille Ngonga Ngomo, Ricardo Usbeck", 
                    "uri": "#Real-time_RDF_extraction_from_unstructured_data_streams", 
                    "authors": [
                        {
                            "name": "Daniel Gerber", 
                            "uri": "#Daniel_Gerber"
                        }, 
                        {
                            "name": "Sebastian Hellmann", 
                            "uri": "#Sebastian_Hellmann"
                        }, 
                        {
                            "name": "Lorenz B\u00fchmann", 
                            "uri": "#Lorenz_B%C3%BChmann"
                        }, 
                        {
                            "name": "Tommaso Soru", 
                            "uri": "#Tommaso_Soru"
                        }, 
                        {
                            "name": "Axel-Cyrille Ngonga Ngomo", 
                            "uri": "#Axel-Cyrille_Ngonga_Ngomo"
                        }, 
                        {
                            "name": "Ricardo Usbeck", 
                            "uri": "#Ricardo_Usbeck"
                        }
                    ], 
                    "link_local": "82180129-real-time-rdf-extraction-from-unstructured-data-streams.pdf", 
                    "pages": "129-144"
                }, 
                {
                    "page_start": "145", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180145-one-license-to-compose-them-all-a-deontic-logic-approach-to-data-licensing-on-the-web-of-data.pdf", 
                    "title": "One License to Compose Them All: a deontic logic approach to data licensing on the Web of Data", 
                    "abstract": "In the domain of Linked Open Data a need is emerging for developing automated frameworks able to generate the licensing terms associated to data coming from heterogeneous distributed sources. This paper proposes and evaluates a deontic logic semantics which allows us to de\ufb01ne the deontic components of the licenses, i.e., permissions, obligations, and prohibitions, and generate a composite license compliant with the licensing items of the composed different licenses. Some heuristics are proposed to support the data publisher in choosing the licenses composition strategy which better suits her needs w.r.t. the data she is publishing.", 
                    "author": "Guido Governatori, Antonino Rotolo, Serena Villata, Fabien Gandon", 
                    "author_latex": "Guido Governatori, Antonino Rotolo, Serena Villata, Fabien Gandon", 
                    "uri": "#One_License_to_Compose_Them_All%3A_a_deontic_logic_approach_to_data_licensing_on_the_Web_of_Data", 
                    "authors": [
                        {
                            "name": "Guido Governatori", 
                            "uri": "#Guido_Governatori"
                        }, 
                        {
                            "name": "Antonino Rotolo", 
                            "uri": "#Antonino_Rotolo"
                        }, 
                        {
                            "name": "Serena Villata", 
                            "uri": "#Serena_Villata"
                        }, 
                        {
                            "name": "Fabien Gandon", 
                            "uri": "#Fabien_Gandon"
                        }
                    ], 
                    "link_local": "82180145-one-license-to-compose-them-all-a-deontic-logic-approach-to-data-licensing-on-the-web-of-data.pdf", 
                    "pages": "145-160"
                }, 
                {
                    "page_start": "161", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180161-federated-entity-search-using-on-the-fly-consolidation.pdf", 
                    "title": "Federated Entity Search using On-The-Fly Consolidation", 
                    "abstract": "Nowadays, search on the Web goes beyond the retrieval of textual Web sites and increasingly takes advantage of the growing amount of structured data. Of particular interest is entity search, where the units of retrieval are structured entities instead of textual documents. These entities reside in di\ufb00erent sources, which may provide only limited information about their content and are therefore called \u201cuncooperative\u201d. Further, these sources capture complementary but also redundant information about entities. In this environment of uncooperative data sources, we study the problem of federated entity search, where redundant information about entities is reduced on-the-\ufb02y through entity consolidation performed at query time. We propose a novel method for entity consolidation that is based on using language models and completely unsupervised, hence more suitable for this on-the-\ufb02y uncooperative setting than state-of-the-art methods that require training data. Further, we apply the same language model technique to deal with the federated search problem of ranking results returned from di\ufb00erent sources. Particular novel are the mechanisms we propose to incorporate consolidation results into this ranking. We perform experiments using real Web queries and data sources. Our experiments show that our approach for federated entity search with on-the-\ufb02y consolidation improves upon the performance of a state-of-the-art preference aggregation baseline and also bene\ufb01ts from consolidation.", 
                    "author": "Daniel M. Herzig, Roi Blanco, Peter Mika, Thanh Tran", 
                    "author_latex": "Daniel M. Herzig, Roi Blanco, Peter Mika, Thanh Tran", 
                    "uri": "#Federated_Entity_Search_using_On-The-Fly_Consolidation", 
                    "authors": [
                        {
                            "name": "Daniel M. Herzig", 
                            "uri": "#Daniel_M._Herzig"
                        }, 
                        {
                            "name": "Roi Blanco", 
                            "uri": "#Roi_Blanco"
                        }, 
                        {
                            "name": "Peter Mika", 
                            "uri": "#Peter_Mika"
                        }, 
                        {
                            "name": "Thanh Tran", 
                            "uri": "#Thanh_Tran"
                        }
                    ], 
                    "link_local": "82180161-federated-entity-search-using-on-the-fly-consolidation.pdf", 
                    "pages": "161-176"
                }, 
                {
                    "page_start": "177", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180177-proswip-property-based-data-access-for-semantic-web-interactive-programming.pdf", 
                    "title": "ProSWIP: Property-based Data Access for Semantic Web Interactive Programming", 
                    "abstract": "The Semantic Web has matured from a mere theoretical vision to a variety of ready-to-use linked open data sources currently available on the Web. Still, with respect to application development , the Web community is just starting to develop new paradigms in which data as the main driver of applications is promoted to first class status. Relying on properties of resources as an indicator for the type, property-based typing is such a paradigm. In this paper, we inspect the feasibility of property-based typing for accessing data from the linked open data cloud. Problems in terms of transparency and quality of the selected data were noticeable. To alleviate these problems, we developed an iterative approach that builds on human feedback.", 
                    "author": "Silviu Homoceanu, Philipp Wille, Wolf-Tilo Balke", 
                    "author_latex": "Silviu Homoceanu, Philipp Wille, Wolf-Tilo Balke", 
                    "uri": "#ProSWIP%3A_Property-based_Data_Access_for_Semantic_Web_Interactive_Programming", 
                    "authors": [
                        {
                            "name": "Silviu Homoceanu", 
                            "uri": "#Silviu_Homoceanu"
                        }, 
                        {
                            "name": "Philipp Wille", 
                            "uri": "#Philipp_Wille"
                        }, 
                        {
                            "name": "Wolf-Tilo Balke", 
                            "uri": "#Wolf-Tilo_Balke"
                        }
                    ], 
                    "link_local": "82180177-proswip-property-based-data-access-for-semantic-web-interactive-programming.pdf", 
                    "pages": "177-192"
                }, 
                {
                    "page_start": "193", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180193-simplified-owl-ontology-editing-for-the-web-is-webprot-g-enough-.pdf", 
                    "title": "Simplified OWL Ontology Editing for the Web: Is WebProt\u00e9g\u00e9 Enough?", 
                    "abstract": "Ontology engineering is a task that is notorious for its di\ufb03culty. As the group that developed Prot\u00e9g\u00e9, the most widely used ontology editor, we are keenly aware of how di\ufb03cult the users perceive this task to be. In this paper, we present the new version of WebProt\u00e9g\u00e9 that we designed with two main goals in mind: (1) create a tool that will be easy to use while still accounting for commonly used OWL constructs; (2) support collaboration and social interaction around distributed ontology editing as part of the core tool design. We designed this new version of the WebProt\u00e9g\u00e9 user interface empirically, by analysing the use of OWL constructs in a large corpus of publicly available ontologies. Since the beta release of this new WebProt\u00e9g\u00e9 interface in January 2013, our users from around the world have created and uploaded 519 ontologies on our server. In this paper, we describe the key features of the new tool and our empirical design approach. We evaluate language coverage in WebProt\u00e9g\u00e9 by assessing how well it covers the OWL constructs that are present in ontologies that users have uploaded to WebProt\u00e9g\u00e9. We evaluate the usability of WebProt\u00e9g\u00e9 through a usability survey. Our analysis validates our empirical design, suggests additional language constructors to explore, and demonstrates that an easy-to-use web-based tool that covers most of the frequently used OWL constructs is su\ufb03cient for many users to start editing their ontologies.", 
                    "author": "Matthew Horridge, Tania Tudorache, Jennifer Vendetti, Csongor Nyulas, Mark Musen, Natasha F. Noy", 
                    "author_latex": "Matthew Horridge, Tania Tudorache, Jennifer Vendetti, Csongor Nyulas, Mark Musen, Natasha F. Noy", 
                    "uri": "#Simplified_OWL_Ontology_Editing_for_the_Web%3A_Is_WebProt%C3%A9g%C3%A9_Enough%3F", 
                    "authors": [
                        {
                            "name": "Matthew Horridge", 
                            "uri": "#Matthew_Horridge"
                        }, 
                        {
                            "name": "Tania Tudorache", 
                            "uri": "#Tania_Tudorache"
                        }, 
                        {
                            "name": "Jennifer Vendetti", 
                            "uri": "#Jennifer_Vendetti"
                        }, 
                        {
                            "name": "Csongor Nyulas", 
                            "uri": "#Csongor_Nyulas"
                        }, 
                        {
                            "name": "Mark Musen", 
                            "uri": "#Mark_Musen"
                        }, 
                        {
                            "name": "Natasha F. Noy", 
                            "uri": "#Natasha_F._Noy"
                        }
                    ], 
                    "link_local": "82180193-simplified-owl-ontology-editing-for-the-web-is-webprot-g-enough-.pdf", 
                    "pages": "193-208"
                }, 
                {
                    "page_start": "209", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180209-a-query-tool-for-el-with-non-monotonic-rules.pdf", 
                    "title": "A Query Tool for EL with Non-monotonic rules", 
                    "abstract": "We present the Prot\u00e9g\u00e9 plug-in NoHR that allows the user to take an E L+\u22a5 ontology, add a set of non-monotonic (logic programming) rules \u2013 suitable e.g. to express defaults and exceptions \u2013 and query the combined knowledge base. Our approach uses the well-founded semantics for MKNF knowledge bases as underlying formalism, so no restriction other than DL-safety is imposed on the rules that can be written. The tool itself builds on the procedure SLG(O) and, with the help of OWL 2 EL reasoner ELK, pre-processes the ontology into rules, whose result together with the non-monotonic rules serve as input for the topdown querying engine XSB Prolog. With the resulting plug-in, even queries to very large ontologies, such as SNOMED CT, augmented with a large number of rules, can be processed at an interactive response time after one initial brief pre-processing period. At the same time, our system is able to deal with possible inconsistencies between the rules and an ontology that alone is consistent.", 
                    "author": "Vadim Ivanov, Matthias Knorr, Joao Leite", 
                    "author_latex": "Vadim Ivanov, Matthias Knorr, Joao Leite", 
                    "uri": "#A_Query_Tool_for_EL_with_Non-monotonic_rules", 
                    "authors": [
                        {
                            "name": "Vadim Ivanov", 
                            "uri": "#Vadim_Ivanov"
                        }, 
                        {
                            "name": "Matthias Knorr", 
                            "uri": "#Matthias_Knorr"
                        }, 
                        {
                            "name": "Joao Leite", 
                            "uri": "#Joao_Leite"
                        }
                    ], 
                    "link_local": "82180209-a-query-tool-for-el-with-non-monotonic-rules.pdf", 
                    "pages": "209-224"
                }, 
                {
                    "page_start": "225", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180225-incremental-reasoning-in-owl-el-without-bookkeeping.pdf", 
                    "title": "Incremental Reasoning in OWL EL without Bookkeeping", 
                    "abstract": "We describe a method for updating the classi\ufb01cation of ontologies expressed in the E L family of Description Logics after some axioms have been added or deleted. While incremental classi\ufb01cation modulo additions is relatively straightforward, handling deletions is more problematic since it requires retracting logical consequences that are no longer valid. Known algorithms address this problem using various forms of bookkeeping to trace the consequences back to premises. But such additional data can consume memory and place an extra burden on the reasoner during application of inferences. In this paper, we present a technique, which avoids this extra cost while being very ef\ufb01cient for small incremental changes in ontologies. The technique is freely available as a part of the open-source E L reasoner ELK and its ef\ufb01ciency is demonstrated on naturally occurring and synthetic data.", 
                    "author": "Yevgeny Kazakov, Pavel Klinov", 
                    "author_latex": "Yevgeny Kazakov, Pavel Klinov", 
                    "uri": "#Incremental_Reasoning_in_OWL_EL_without_Bookkeeping", 
                    "authors": [
                        {
                            "name": "Yevgeny Kazakov", 
                            "uri": "#Yevgeny_Kazakov"
                        }, 
                        {
                            "name": "Pavel Klinov", 
                            "uri": "#Pavel_Klinov"
                        }
                    ], 
                    "link_local": "82180225-incremental-reasoning-in-owl-el-without-bookkeeping.pdf", 
                    "pages": "225-240"
                }, 
                {
                    "page_start": "241", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180241-secure-manipulation-of-linked-data.pdf", 
                    "title": "Secure Manipulation of Linked Data", 
                    "abstract": "When it comes to publishing data on the web, the level of access control required (if any) is highly dependent on the type of content exposed. Up until now RDF data publishers have focused on exposing and linking public data. With the advent of SPARQL 1.1, the linked data infrastructure can be used, not only as a means of publishing open data but also, as a general mechanism for managing distributed graph data. However, such a decentralised architecture brings with it a number of additional challenges with respect to both data security and integrity. In this paper, we propose a general authorisation framework that can be used to deliver dynamic query results based on user credentials and to cater for the secure manipulation of linked data. Speci\ufb01cally we describe how graph patterns, propagation rules, con\ufb02ict resolution policies and integrity constraints can together be used to specify and enforce consistent access control policies.", 
                    "author": "Sabrina Kirrane, Ahmed Abdelrahman, Alessandra Mileo, Stefan Decker", 
                    "author_latex": "Sabrina Kirrane, Ahmed Abdelrahman, Alessandra Mileo, Stefan Decker", 
                    "uri": "#Secure_Manipulation_of_Linked_Data", 
                    "authors": [
                        {
                            "name": "Sabrina Kirrane", 
                            "uri": "#Sabrina_Kirrane"
                        }, 
                        {
                            "name": "Ahmed Abdelrahman", 
                            "uri": "#Ahmed_Abdelrahman"
                        }, 
                        {
                            "name": "Alessandra Mileo", 
                            "uri": "#Alessandra_Mileo"
                        }, 
                        {
                            "name": "Stefan Decker", 
                            "uri": "#Stefan_Decker"
                        }
                    ], 
                    "link_local": "82180241-secure-manipulation-of-linked-data.pdf", 
                    "pages": "241-256"
                }, 
                {
                    "page_start": "257", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180257-a-decision-procedure-for-shoiq-with-transitive-closure-of-roles.pdf", 
                    "title": "A decision procedure for SHOIQ with transitive closure of roles", 
                    "abstract": "The Semantic Web makes an extensive use of the OWL DL ontology language, underlied by the SHOIQ description logic, to formalize its resources. In this paper, we propose a decision procedure for this logic extended with the transitive closure of roles in concept axioms, a feature needed in several application domains. The most challenging issue we have to deal with when designing such a decision procedure is to represent in\ufb01nitely non-tree-shaped models, which are different from those of SHOIQ ontologies. To address this issue, we introduce a new blocking condition for characterizing models which may have an in\ufb01nite non-tree-shaped part.", 
                    "author": "Chan Le Duc, Myriam Lamolle, Olivier Cur\u00e9", 
                    "author_latex": "Chan Le Duc, Myriam Lamolle, Olivier Cur\\'e", 
                    "uri": "#A_decision_procedure_for_SHOIQ_with_transitive_closure_of_roles", 
                    "authors": [
                        {
                            "name": "Chan Le Duc", 
                            "uri": "#Chan_Le_Duc"
                        }, 
                        {
                            "name": "Myriam Lamolle", 
                            "uri": "#Myriam_Lamolle"
                        }, 
                        {
                            "name": "Olivier Cur\u00e9", 
                            "uri": "#Olivier_Cur%C3%A9"
                        }
                    ], 
                    "link_local": "82180257-a-decision-procedure-for-shoiq-with-transitive-closure-of-roles.pdf", 
                    "pages": "257-272"
                }, 
                {
                    "page_start": "273", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180273-elastic-and-scalable-processing-of-linked-stream-data-in-the-cloud.pdf", 
                    "title": "Elastic and scalable processing of Linked Stream Data in the Cloud", 
                    "abstract": "Linked Stream Data extends the Linked Data paradigm to dynamic data sources. It enables the integration and joint processing of heterogeneous stream data with quasi-static data from the Linked Data Cloud in near-real-time. Several Linked Stream Data processing engines exist but their scalability still needs to be in improved in terms of (static and dynamic) data sizes, number of concurrent queries, stream update frequencies, etc. So far, none of them supports parallel processing in the Cloud, i.e., elastic load pro\ufb01les in a hosted environment. To remedy these limitations, this paper presents an approach for elastically parallelizing the continuous execution of queries over Linked Stream Data. For this, we have developed novel, highly e\ufb03cient, and scalable parallel algorithms for continuous query operators. Our approach and algorithms are implemented in our CQELS Cloud system and we present extensive evaluations of their superior performance on Amazon EC2 demonstrating their high scalability and excellent elasticity in a real deployment.", 
                    "author": "Danh Le Phuoc, Hoan Nguyen Mau Quoc, Chan Le Van, Manfred Hauswirth", 
                    "author_latex": "Danh Le Phuoc, Hoan Nguyen Mau Quoc, Chan Le Van, Manfred Hauswirth", 
                    "uri": "#Elastic_and_scalable_processing_of_Linked_Stream_Data_in_the_Cloud", 
                    "authors": [
                        {
                            "name": "Danh Le Phuoc", 
                            "uri": "#Danh_Le_Phuoc"
                        }, 
                        {
                            "name": "Hoan Nguyen Mau Quoc", 
                            "uri": "#Hoan_Nguyen_Mau_Quoc"
                        }, 
                        {
                            "name": "Chan Le Van", 
                            "uri": "#Chan_Le_Van"
                        }, 
                        {
                            "name": "Manfred Hauswirth", 
                            "uri": "#Manfred_Hauswirth"
                        }
                    ], 
                    "link_local": "82180273-elastic-and-scalable-processing-of-linked-stream-data-in-the-cloud.pdf", 
                    "pages": "273-288"
                }, 
                {
                    "page_start": "289", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180289-towards-constructive-evidence-of-data-flow-oriented-web-service-composition.pdf", 
                    "title": "Towards Constructive Evidence of Data Flow-oriented Web Service Composition", 
                    "abstract": "Automation of service composition is one of the most interesting challenges facing the Semantic Web and the Web of services today. Despite approaches which are able to infer a partial order of services, its data \ufb02ow remains implicit and dif\ufb01cult to be automatically generated. Enhanced with formal representations, the semantic links between output and input parameters of services can be then exploited to infer their data \ufb02ow. This work addresses the problem of effectively inferring data \ufb02ow between services based on their representations. To this end, we introduce the non standard Description Logic reasoning join, aiming to provide a \u201cconstructive evidence\u201d of why services can be connected and how non trivial links (many to many parameters) can be inferred in data \ufb02ow. The preliminary evaluation provides evidence in favor of our approach regarding the completeness of data \ufb02ow.", 
                    "author": "Freddy Lecue", 
                    "author_latex": "Freddy Lecue", 
                    "uri": "#Towards_Constructive_Evidence_of_Data_Flow-oriented_Web_Service_Composition", 
                    "authors": [
                        {
                            "name": "Freddy Lecue", 
                            "uri": "#Freddy_Lecue"
                        }
                    ], 
                    "link_local": "82180289-towards-constructive-evidence-of-data-flow-oriented-web-service-composition.pdf", 
                    "pages": "289-304"
                }, 
                {
                    "page_start": "305", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180305-the-combined-approach-to-obda-taming-role-hierarchies-using-filters.pdf", 
                    "title": "The Combined Approach to OBDA: Taming Role Hierarchies using Filters", 
                    "abstract": "The basic idea of the combined approach to query answering in the presence of ontologies is to materialize the consequences of the ontology in the data and then use a limited form of query rewriting to deal with in\ufb01nite materializations. While this approach is e\ufb03cient and scalable for ontologies that are formulated in the basic version of the description logic DL-Lite, it incurs an exponential blowup during query rewriting when DL-Lite is extended with the popular role hierarchies. In this paper, we show how to replace the query rewriting with a \ufb01ltering technique. This is natural from an implementation perspective and allows us to handle role hierarchies without an exponential blowup. We also carry out an experimental evaluation that demonstrates the scalability of this approach.", 
                    "author": "Carsten Lutz, Inanc Seylan, David Toman, Frank Wolter", 
                    "author_latex": "Carsten Lutz, Inanc Seylan, David Toman, Frank Wolter", 
                    "uri": "#The_Combined_Approach_to_OBDA%3A_Taming_Role_Hierarchies_using_Filters", 
                    "authors": [
                        {
                            "name": "Carsten Lutz", 
                            "uri": "#Carsten_Lutz"
                        }, 
                        {
                            "name": "Inanc Seylan", 
                            "uri": "#Inanc_Seylan"
                        }, 
                        {
                            "name": "David Toman", 
                            "uri": "#David_Toman"
                        }, 
                        {
                            "name": "Frank Wolter", 
                            "uri": "#Frank_Wolter"
                        }
                    ], 
                    "link_local": "82180305-the-combined-approach-to-obda-taming-role-hierarchies-using-filters.pdf", 
                    "pages": "305-320"
                }, 
                {
                    "page_start": "321", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180321-a-snapshot-of-the-owl-web.pdf", 
                    "title": "A snapshot of the OWL Web", 
                    "abstract": "Tool development for and empirical experimentation in OWL ontology engineering require a wide variety of suitable ontologies as input for testing and evaluation purposes and detailed characterisations of real ontologies. Empirical activities often resort to (somewhat arbitrarily) hand curated corpora available on the web, such as the NCBO BioPortal and the TONES Repository, or manually selected sets of well-known ontologies. Findings of surveys and results of benchmarking activities may be biased, even heavily, towards these datasets. Sampling from a large corpus of ontologies, on the other hand, may lead to more representative results. Current large scale repositories and web crawls are mostly uncurated and su\ufb00er from duplication, small and (for many purposes) uninteresting ontology \ufb01les, and contain large numbers of ontology versions, variants, and facets, and therefore do not lend themselves to random sampling. In this paper, we survey ontologies as they exist on the web and describe the creation of a corpus of OWL DL ontologies using strategies such as web crawling, various forms of de-duplications and manual cleaning, which allows random sampling of ontologies for a variety of empirical applications.", 
                    "author": "Nicolas Matentzoglu, Samantha Bail, Bijan Parsia", 
                    "author_latex": "Nicolas Matentzoglu, Samantha Bail, Bijan Parsia", 
                    "uri": "#A_snapshot_of_the_OWL_Web", 
                    "authors": [
                        {
                            "name": "Nicolas Matentzoglu", 
                            "uri": "#Nicolas_Matentzoglu"
                        }, 
                        {
                            "name": "Samantha Bail", 
                            "uri": "#Samantha_Bail"
                        }, 
                        {
                            "name": "Bijan Parsia", 
                            "uri": "#Bijan_Parsia"
                        }
                    ], 
                    "link_local": "82180321-a-snapshot-of-the-owl-web.pdf", 
                    "pages": "321-336"
                }, 
                {
                    "page_start": "337", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180337-semantic-rule-filtering-for-web-scale-relation-extraction.pdf", 
                    "title": "Semantic Rule Filtering for Web-Scale Relation Extraction", 
                    "abstract": "Web-scale relation extraction is a means for building and extending large repositories of formalized knowledge. This type of automated knowledge building requires a decent level of precision, which is hard to achieve with automatically acquired rule sets learned from unlabeled data by means of distant or minimal supervision. This paper shows how precision of relation extraction can be considerably improved by employing a wide-coverage, general-purpose lexical semantic network, i.e., BabelNet, for effective semantic rule \ufb01ltering. We apply Word Sense Disambiguation to the content words of the automatically extracted rules. As a result a set of relation-speci\ufb01c relevant concepts is obtained, and each of these concepts is then used to represent the structured semantics of the corresponding relation. The resulting relation-speci\ufb01c subgraphs of BabelNet are used as semantic \ufb01lters for estimating the adequacy of the extracted rules. For the seven semantic relations tested here, the semantic \ufb01lter consistently yields a higher precision at any relative recall value in the high-recall range.", 
                    "author": "Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu, Roberto Navigli, Hans Uszkoreit", 
                    "author_latex": "Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu, Roberto Navigli, Hans Uszkoreit", 
                    "uri": "#Semantic_Rule_Filtering_for_Web-Scale_Relation_Extraction", 
                    "authors": [
                        {
                            "name": "Andrea Moro", 
                            "uri": "#Andrea_Moro"
                        }, 
                        {
                            "name": "Hong Li", 
                            "uri": "#Hong_Li"
                        }, 
                        {
                            "name": "Sebastian Krause", 
                            "uri": "#Sebastian_Krause"
                        }, 
                        {
                            "name": "Feiyu Xu", 
                            "uri": "#Feiyu_Xu"
                        }, 
                        {
                            "name": "Roberto Navigli", 
                            "uri": "#Roberto_Navigli"
                        }, 
                        {
                            "name": "Hans Uszkoreit", 
                            "uri": "#Hans_Uszkoreit"
                        }
                    ], 
                    "link_local": "82180337-semantic-rule-filtering-for-web-scale-relation-extraction.pdf", 
                    "pages": "337-352"
                }, 
                {
                    "page_start": "353", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180353-semantic-message-passing-for-generating-linked-data-from-tables.pdf", 
                    "title": "Semantic Message Passing for Generating Linked Data from Tables", 
                    "abstract": "We describe work on automatically inferring the intended meaning of tables and representing it as RDF linked data, making it available for improving search, interoperability and integration. We present implementation details of a joint inference module that uses knowledge from the linked open data (LOD) cloud to jointly infer the semantics of column headers, table cell values (e.g., strings and numbers) and relations between columns. We also implement a novel Semantic Message Passing algorithm which uses LOD knowledge to improve existing message passing schemes. We evaluate our implemented techniques on tables from the Web and Wikipedia.", 
                    "author": "Varish Mulwad, Tim Finin, Anupam Joshi", 
                    "author_latex": "Varish Mulwad, Tim Finin, Anupam Joshi", 
                    "uri": "#Semantic_Message_Passing_for_Generating_Linked_Data_from_Tables", 
                    "authors": [
                        {
                            "name": "Varish Mulwad", 
                            "uri": "#Varish_Mulwad"
                        }, 
                        {
                            "name": "Tim Finin", 
                            "uri": "#Tim_Finin"
                        }, 
                        {
                            "name": "Anupam Joshi", 
                            "uri": "#Anupam_Joshi"
                        }
                    ], 
                    "link_local": "82180353-semantic-message-passing-for-generating-linked-data-from-tables.pdf", 
                    "pages": "353-368"
                }, 
                {
                    "page_start": "369", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180369-bringing-math-to-lod-a-semantic-publishing-platform-prototype-for-scientific-collections-in-mathematics.pdf", 
                    "title": "Bringing Math to LOD: A Semantic Publishing Platform Prototype for Scientific Collections in Mathematics", 
                    "abstract": "We present our work on developing a software platform for mining mathematical scholarly papers to obtain a Linked Data representation. Currently, the Linking Open Data (LOD) cloud lacks up-to-date and detailed information on professional level mathematics. To our mind, the main reason for that is the absence of appropriate tools that could analyze the underlying semantics in mathematical papers and e\ufb00ectively build their consolidated representation. We have developed a holistic approach to analysis of mathematical documents, including ontology based extraction, conversion of the article body as well as its metadata into RDF, integration with some existing LOD data sets, and semantic search. We argue that the platform may be helpful for enriching user experience on modern online scienti\ufb01c collections.", 
                    "author": "Olga Nevzorova, Nikita Zhiltsov, Danila Zaikin, Olga Zhibrik, Alexander Kirillovich, Vladimir Nevzorov, Evgeniy Birialtsev", 
                    "author_latex": "Olga Nevzorova, Nikita Zhiltsov, Danila Zaikin, Olga Zhibrik, Alexander Kirillovich, Vladimir Nevzorov, Evgeniy Birialtsev", 
                    "uri": "#Bringing_Math_to_LOD%3A_A_Semantic_Publishing_Platform_Prototype_for_Scientific_Collections_in_Mathematics", 
                    "authors": [
                        {
                            "name": "Olga Nevzorova", 
                            "uri": "#Olga_Nevzorova"
                        }, 
                        {
                            "name": "Nikita Zhiltsov", 
                            "uri": "#Nikita_Zhiltsov"
                        }, 
                        {
                            "name": "Danila Zaikin", 
                            "uri": "#Danila_Zaikin"
                        }, 
                        {
                            "name": "Olga Zhibrik", 
                            "uri": "#Olga_Zhibrik"
                        }, 
                        {
                            "name": "Alexander Kirillovich", 
                            "uri": "#Alexander_Kirillovich"
                        }, 
                        {
                            "name": "Vladimir Nevzorov", 
                            "uri": "#Vladimir_Nevzorov"
                        }, 
                        {
                            "name": "Evgeniy Birialtsev", 
                            "uri": "#Evgeniy_Birialtsev"
                        }
                    ], 
                    "link_local": "82180369-bringing-math-to-lod-a-semantic-publishing-platform-prototype-for-scientific-collections-in-mathematics.pdf", 
                    "pages": "369-384"
                }, 
                {
                    "page_start": "385", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180385-orchid-reduction-ratio-optimal-computation-of-geo-spatial-distances-for-link-discovery.pdf", 
                    "title": "ORCHID \u2013 Reduction-Ratio-Optimal Computation of Geo-Spatial Distances for Link Discovery", 
                    "abstract": "The discovery of links between resources within knowledge bases is of crucial importance to realize the vision of the Semantic Web. Addressing this task is especially challenging when dealing with geo-spatial datasets due to their sheer size and the potential complexity of single geo-spatial objects. Yet, so far, little attention has been paid to the characteristics of geo-spatial data within the context of link discovery. In this paper, we address this gap by presenting Orchid, a reduction-ratio-optimal link discovery approach designed especially for geo-spatial data. Orchid relies on a combination of the Hausdor\ufb00 and orthodromic metrics to compute the distance between geo-spatial objects. We \ufb01rst present two novel approaches for the e\ufb03cient computation of Hausdor\ufb00 distances. Then, we present the space tiling approach implemented by Orchid and prove that it is optimal with respect to the reduction ratio that it can achieve. The evaluation of our approaches is carried out on three real datasets of di\ufb00erent size and complexity. Our results suggest that our approaches to the computation of Hausdor\ufb00 distances require two orders of magnitude less orthodromic distances computations to compare geographical data. Moreover, they require two orders of magnitude less time than a naive approach to achieve this goal. Finally, our results indicate that Orchid scales to large datasets while outperforming the state of the art signi\ufb01cantly.", 
                    "author": "Axel-Cyrille Ngonga Ngomo", 
                    "author_latex": "Axel-Cyrille Ngonga Ngomo", 
                    "uri": "#ORCHID_%E2%80%93_Reduction-Ratio-Optimal_Computation_of_Geo-Spatial_Distances_for_Link_Discovery", 
                    "authors": [
                        {
                            "name": "Axel-Cyrille Ngonga Ngomo", 
                            "uri": "#Axel-Cyrille_Ngonga_Ngomo"
                        }
                    ], 
                    "link_local": "82180385-orchid-reduction-ratio-optimal-computation-of-geo-spatial-distances-for-link-discovery.pdf", 
                    "pages": "385-400"
                }, 
                {
                    "page_start": "401", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180401-simplifying-description-logic-ontologies.pdf", 
                    "title": "Simplifying Description Logic Ontologies", 
                    "abstract": "We discuss the problem of minimizing TBoxes expressed in the light-weight description logic E L, which forms a basis of some large ontologies like SNOMED, Gene Ontology, NCI and Galen. We show that the minimization of TBoxes is intractable (NP-complete). While this looks like a bad news result, we also provide a heuristic technique for minimizing TBoxes. We prove the correctness of the heuristics and show that it provides optimal results for a class of ontologies, which we de\ufb01ne through an acyclicity constraint over a reference relation between equivalence classes of concepts. To establish the feasibility of our approach, we have implemented the algorithm and evaluated its effectiveness on a small suite of benchmarks.", 
                    "author": "Nadeschda Nikitina, Sven Schewe", 
                    "author_latex": "Nadeschda Nikitina, Sven Schewe", 
                    "uri": "#Simplifying_Description_Logic_Ontologies", 
                    "authors": [
                        {
                            "name": "Nadeschda Nikitina", 
                            "uri": "#Nadeschda_Nikitina"
                        }, 
                        {
                            "name": "Sven Schewe", 
                            "uri": "#Sven_Schewe"
                        }
                    ], 
                    "link_local": "82180401-simplifying-description-logic-ontologies.pdf", 
                    "pages": "401-416"
                }, 
                {
                    "page_start": "417", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180417-fedsearch-efficiently-combining-structured-queries-and-full-text-search-in-a-sparql-federation.pdf", 
                    "title": "FedSearch: efficiently combining structured queries and full-text search in a SPARQL federation", 
                    "abstract": "Combining structured queries with full-text search provides a powerful means to access distributed linked data. However, executing hybrid search queries in a federation of multiple data sources presents a number of challenges due to data source heterogeneity and lack of statistical data about keyword selectivity. To address these challenges, we present FedSearch \u2013 a novel hybrid query engine based on the SPARQL federation framework FedX. We extend the SPARQL algebra to incorporate keyword search clauses as \ufb01rst-class citizens and apply novel optimization techniques to improve the query processing e\ufb03ciency while maintaining a meaningful ranking of results. By performing on-the-\ufb02y adaptation of the query execution plan and intelligent grouping of query clauses, we are able to reduce signi\ufb01cantly the communication costs making our approach suitable for top-k hybrid search across multiple data sources. In experiments we demonstrate that our optimization techniques can lead to a substantial performance improvement, reducing the execution time of hybrid queries by more than an order of magnitude.", 
                    "author": "Andriy Nikolov, Andreas Schwarte, Christian H\u00fctter", 
                    "author_latex": "Andriy Nikolov, Andreas Schwarte, Christian H\\\"utter", 
                    "uri": "#FedSearch%3A_efficiently_combining_structured_queries_and_full-text_search_in_a_SPARQL_federation", 
                    "authors": [
                        {
                            "name": "Andriy Nikolov", 
                            "uri": "#Andriy_Nikolov"
                        }, 
                        {
                            "name": "Andreas Schwarte", 
                            "uri": "#Andreas_Schwarte"
                        }, 
                        {
                            "name": "Christian H\u00fctter", 
                            "uri": "#Christian_H%C3%BCtter"
                        }
                    ], 
                    "link_local": "82180417-fedsearch-efficiently-combining-structured-queries-and-full-text-search-in-a-sparql-federation.pdf", 
                    "pages": "417-432"
                }, 
                {
                    "page_start": "433", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180433-getting-lucky-in-ontology-search-a-data-driven-evaluation-framework-for-ontology-ranking.pdf", 
                    "title": "Getting Lucky in Ontology Search: A Data-Driven Evaluation Framework for Ontology Ranking", 
                    "abstract": "With hundreds, if not thousands, of ontologies available today in many di\ufb00erent domains, ontology search and ranking has become an important and timely problem. When a user searches a collection of ontologies for her terms of interest, there are often dozens of ontologies that contain these terms. How does she know which ontology is the most relevant to her search? Our research group hosts BioPortal, a public repository of more than 330 ontologies in the biomedical domain. When a term that a user searches for is available in multiple ontologies, how do we rank the results and how do we measure how well our ranking works? In this paper, we develop an evaluation framework that enables developers to compare and analyze the performance of di\ufb00erent ontology-ranking methods. Our framework is based on processing search logs and determining how often users select the top link that the search engine o\ufb00ers. We evaluate our framework by analyzing the data on BioPortal searches. We explore several di\ufb00erent ranking algorithms and measure the e\ufb00ectiveness of each ranking by measuring how often users click on the highest ranked ontology. We collected log data from more than 4,800 BioPortal searches. Our results show that regardless of the ranking, in more than half the searches, users select the \ufb01rst link. Thus, it is even more critical to ensure that the ranking is appropriate if we want to have satis\ufb01ed users. Our further analysis demonstrates that ranking ontologies based on page view data signi\ufb01cantly improves the user experience, with an approximately 26% increase in the number of users who select the highest ranked ontology for the search.", 
                    "author": "Natasha F. Noy, Paul Alexander, Rave Harpaz, Trish Whetzel, Raymond Fergerson, Mark Musen", 
                    "author_latex": "Natasha F. Noy, Paul Alexander, Rave Harpaz, Trish Whetzel, Raymond Fergerson, Mark Musen", 
                    "uri": "#Getting_Lucky_in_Ontology_Search%3A_A_Data-Driven_Evaluation_Framework_for_Ontology_Ranking", 
                    "authors": [
                        {
                            "name": "Natasha F. Noy", 
                            "uri": "#Natasha_F._Noy"
                        }, 
                        {
                            "name": "Paul Alexander", 
                            "uri": "#Paul_Alexander"
                        }, 
                        {
                            "name": "Rave Harpaz", 
                            "uri": "#Rave_Harpaz"
                        }, 
                        {
                            "name": "Trish Whetzel", 
                            "uri": "#Trish_Whetzel"
                        }, 
                        {
                            "name": "Raymond Fergerson", 
                            "uri": "#Raymond_Fergerson"
                        }, 
                        {
                            "name": "Mark Musen", 
                            "uri": "#Mark_Musen"
                        }
                    ], 
                    "link_local": "82180433-getting-lucky-in-ontology-search-a-data-driven-evaluation-framework-for-ontology-ranking.pdf", 
                    "pages": "433-448"
                }, 
                {
                    "page_start": "449", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180449-exploring-scholarly-data-with-rexplore.pdf", 
                    "title": "Exploring Scholarly Data with Rexplore", 
                    "abstract": "Despite the large number and variety of tools and services available today for exploring scholarly data, current support is still very limited in the context of sensemaking tasks, which go beyond standard search and ranking of authors and publications, and focus instead on i) understanding the dynamics of research areas, ii) relating authors \u2018semantically\u2019 (e.g., in terms of common interests or shared academic trajectories), or iii) performing fine-grained academic expert search along multiple dimensions. To address this gap we have developed a novel tool, Rexplore, which integrates statistical analysis, semantic technologies, and visual analytics to provide effective support for exploring and making sense of scholarly data. Here, we describe the main innovative elements of the tool and we present the results from a task-centric empirical evaluation, which shows that Rexplore is highly effective at providing support for the aforementioned sensemaking tasks. In addition, these results are robust both with respect to the background of the users (i.e., expert analysts vs. \u2018ordinary\u2019 users) and also with respect to whether the tasks are selected by the evaluators or proposed by the users themselves.", 
                    "author": "Francesco Osborne, Enrico Motta, Paul Mulholland", 
                    "author_latex": "Francesco Osborne, Enrico Motta, Paul Mulholland", 
                    "uri": "#Exploring_Scholarly_Data_with_Rexplore", 
                    "authors": [
                        {
                            "name": "Francesco Osborne", 
                            "uri": "#Francesco_Osborne"
                        }, 
                        {
                            "name": "Enrico Motta", 
                            "uri": "#Enrico_Motta"
                        }, 
                        {
                            "name": "Paul Mulholland", 
                            "uri": "#Paul_Mulholland"
                        }
                    ], 
                    "link_local": "82180449-exploring-scholarly-data-with-rexplore.pdf", 
                    "pages": "449-464"
                }, 
                {
                    "page_start": "465", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180465-personalized-best-answer-computation-in-graph-databases.pdf", 
                    "title": "Personalized Best Answer Computation in Graph Databases", 
                    "abstract": "Though subgraph matching has been extensively studied as a query paradigm in semantic web and social network data environments, a user can get a large number of answers in response to a query. Just like Google does, these answers can be shown to the user in accordance with an importance ranking. In this paper, we present scalable algorithms to \ufb01nd the top-K answers to a practically important subset of SPARQL-queries, denoted as importance queries, via a suite of pruning techniques. We test our algorithms on multiple real-world graph data sets, showing that our algorithms are e\ufb03cient even on networks with up to 6M vertices and 15M edges and far more e\ufb03cient than popular triple stores.", 
                    "author": "Michael Ovelg\u00f6nne, Noseong Park, V.S. Subrahmanian, Elizabeth K. Bowman, Kirk A. Ogaard", 
                    "author_latex": "Michael Ovelg\\\"onne, Noseong Park, V.S. Subrahmanian, Elizabeth K. Bowman, Kirk A. Ogaard", 
                    "uri": "#Personalized_Best_Answer_Computation_in_Graph_Databases", 
                    "authors": [
                        {
                            "name": "Michael Ovelg\u00f6nne", 
                            "uri": "#Michael_Ovelg%C3%B6nne"
                        }, 
                        {
                            "name": "Noseong Park", 
                            "uri": "#Noseong_Park"
                        }, 
                        {
                            "name": "V.S. Subrahmanian", 
                            "uri": "#V.S._Subrahmanian"
                        }, 
                        {
                            "name": "Elizabeth K. Bowman", 
                            "uri": "#Elizabeth_K._Bowman"
                        }, 
                        {
                            "name": "Kirk A. Ogaard", 
                            "uri": "#Kirk_A._Ogaard"
                        }
                    ], 
                    "link_local": "82180465-personalized-best-answer-computation-in-graph-databases.pdf", 
                    "pages": "465-480"
                }, 
                {
                    "page_start": "481", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180481-towards-an-automatic-creation-of-localized-versions-of-dbpedia.pdf", 
                    "title": "Towards an automatic creation of localized versions of DBpedia", 
                    "abstract": "DBpedia is a large-scale knowledge base that exploits Wikipedia as primary data source. The extraction procedure requires to manually map Wikipedia infoboxes into the DBpedia ontology. Thanks to crowdsourcing, a large number of infoboxes has been mapped in the English DBpedia. Consequently, the same procedure has been applied to other languages to create the localized versions of DBpedia. However, the number of accomplished mappings is still small and limited to most frequent infoboxes. Furthermore, mappings need maintenance due to the constant and quick changes of Wikipedia articles. In this paper, we focus on the problem of automatically mapping infobox attributes to properties into the DBpedia ontology for extending the coverage of the existing localized versions or building from scratch versions for languages not covered in the current version. The evaluation has been performed on the Italian mappings. We compared our results with the current mappings on a random sample re-annotated by the authors. We report results comparable to the ones obtained by a human annotator in term of precision, but our approach leads to a signi\ufb01cant improvement in recall and speed. Speci\ufb01cally, we mapped 45,978 Wikipedia infobox attributes to DBpedia properties in 14 different languages for which mappings were not yet available. The resource is made available in an open format.", 
                    "author": "Alessio Palmero Aprosio, Claudio Giuliano, Alberto Lavelli", 
                    "author_latex": "Alessio Palmero Aprosio, Claudio Giuliano, Alberto Lavelli", 
                    "uri": "#Towards_an_automatic_creation_of_localized_versions_of_DBpedia", 
                    "authors": [
                        {
                            "name": "Alessio Palmero Aprosio", 
                            "uri": "#Alessio_Palmero_Aprosio"
                        }, 
                        {
                            "name": "Claudio Giuliano", 
                            "uri": "#Claudio_Giuliano"
                        }, 
                        {
                            "name": "Alberto Lavelli", 
                            "uri": "#Alberto_Lavelli"
                        }
                    ], 
                    "link_local": "82180481-towards-an-automatic-creation-of-localized-versions-of-dbpedia.pdf", 
                    "pages": "481-496"
                }, 
                {
                    "page_start": "497", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180497-type-inference-on-noisy-rdf-data.pdf", 
                    "title": "Type Inference on Noisy RDF Data", 
                    "abstract": "Type information is very valuable in knowledge bases. However, most large open knowledge bases are incomplete with respect to type information, and, at the same time, contain noisy and incorrect data. That makes classic type inference by reasoning di\ufb03cult. In this paper, we propose the heuristic link-based type inference mechanism SD-Type, which can handle noisy and incorrect data. Instead of leveraging T-box information from the schema, SDType takes the actual use of a schema into account and thus is also robust to misused schema elements.", 
                    "author": "Heiko Paulheim, Christian Bizer", 
                    "author_latex": "Heiko Paulheim, Christian Bizer", 
                    "uri": "#Type_Inference_on_Noisy_RDF_Data", 
                    "authors": [
                        {
                            "name": "Heiko Paulheim", 
                            "uri": "#Heiko_Paulheim"
                        }, 
                        {
                            "name": "Christian Bizer", 
                            "uri": "#Christian_Bizer"
                        }
                    ], 
                    "link_local": "82180497-type-inference-on-noisy-rdf-data.pdf", 
                    "pages": "497-512"
                }, 
                {
                    "page_start": "513", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180513-what-s-in-a-nym-synonyms-in-biomedical-ontology-matching.pdf", 
                    "title": "What's in a 'nym'? Synonyms in Biomedical Ontology Matching", 
                    "abstract": "To bring the Life Sciences domain closer to a Semantic Web realization it is fundamental to establish meaningful relations between biomedical ontologies. The successful application of ontology matching techniques is strongly tied to an e\ufb00ective exploration of the complex and diverse biomedical terminology contained in biomedical ontologies. In this paper, we present an overview of the lexical components of several biomedical ontologies and investigate how di\ufb00erent approaches for their use can impact the performance of ontology matching techniques. We propose novel approaches for exploring the di\ufb00erent types of synonyms encoded by the ontologies and for extending them based both on internal synonym derivation and on external ontologies. We evaluate our approaches using AgreementMaker, a successful ontology matching platform that implements several lexical matchers, and apply them to a set of four benchmark biomedical ontology matching tasks. Our results demonstrate the impact that an adequate consideration of ontology synonyms can have on matching performance, and validate our novel approach for combining internal and external synonym sources as a competitive and in many cases improved solution for biomedical ontology matching.", 
                    "author": "Catia Pesquita, Cosmin Stroe, Daniel Faria, Emanuel Santos, Isabel Cruz, Francisco Couto", 
                    "author_latex": "Catia Pesquita, Cosmin Stroe, Daniel Faria, Emanuel Santos, Isabel Cruz, Francisco Couto", 
                    "uri": "#What%27s_in_a_%27nym%27%3F_Synonyms_in_Biomedical_Ontology_Matching", 
                    "authors": [
                        {
                            "name": "Catia Pesquita", 
                            "uri": "#Catia_Pesquita"
                        }, 
                        {
                            "name": "Cosmin Stroe", 
                            "uri": "#Cosmin_Stroe"
                        }, 
                        {
                            "name": "Daniel Faria", 
                            "uri": "#Daniel_Faria"
                        }, 
                        {
                            "name": "Emanuel Santos", 
                            "uri": "#Emanuel_Santos"
                        }, 
                        {
                            "name": "Isabel Cruz", 
                            "uri": "#Isabel_Cruz"
                        }, 
                        {
                            "name": "Francisco Couto", 
                            "uri": "#Francisco_Couto"
                        }
                    ], 
                    "link_local": "82180513-what-s-in-a-nym-synonyms-in-biomedical-ontology-matching.pdf", 
                    "pages": "513-528"
                }, 
                {
                    "page_start": "529", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180529-knowledge-graph-identification.pdf", 
                    "title": "Knowledge Graph Identification", 
                    "abstract": "Large-scale information processing systems are able to extract massive collections of interrelated facts, but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge. In this paper, we show how uncertain extractions about entities and their relations can be transformed into a know ledge graph. The extractions form an extraction graph and we refer to the task of removing noise, inferring missing information, and determining which candidate facts should be included into a knowledge graph as know ledge graph identi\ufb01cation. In order to perform this task, we must reason jointly about candidate facts and their associated extraction con\ufb01dences, identify coreferent entities, and incorporate ontological constraints. Our proposed approach uses probabilistic soft logic (PSL), a recently introduced probabilistic modeling framework which easily scales to millions of facts. We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL pro ject containing over 1M extractions and 70K ontological relations. We show that compared to existing methods, our approach is able to achieve improved AUC and F1 with signi\ufb01cantly lower running time.", 
                    "author": "Jay Pujara, Hui Miao, Lise Getoor, William Cohen", 
                    "author_latex": "Jay Pujara, Hui Miao, Lise Getoor, William Cohen", 
                    "uri": "#Knowledge_Graph_Identification", 
                    "authors": [
                        {
                            "name": "Jay Pujara", 
                            "uri": "#Jay_Pujara"
                        }, 
                        {
                            "name": "Hui Miao", 
                            "uri": "#Hui_Miao"
                        }, 
                        {
                            "name": "Lise Getoor", 
                            "uri": "#Lise_Getoor"
                        }, 
                        {
                            "name": "William Cohen", 
                            "uri": "#William_Cohen"
                        }
                    ], 
                    "link_local": "82180529-knowledge-graph-identification.pdf", 
                    "pages": "529-544"
                }, 
                {
                    "page_start": "545", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180545-ontology-based-data-access-ontop-of-databases.pdf", 
                    "title": "Ontology-Based Data Access: Ontop of Databases", 
                    "abstract": "We present the architecture and technologies underpinning the OBDA system Ontop and taking full advantage of storing data in relational databases. We discuss the theoretical foundations of Ontop : the tree-witness query rewriting, T -mappings and optimisations based on database integrity constraints and SQL features. We analyse the performance of Ontop in a series of experiments and demonstrate that, for standard ontologies, queries and data stored in relational databases, Ontop is fast, e\ufb03cient and produces SQL rewritings of high quality.", 
                    "author": "Mariano Rodriguez-Muro, Roman Kontchakov, Michael Zakharyaschev", 
                    "author_latex": "Mariano Rodriguez-Muro, Roman Kontchakov, Michael Zakharyaschev", 
                    "uri": "#Ontology-Based_Data_Access%3A_Ontop_of_Databases", 
                    "authors": [
                        {
                            "name": "Mariano Rodriguez-Muro", 
                            "uri": "#Mariano_Rodriguez-Muro"
                        }, 
                        {
                            "name": "Roman Kontchakov", 
                            "uri": "#Roman_Kontchakov"
                        }, 
                        {
                            "name": "Michael Zakharyaschev", 
                            "uri": "#Michael_Zakharyaschev"
                        }
                    ], 
                    "link_local": "82180545-ontology-based-data-access-ontop-of-databases.pdf", 
                    "pages": "545-560"
                }, 
                {
                    "page_start": "561", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180561-daw-duplicate-aware-federated-query-processing-over-the-web-of-data.pdf", 
                    "title": "DAW: Duplicate-AWare Federated Query Processing over the Web of Data", 
                    "abstract": "Over the last years the Web of Data has developed into a large compendium of interlinked data sets from multiple domains. Due to the decentralised architecture of this compendium, several of these datasets contain duplicated data. Yet, so far, only little attention has been paid to the e\ufb00ect of duplicated data on federated querying. This work presents DAW, a novel duplicate-aware approach to federated querying over the Web of Data. DAW is based on a combination of min-wise independent permutations and compact data summaries. It can be directly combined with existing federated query engines in order to achieve the same query recall values while querying fewer data sources. We extend three well-known federated query processing engines \u2013 DARQ, SPLENDID, and FedX \u2013 with DAW and compare our extensions with the original approaches. The comparison shows that DAW can greatly reduce the number of queries sent to the endpoints, while keeping high query recall values. Therefore, it can signi\ufb01cantly improve the performance of federated query processing engines. Moreover, DAW provides a source selection mechanism that maximises the query recall, when the query processing is limited to a subset of the sources.", 
                    "author": "Muhammad Saleem, Axel-Cyrille Ngonga Ngomo, Josiane Xavier Parreira, Helena Deus, Manfred Hauswirth", 
                    "author_latex": "Muhammad Saleem, Axel-Cyrille Ngonga Ngomo, Josiane Xavier Parreira, Helena Deus, Manfred Hauswirth", 
                    "uri": "#DAW%3A_Duplicate-AWare_Federated_Query_Processing_over_the_Web_of_Data", 
                    "authors": [
                        {
                            "name": "Muhammad Saleem", 
                            "uri": "#Muhammad_Saleem"
                        }, 
                        {
                            "name": "Axel-Cyrille Ngonga Ngomo", 
                            "uri": "#Axel-Cyrille_Ngonga_Ngomo"
                        }, 
                        {
                            "name": "Josiane Xavier Parreira", 
                            "uri": "#Josiane_Xavier_Parreira"
                        }, 
                        {
                            "name": "Helena Deus", 
                            "uri": "#Helena_Deus"
                        }, 
                        {
                            "name": "Manfred Hauswirth", 
                            "uri": "#Manfred_Hauswirth"
                        }
                    ], 
                    "link_local": "82180561-daw-duplicate-aware-federated-query-processing-over-the-web-of-data.pdf", 
                    "pages": "561-576"
                }, 
                {
                    "page_start": "577", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180577-on-the-status-of-experimental-research-on-the-semantic-web.pdf", 
                    "title": "On the Status of Experimental Research on the Semantic Web", 
                    "abstract": "Experimentation is an important way to validate results of Semantic Web and Computer Science research in general. In this paper, we investigate the development and the current status of experimental work on the Semantic Web. Based on a corpus of 500 papers collected from the International Semantic Web Conferences (ISWC) over the past decade, we analyse the importance and the quality of experimental research conducted and compare it to general Computer Science. We observe that the amount and quality of experiments are steadily increasing over time. Unlike hypothesised, we cannot con\ufb01rm a statistically signi\ufb01cant correlation between a paper\u2019s citations and the amount of experimental work reported. Our analysis, however, shows that papers comparing themselves to other systems are more often cited than other papers.", 
                    "author": "Heiner Stuckenschmidt, Michael Schuhmacher, Johannes Knopp, Christian Meilicke, Ansgar Scherp", 
                    "author_latex": "Heiner Stuckenschmidt, Michael Schuhmacher, Johannes Knopp, Christian Meilicke, Ansgar Scherp", 
                    "uri": "#On_the_Status_of_Experimental_Research_on_the_Semantic_Web", 
                    "authors": [
                        {
                            "name": "Heiner Stuckenschmidt", 
                            "uri": "#Heiner_Stuckenschmidt"
                        }, 
                        {
                            "name": "Michael Schuhmacher", 
                            "uri": "#Michael_Schuhmacher"
                        }, 
                        {
                            "name": "Johannes Knopp", 
                            "uri": "#Johannes_Knopp"
                        }, 
                        {
                            "name": "Christian Meilicke", 
                            "uri": "#Christian_Meilicke"
                        }, 
                        {
                            "name": "Ansgar Scherp", 
                            "uri": "#Ansgar_Scherp"
                        }
                    ], 
                    "link_local": "82180577-on-the-status-of-experimental-research-on-the-semantic-web.pdf", 
                    "pages": "577-592"
                }, 
                {
                    "page_start": "593", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180593-a-graph-based-approach-to-learn-semantic-descriptions-of-data-sources.pdf", 
                    "title": "A Graph-Based Approach to Learn Semantic Descriptions of Data Sources", 
                    "abstract": "Semantic models of data sources and services provide support to automate many tasks such as source discovery, data integration, and service composition, but writing these semantic descriptions by hand is a tedious and time-consuming task. Most of the related work focuses on automatic annotation with classes or properties of source attributes or input and output parameters. However, constructing a source model that includes the relationships between the attributes in addition to their semantic types remains a largely unsolved problem. In this paper, we present a graph-based approach to hypothesize a rich semantic description of a new target source from a set of known sources that have been modeled over the same domain ontology. We exploit the domain ontology and the known source models to build a graph that represents the space of plausible source descriptions. Then, we compute the top k candidates and suggest to the user a ranked list of the semantic models for the new source. The approach takes into account user corrections to learn more accurate semantic descriptions of future data sources. Our evaluation shows that our method produces models that are twice as accurate than the models produced using a state of the art system that does not learn from prior models.", 
                    "author": "Mohsen Taheriyan, Craig Knoblock, Pedro Szekely, Jos\u00e9 Luis Ambite", 
                    "author_latex": "Mohsen Taheriyan, Craig Knoblock, Pedro Szekely, Jos\\'e Luis Ambite", 
                    "uri": "#A_Graph-Based_Approach_to_Learn_Semantic_Descriptions_of_Data_Sources", 
                    "authors": [
                        {
                            "name": "Mohsen Taheriyan", 
                            "uri": "#Mohsen_Taheriyan"
                        }, 
                        {
                            "name": "Craig Knoblock", 
                            "uri": "#Craig_Knoblock"
                        }, 
                        {
                            "name": "Pedro Szekely", 
                            "uri": "#Pedro_Szekely"
                        }, 
                        {
                            "name": "Jos\u00e9 Luis Ambite", 
                            "uri": "#Jos%C3%A9_Luis_Ambite"
                        }
                    ], 
                    "link_local": "82180593-a-graph-based-approach-to-learn-semantic-descriptions-of-data-sources.pdf", 
                    "pages": "593-608"
                }, 
                {
                    "page_start": "609", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180609-qodi-query-as-context-in-automatic-data-integration.pdf", 
                    "title": "QODI: Query as Context in Automatic Data Integration", 
                    "abstract": "QODI is an automatic ontology-based data integration system (OBDI). QODI is distinguished in that the ontology mapping algorithm dynamically determines a partial mapping speci\ufb01c to the reformulation of each query. The query provides application context not available in the ontologies alone; thereby the system is able to disambiguate mappings for different queries. The mapping algorithm decomposes the query into a set of paths, and compares the set of paths with a similar decomposition of a source ontology. Using test sets from three real world applications, QODI achieves favorable results compared with AgreementMaker, a leading ontology matcher, and an ontology-based implementation of the mapping methods detailed for Clio, the state-of-the-art relational data integration and data exchange system.", 
                    "author": "Aibo Tian, Juan F. Sequeda, Daniel Miranker", 
                    "author_latex": "Aibo Tian, Juan F. Sequeda, Daniel Miranker", 
                    "uri": "#QODI%3A_Query_as_Context_in_Automatic_Data_Integration", 
                    "authors": [
                        {
                            "name": "Aibo Tian", 
                            "uri": "#Aibo_Tian"
                        }, 
                        {
                            "name": "Juan F. Sequeda", 
                            "uri": "#Juan_F._Sequeda"
                        }, 
                        {
                            "name": "Daniel Miranker", 
                            "uri": "#Daniel_Miranker"
                        }
                    ], 
                    "link_local": "82180609-qodi-query-as-context-in-automatic-data-integration.pdf", 
                    "pages": "609-624"
                }, 
                {
                    "page_start": "625", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180625-trank-ranking-entity-types-using-the-web-of-data.pdf", 
                    "title": "TRank: Ranking Entity Types Using the Web of Data", 
                    "abstract": "Much of Web search and browsing activity is today centered around entities. For this reason, Search Engine Result Pages (SERPs) increasingly contain information about the searched entities such as pictures, short summaries, related entities, and factual information. A key facet that is often displayed on the SERPs and that is instrumental for many applications is the entity type. However, an entity is usually not associated to a single generic type in the background knowledge bases but rather to a set of more speci\ufb01c types, which may be relevant or not given the document context. For example, one can \ufb01nd on the Linked Open Data cloud the fact that Tom Hanks is a person, an actor, and a person from Concord, California. All those types are correct but some may be too general to be interesting (e.g., person), while other may be interesting but already known to the user (e.g., actor), or may be irrelevant given the current browsing context (e.g., person from Concord, California). In this paper, we de\ufb01ne the new task of ranking entity types given an entity and its context. We propose and evaluate new methods to \ufb01nd the most relevant entity type based on collection statistics and on the graph structure interconnecting entities and types. An extensive experimental evaluation over several document collections at di\ufb00erent levels of granularity (e.g., sentences, paragraphs, etc.) and di\ufb00erent type hierarchies (including DBPedia, Freebase, and schema.org) shows that hierarchy-based approaches provide more accurate results when picking entity types to be displayed to the end-user while still being highly scalable.", 
                    "author": "Alberto Tonon, Michele Catasta, Gianluca Demartini, Philippe Cudr\u00e9-Mauroux, Karl Aberer", 
                    "author_latex": "Alberto Tonon, Michele Catasta, Gianluca Demartini, Philippe Cudr\\'e-Mauroux, Karl Aberer", 
                    "uri": "#TRank%3A_Ranking_Entity_Types_Using_the_Web_of_Data", 
                    "authors": [
                        {
                            "name": "Alberto Tonon", 
                            "uri": "#Alberto_Tonon"
                        }, 
                        {
                            "name": "Michele Catasta", 
                            "uri": "#Michele_Catasta"
                        }, 
                        {
                            "name": "Gianluca Demartini", 
                            "uri": "#Gianluca_Demartini"
                        }, 
                        {
                            "name": "Philippe Cudr\u00e9-Mauroux", 
                            "uri": "#Philippe_Cudr%C3%A9-Mauroux"
                        }, 
                        {
                            "name": "Karl Aberer", 
                            "uri": "#Karl_Aberer"
                        }
                    ], 
                    "link_local": "82180625-trank-ranking-entity-types-using-the-web-of-data.pdf", 
                    "pages": "625-640"
                }, 
                {
                    "page_start": "641", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180641-dynamite-parallel-materialization-of-dynamic-rdf-data.pdf", 
                    "title": "DynamiTE: Parallel Materialization of Dynamic RDF Data", 
                    "abstract": "One of the main advantages of using semantically annotated data is that machines can reason on it, deriving implicit knowledge from explicit information. In this context, materializing every possible implicit derivation from a given input can be computationally expensive, especially when considering large data volumes. Most of the solutions that address this problem rely on the assumption that the information is static, i.e., that it does not change, or changes very infrequently. However, the Web is extremely dynamic: online newspapers, blogs, social networks, etc., are frequently changed so that outdated information is removed and replaced with fresh data. This demands for a materialization that is not only scalable, but also reactive to changes. In this paper, we consider the problem of incremental materialization, that is, how to update the materialized derivations when new data is added or removed. To this purpose, we consider the \u03c1df RDFS fragment [12], and present a parallel system that implements a number of algorithms to quickly recalculate the derivation. In case new data is added, our system uses a parallel version of the well-known semi-naive evaluation of Datalog. In case of removals, we have implemented two algorithms, one based on previous theoretical work, and another one that is more e\ufb03cient since it does not require a complete scan of the input. We have evaluated the performance using a prototype system called DynamiTE , which organizes the knowledge bases with a number of indices to facilitate the query process and exploits parallelism to improve the performance. The results show that our methods are indeed capable to recalculate the derivation in a short time, opening the door to reasoning on much more dynamic data than is currently possible.", 
                    "author": "Jacopo Urbani, Alessandro Margara, Ceriel Jacobs, Frank Van Harmelen, Henri Bal", 
                    "author_latex": "Jacopo Urbani, Alessandro Margara, Ceriel Jacobs, Frank Van Harmelen, Henri Bal", 
                    "uri": "#DynamiTE%3A_Parallel_Materialization_of_Dynamic_RDF_Data", 
                    "authors": [
                        {
                            "name": "Jacopo Urbani", 
                            "uri": "#Jacopo_Urbani"
                        }, 
                        {
                            "name": "Alessandro Margara", 
                            "uri": "#Alessandro_Margara"
                        }, 
                        {
                            "name": "Ceriel Jacobs", 
                            "uri": "#Ceriel_Jacobs"
                        }, 
                        {
                            "name": "Frank Van Harmelen", 
                            "uri": "#Frank_Van_Harmelen"
                        }, 
                        {
                            "name": "Henri Bal", 
                            "uri": "#Henri_Bal"
                        }
                    ], 
                    "link_local": "82180641-dynamite-parallel-materialization-of-dynamic-rdf-data.pdf", 
                    "pages": "641-656"
                }, 
                {
                    "page_start": "657", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180657-discovering-missing-semantic-relations-between-entities-in-wikipedia.pdf", 
                    "title": "Discovering Missing Semantic Relations between Entities in Wikipedia", 
                    "abstract": "Wikipedia\u2019s infoboxes contain rich structured information of various entities, which have been explored by the DBpedia pro ject to generate large scale Linked Data sets. Among all the infobox attributes, those attributes having hyperlinks in its values identify semantic relations between entities, which are important for creating RDF links between DBpedia\u2019s instances. However, quite a few hyperlinks have not been anotated by editors in infoboxes, which causes lots of relations between entities being missing in Wikipedia. In this paper, we propose an approach for automatically discovering the missing entity links in Wikipedia\u2019s infoboxes, so that the missing semantic relations between entities can be established. Our approach \ufb01rst identi\ufb01es entity mentions in the given infoboxes, and then computes several features to estimate the possibilities that a given attribute value might link to a candidate entity. A learning model is used to obtain the weights of di\ufb00erent features, and predict the destination entity for each attribute value. We evaluated our approach on the English Wikipedia data, the experimental results show that our approach can e\ufb00ectively \ufb01nd the missing relations between entities, and it signi\ufb01cantly outperforms the baseline methods in terms of both precision and recall.", 
                    "author": "Mengling Xu, Zhichun Wang, Rongfang Bie, Juanzi Li, Chen Zheng, Wantian Ke, Mingquan Zhou", 
                    "author_latex": "Mengling Xu, Zhichun Wang, Rongfang Bie, Juanzi Li, Chen Zheng, Wantian Ke, Mingquan Zhou", 
                    "uri": "#Discovering_Missing_Semantic_Relations_between_Entities_in_Wikipedia", 
                    "authors": [
                        {
                            "name": "Mengling Xu", 
                            "uri": "#Mengling_Xu"
                        }, 
                        {
                            "name": "Zhichun Wang", 
                            "uri": "#Zhichun_Wang"
                        }, 
                        {
                            "name": "Rongfang Bie", 
                            "uri": "#Rongfang_Bie"
                        }, 
                        {
                            "name": "Juanzi Li", 
                            "uri": "#Juanzi_Li"
                        }, 
                        {
                            "name": "Chen Zheng", 
                            "uri": "#Chen_Zheng"
                        }, 
                        {
                            "name": "Wantian Ke", 
                            "uri": "#Wantian_Ke"
                        }, 
                        {
                            "name": "Mingquan Zhou", 
                            "uri": "#Mingquan_Zhou"
                        }
                    ], 
                    "link_local": "82180657-discovering-missing-semantic-relations-between-entities-in-wikipedia.pdf", 
                    "pages": "657-670"
                }, 
                {
                    "page_start": "671", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180671-infrastructure-for-efficient-exploration-of-large-scale-linked-data-via-contextual-tag-clouds.pdf", 
                    "title": "Infrastructure for Efficient Exploration of Large Scale Linked Data via Contextual Tag Clouds", 
                    "abstract": "In this paper we present the infrastructure of the contextual tag cloud system which can execute large volumes of queries about the number of instances that use particular ontological terms. The contextual tag cloud system is a novel application that helps users explore a large scale RDF dataset: the tags are ontological terms (classes and properties), the context is a set of tags that de\ufb01nes a subset of instances, and the font sizes re\ufb02ect the number of instances that use each tag. It visualizes the patterns of instances speci\ufb01ed by the context a user constructs. Given a request with a speci\ufb01c context, the system needs to quickly \ufb01nd what other tags the instances in the context use, and how many instances in the context use each tag. The key question we answer in this paper is how to scale to Linked Data; in particular we use a dataset with 1.4 billion triples and over 380,000 tags. This is complicated by the fact that the calculation should, when directed by the user, consider the entailment of taxonomic and/or domain/range axioms in the ontology. We combine a scalable preprocessing approach with a specially-constructed inverted index and use three approaches to prune unnecessary counts for faster intersection computations. We compare our system with a state-of-the-art triple store, examine how pruning rules interact with inference and analyze our design choices.", 
                    "author": "Xingjian Zhang, Dezhao Song, Sambhawa Priya, Jeff Heflin", 
                    "author_latex": "Xingjian Zhang, Dezhao Song, Sambhawa Priya, Jeff Heflin", 
                    "uri": "#Infrastructure_for_Efficient_Exploration_of_Large_Scale_Linked_Data_via_Contextual_Tag_Clouds", 
                    "authors": [
                        {
                            "name": "Xingjian Zhang", 
                            "uri": "#Xingjian_Zhang"
                        }, 
                        {
                            "name": "Dezhao Song", 
                            "uri": "#Dezhao_Song"
                        }, 
                        {
                            "name": "Sambhawa Priya", 
                            "uri": "#Sambhawa_Priya"
                        }, 
                        {
                            "name": "Jeff Heflin", 
                            "uri": "#Jeff_Heflin"
                        }
                    ], 
                    "link_local": "82180671-infrastructure-for-efficient-exploration-of-large-scale-linked-data-via-contextual-tag-clouds.pdf", 
                    "pages": "671-686"
                }, 
                {
                    "page_start": "687", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180687-statistical-knowledge-patterns-identifying-synonymous-relations-in-large-linked-datasets.pdf", 
                    "title": "Statistical Knowledge Patterns: Identifying Synonymous Relations in Large Linked Datasets", 
                    "abstract": "The Web of Data is a rich common resource with billions of triples available in thousands of datasets and individual Web documents created by both expert and non-expert ontologists. A common problem is the imprecision in the use of vocabularies: annotators can misunderstand the semantics of a class or property or may not be able to \ufb01nd the right objects to annotate with. This decreases the quality of data and may eventually hamper its usability over large scale. This paper describes Statistical Knowledge Patterns (SKP) as a means to address this issue. SKPs encapsulate key information about ontology classes, including synonymous properties in (and across) datasets, and are automatically generated based on statistical data analysis. SKPs can be e\ufb00ectively used to automatically normalise data, and hence increase recall in querying. Both pattern extraction and pattern usage are completely automated. The main bene\ufb01ts of SKPs are that: (1) their structure allows for both accurate query expansion and restriction; (2) they are context dependent, hence they describe the usage and meaning of properties in the context of a particular class; and (3) they can be generated o\ufb04ine, hence the equivalence among relations can be used e\ufb03ciently at run time.", 
                    "author": "Ziqi Zhang, Anna Lisa Gentile, Eva Blomqvist, Isabelle Augenstein, Fabio Ciravegna", 
                    "author_latex": "Ziqi Zhang, Anna Lisa Gentile, Eva Blomqvist, Isabelle Augenstein, Fabio Ciravegna", 
                    "uri": "#Statistical_Knowledge_Patterns%3A_Identifying_Synonymous_Relations_in_Large_Linked_Datasets", 
                    "authors": [
                        {
                            "name": "Ziqi Zhang", 
                            "uri": "#Ziqi_Zhang"
                        }, 
                        {
                            "name": "Anna Lisa Gentile", 
                            "uri": "#Anna_Lisa_Gentile"
                        }, 
                        {
                            "name": "Eva Blomqvist", 
                            "uri": "#Eva_Blomqvist"
                        }, 
                        {
                            "name": "Isabelle Augenstein", 
                            "uri": "#Isabelle_Augenstein"
                        }, 
                        {
                            "name": "Fabio Ciravegna", 
                            "uri": "#Fabio_Ciravegna"
                        }
                    ], 
                    "link_local": "82180687-statistical-knowledge-patterns-identifying-synonymous-relations-in-large-linked-datasets.pdf", 
                    "pages": "687-702"
                }, 
                {
                    "page_start": "703", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82180703-complete-query-answering-over-horn-ontologies-using-a-triple-store.pdf", 
                    "title": "Complete Query Answering Over Horn Ontologies Using a Triple Store", 
                    "abstract": "In our previous work, we showed how a scalable OWL 2 RL reasoner can be used to compute both lower and upper bound query answers over very large datasets and arbitrary OWL 2 ontologies. However, when these bounds do not coincide, there still remain a number of possible answer tuples whose status is not determined. In this paper, we show how in the case of Horn ontologies one can exploit the lower and upper bounds computed by the RL reasoner to e\ufb03ciently identify a subset of the data and ontology that is large enough to resolve the status of these tuples, yet small enough so that the status can be computed using a fully-\ufb02edged OWL 2 reasoner. The resulting hybrid approach has enabled us to compute exact answers to queries over datasets and ontologies where previously only approximate query answering was possible.", 
                    "author": "Yujiao Zhou, Yavor Nenov, Bernardo Cuenca Grau, Ian Horrocks", 
                    "author_latex": "Yujiao Zhou, Yavor Nenov, Bernardo Cuenca Grau, Ian Horrocks", 
                    "uri": "#Complete_Query_Answering_Over_Horn_Ontologies_Using_a_Triple_Store", 
                    "authors": [
                        {
                            "name": "Yujiao Zhou", 
                            "uri": "#Yujiao_Zhou"
                        }, 
                        {
                            "name": "Yavor Nenov", 
                            "uri": "#Yavor_Nenov"
                        }, 
                        {
                            "name": "Bernardo Cuenca Grau", 
                            "uri": "#Bernardo_Cuenca_Grau"
                        }, 
                        {
                            "name": "Ian Horrocks", 
                            "uri": "#Ian_Horrocks"
                        }
                    ], 
                    "link_local": "82180703-complete-query-answering-over-horn-ontologies-using-a-triple-store.pdf", 
                    "pages": "703-718"
                }
            ], 
            "label": "Research Track Paper"
        }, 
        {
            "papers": [
                {
                    "page_start": "1", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190001-social-listening-of-city-scale-events-using-the-streaming-linked-data-framework.pdf", 
                    "title": "Social listening of City Scale Events using the Streaming Linked Data Framework", 
                    "abstract": "City-scale events may easily attract half a million of visitors in hundreds of venues over just a few days. Which are the most attended venues? What do visitors think about them? How do they feel before, during and after the event? These are few of the questions a city-scale event manger would like to see answered in real-time. In this paper, we report on our experience in social listening of two city-scale events (London Olympic Games 2012, and Milano Design Week 2013) using the Streaming Linked Data Framework.", 
                    "author": "Marco Balduini, Emanuele Della Valle, Daniele Dell'Aglio, Themis Palpanas, Mikalai Tsytsarau, Cristian Confalonieri", 
                    "author_latex": "Marco Balduini, Emanuele Della Valle, Daniele Dell'Aglio, Themis Palpanas, Mikalai Tsytsarau, Cristian Confalonieri", 
                    "uri": "#Social_listening_of_City_Scale_Events_using_the_Streaming_Linked_Data_Framework", 
                    "authors": [
                        {
                            "name": "Marco Balduini", 
                            "uri": "#Marco_Balduini"
                        }, 
                        {
                            "name": "Emanuele Della Valle", 
                            "uri": "#Emanuele_Della_Valle"
                        }, 
                        {
                            "name": "Daniele Dell'Aglio", 
                            "uri": "#Daniele_Dell%27Aglio"
                        }, 
                        {
                            "name": "Themis Palpanas", 
                            "uri": "#Themis_Palpanas"
                        }, 
                        {
                            "name": "Mikalai Tsytsarau", 
                            "uri": "#Mikalai_Tsytsarau"
                        }, 
                        {
                            "name": "Cristian Confalonieri", 
                            "uri": "#Cristian_Confalonieri"
                        }
                    ], 
                    "link_local": "82190001-social-listening-of-city-scale-events-using-the-streaming-linked-data-framework.pdf", 
                    "pages": "1-16"
                }, 
                {
                    "page_start": "17", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190017-deployment-of-rdfa-microdata-and-microformats-on-the-web-a-quantitative-analysis.pdf", 
                    "title": "Deployment of RDFa, Microdata, and Microformats on the Web \u2013 A Quantitative Analysis", 
                    "abstract": "More and more websites embed structured data describing for instance products, reviews, blog posts, people, organizations, events, and cooking recipes into their HTML pages using markup standards such as Microformats, Microdata and RDFa. This development has accelerated in the last two years as major Web companies, such as Google, Facebook, Yahoo!, and Microsoft, have started to use the embedded data within their applications. In this paper, we analyze the adoption of RDFa, Microdata, and Microformats across the Web. Our study is based on a large public Web crawl dating from early 2012 and consisting of 3 billion HTML pages which originate from over 40 million websites. The analysis reveals the deployment of the different markup standards, the main topical areas of the published data as well as the different vocabularies that are used within each topical area to represent data. What distinguishes our work from earlier studies, published by the large Web companies, is that the analyzed crawl as well as the extracted data are publicly available. This allows our \ufb01ndings to be veri\ufb01ed and to be used as starting points for further domain-speci\ufb01c investigations as well as for focused information extraction endeavors.", 
                    "author": "Christian Bizer, Kai Eckert, Robert Meusel, Hannes M\u00fchleisen, Michael Schuhmacher, Johanna V\u00f6lker", 
                    "author_latex": "Christian Bizer, Kai Eckert, Robert Meusel, Hannes M\\\"uhleisen, Michael Schuhmacher, Johanna V\\\"olker", 
                    "uri": "#Deployment_of_RDFa%2C_Microdata%2C_and_Microformats_on_the_Web_%E2%80%93_A_Quantitative_Analysis", 
                    "authors": [
                        {
                            "name": "Christian Bizer", 
                            "uri": "#Christian_Bizer"
                        }, 
                        {
                            "name": "Kai Eckert", 
                            "uri": "#Kai_Eckert"
                        }, 
                        {
                            "name": "Robert Meusel", 
                            "uri": "#Robert_Meusel"
                        }, 
                        {
                            "name": "Hannes M\u00fchleisen", 
                            "uri": "#Hannes_M%C3%BChleisen"
                        }, 
                        {
                            "name": "Michael Schuhmacher", 
                            "uri": "#Michael_Schuhmacher"
                        }, 
                        {
                            "name": "Johanna V\u00f6lker", 
                            "uri": "#Johanna_V%C3%B6lker"
                        }
                    ], 
                    "link_local": "82190017-deployment-of-rdfa-microdata-and-microformats-on-the-web-a-quantitative-analysis.pdf", 
                    "pages": "17-32"
                }, 
                {
                    "page_start": "33", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190033-entity-recommendations-in-web-search.pdf", 
                    "title": "Entity recommendations in Web Search", 
                    "abstract": "While some web search users know exactly what they are looking for, others are willing to explore topics related to an initial interest. Often, the user\u2019s initial interest can be uniquely linked to an entity in a knowledge base. In this case, it is natural to recommend the explicitly linked entities for further exploration. In real world knowledge bases, however, the number of linked entities may be very large and not all related entities may be equally relevant. Thus, there is a need for ranking related entities. In this paper, we describe Spark, a recommendation engine that links a user\u2019s initial query to an entity within a knowledge base and provides a ranking of the related entities. Spark extracts several signals from a variety of data sources, including Yahoo! Web Search, Twitter, and Flickr, using a large cluster of computers running Hadoop. These signals are combined with a machine learned ranking model in order to produce a \ufb01nal recommendation of entities to user queries. This system is currently powering Yahoo! Web Search result pages.", 
                    "author": "Roi Blanco, Berkant Barla Cambazoglu, Peter Mika, Nicolas Torzec", 
                    "author_latex": "Roi Blanco, Berkant Barla Cambazoglu, Peter Mika, Nicolas Torzec", 
                    "uri": "#Entity_recommendations_in_Web_Search", 
                    "authors": [
                        {
                            "name": "Roi Blanco", 
                            "uri": "#Roi_Blanco"
                        }, 
                        {
                            "name": "Berkant Barla Cambazoglu", 
                            "uri": "#Berkant_Barla_Cambazoglu"
                        }, 
                        {
                            "name": "Peter Mika", 
                            "uri": "#Peter_Mika"
                        }, 
                        {
                            "name": "Nicolas Torzec", 
                            "uri": "#Nicolas_Torzec"
                        }
                    ], 
                    "link_local": "82190033-entity-recommendations-in-web-search.pdf", 
                    "pages": "33-48"
                }, 
                {
                    "page_start": "49", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190049-the-energy-management-adviser-at-edf.pdf", 
                    "title": "The Energy Management Adviser at EDF", 
                    "abstract": "The EMA (Energy Management Adviser) aims to produce personalised energy saving advice for EDF\u2019s customers. The advice takes the form of one or more \u2018tips\u2019, and personalisation is achieved using semantic technologies: customers are described using RDF, an OWL ontology provides a conceptual model of the relevant domain (housing, environment, and so on) and the di\ufb00erent kinds of tips, and SPARQL query answering is used to identify relevant tips. The current prototype provides tips to more than 300,000 EDF customers in France at least twice a year. The main challenges for our future work include providing a timely service for all of the 35 million EDF customers in France, simplifying the system\u2019s maintenance, and providing new ways for interacting with customers such as via a Web site.", 
                    "author": "Pierre Chaussecourte, Birte Glimm, Ian Horrocks, Boris Motik, Laurent Pierre", 
                    "author_latex": "Pierre Chaussecourte, Birte Glimm, Ian Horrocks, Boris Motik, Laurent Pierre", 
                    "uri": "#The_Energy_Management_Adviser_at_EDF", 
                    "authors": [
                        {
                            "name": "Pierre Chaussecourte", 
                            "uri": "#Pierre_Chaussecourte"
                        }, 
                        {
                            "name": "Birte Glimm", 
                            "uri": "#Birte_Glimm"
                        }, 
                        {
                            "name": "Ian Horrocks", 
                            "uri": "#Ian_Horrocks"
                        }, 
                        {
                            "name": "Boris Motik", 
                            "uri": "#Boris_Motik"
                        }, 
                        {
                            "name": "Laurent Pierre", 
                            "uri": "#Laurent_Pierre"
                        }
                    ], 
                    "link_local": "82190049-the-energy-management-adviser-at-edf.pdf", 
                    "pages": "49-64"
                }, 
                {
                    "page_start": "65", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190065-incorporating-commercial-and-private-data-into-an-open-linked-data-platform-for-drug-discovery.pdf", 
                    "title": "Incorporating Commercial and Private Data into an Open Linked Data Platform for Drug Discovery", 
                    "abstract": "The Open PHACTS Discovery Platform aims to provide an integrated information space to advance pharmacological research in the area of drug discovery. E\ufb00ective drug discovery requires comprehensive data coverage, i.e. integrating all available sources of pharmacology data. While many relevant data sources are available on the linked open data cloud, their content needs to be combined with that of commercial datasets and the licensing of these commercial datasets respected when providing access to the data. Additionally, pharmaceutical companies have built up their own extensive private data collections that they require to be included in their pharmacological dataspace. In this paper we discuss the challenges of incorporating private and commercial data into a linked dataspace: focusing on the modelling of these datasets and their interlinking. We also present the graph-based access control mechanism that ensures commercial and private datasets are only available to authorized users.", 
                    "author": "Carole Goble, Alasdair J. G. Gray, Lee Harland, Karen Karapetyan, Antonis Loizou, Ivan Mikhailov, Yrjana Rankka, Stefan Senger, Valery Tkachenko, Antony Williams, Egon Willighagen", 
                    "author_latex": "Carole Goble, Alasdair J. G. Gray, Lee Harland, Karen Karapetyan, Antonis Loizou, Ivan Mikhailov, Yrjana Rankka, Stefan Senger, Valery Tkachenko, Antony Williams, Egon Willighagen", 
                    "uri": "#Incorporating_Commercial_and_Private_Data_into_an_Open_Linked_Data_Platform_for_Drug_Discovery", 
                    "authors": [
                        {
                            "name": "Carole Goble", 
                            "uri": "#Carole_Goble"
                        }, 
                        {
                            "name": "Alasdair J. G. Gray", 
                            "uri": "#Alasdair_J._G._Gray"
                        }, 
                        {
                            "name": "Lee Harland", 
                            "uri": "#Lee_Harland"
                        }, 
                        {
                            "name": "Karen Karapetyan", 
                            "uri": "#Karen_Karapetyan"
                        }, 
                        {
                            "name": "Antonis Loizou", 
                            "uri": "#Antonis_Loizou"
                        }, 
                        {
                            "name": "Ivan Mikhailov", 
                            "uri": "#Ivan_Mikhailov"
                        }, 
                        {
                            "name": "Yrjana Rankka", 
                            "uri": "#Yrjana_Rankka"
                        }, 
                        {
                            "name": "Stefan Senger", 
                            "uri": "#Stefan_Senger"
                        }, 
                        {
                            "name": "Valery Tkachenko", 
                            "uri": "#Valery_Tkachenko"
                        }, 
                        {
                            "name": "Antony Williams", 
                            "uri": "#Antony_Williams"
                        }, 
                        {
                            "name": "Egon Willighagen", 
                            "uri": "#Egon_Willighagen"
                        }
                    ], 
                    "link_local": "82190065-incorporating-commercial-and-private-data-into-an-open-linked-data-platform-for-drug-discovery.pdf", 
                    "pages": "65-80"
                }, 
                {
                    "page_start": "81", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190081-when-history-matters-assessing-reliability-for-the-reuse-of-scientific-workflows.pdf", 
                    "title": "When History Matters - Assessing Reliability for the Reuse of Scientific Workflows", 
                    "abstract": "Scienti\ufb01c work\ufb02ows play an important role in computational research as essential artifacts for communicating the methods used to produce research \ufb01ndings. We are witnessing a growing number of efforts that treat work\ufb02ows as \ufb01rst-class artifacts for sharing and exchanging scienti\ufb01c knowledge, either as part of scholarly articles or as standalone ob jects. However, work\ufb02ows are not born to be reliable, which can seriously damage their reusability and trustworthiness as knowledge exchange instruments. Scienti\ufb01c work\ufb02ows are commonly sub ject to decay, which consequently undermines their reliability over their lifetime. The reliability of work\ufb02ows can be notably improved by advocating scientists to preserve a minimal set of information that is essential to assist the interpretations of these work\ufb02ows and hence improve their potential for reproducibility and reusability. In this paper we show how, by measuring and monitoring the completeness and stability of scienti\ufb01c work\ufb02ows over time we are able to provide scientists with a measure of their reliability, supporting the reuse of trustworthy scienti\ufb01c knowledge.", 
                    "author": "Jos\u00e9 Manuel G\u00f3mez-P\u00e9rez, Esteban Garc\u00eda-Cuesta, Aleix Garrido, Jos\u00e9 Enrique Ruiz", 
                    "author_latex": "Jos\\'e Manuel G\\'omez-P\\'erez, Esteban Garc\\'\\ia-Cuesta, Aleix Garrido, Jos\\'e Enrique Ruiz", 
                    "uri": "#When_History_Matters_-_Assessing_Reliability_for_the_Reuse_of_Scientific_Workflows", 
                    "authors": [
                        {
                            "name": "Jos\u00e9 Manuel G\u00f3mez-P\u00e9rez", 
                            "uri": "#Jos%C3%A9_Manuel_G%C3%B3mez-P%C3%A9rez"
                        }, 
                        {
                            "name": "Esteban Garc\u00eda-Cuesta", 
                            "uri": "#Esteban_Garc%C3%ADa-Cuesta"
                        }, 
                        {
                            "name": "Aleix Garrido", 
                            "uri": "#Aleix_Garrido"
                        }, 
                        {
                            "name": "Jos\u00e9 Enrique Ruiz", 
                            "uri": "#Jos%C3%A9_Enrique_Ruiz"
                        }
                    ], 
                    "link_local": "82190081-when-history-matters-assessing-reliability-for-the-reuse-of-scientific-workflows.pdf", 
                    "pages": "81-96"
                }, 
                {
                    "page_start": "97", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190097-integrating-nlp-using-linked-data.pdf", 
                    "title": "Integrating NLP using Linked Data", 
                    "abstract": "We are currently observing a plethora of Natural Language Processing tools and services being made available. Each of the tools and services has its particular strengths and weaknesses, but exploiting the strengths and synergistically combining di\ufb00erent tools is currently an extremely cumbersome and time consuming task. Also, once a particular set of tools is integrated, this integration is not reusable by others. We argue that simplifying the interoperability of di\ufb00erent NLP tools performing similar but also complementary tasks will facilitate the comparability of results and the creation of sophisticated NLP applications. In this paper, we present the NLP Interchange Format (NIF). NIF is based on a Linked Data enabled URI scheme for identifying elements in (hyper-)texts and an ontology for describing common NLP terms and concepts. In contrast to more centralized solutions such as UIMA and GATE, NIF enables the creation of heterogeneous, distributed and loosely coupled NLP applications, which use the Web as an integration platform. We present several use cases of the second version of the NIF speci\ufb01cation (NIF 2.0) and the result of a developer study.", 
                    "author": "Sebastian Hellmann, Jens Lehmann, S\u00f6ren Auer, Martin Br\u00fcmmer", 
                    "author_latex": "Sebastian Hellmann, Jens Lehmann, S\\\"oren Auer, Martin Br\\\"ummer", 
                    "uri": "#Integrating_NLP_using_Linked_Data", 
                    "authors": [
                        {
                            "name": "Sebastian Hellmann", 
                            "uri": "#Sebastian_Hellmann"
                        }, 
                        {
                            "name": "Jens Lehmann", 
                            "uri": "#Jens_Lehmann"
                        }, 
                        {
                            "name": "S\u00f6ren Auer", 
                            "uri": "#S%C3%B6ren_Auer"
                        }, 
                        {
                            "name": "Martin Br\u00fcmmer", 
                            "uri": "#Martin_Br%C3%BCmmer"
                        }
                    ], 
                    "link_local": "82190097-integrating-nlp-using-linked-data.pdf", 
                    "pages": "97-112"
                }, 
                {
                    "page_start": "113", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190113-a-linked-data-driven-and-semantically-enabled-journal-portal-for-scientometrics.pdf", 
                    "title": "A Linked-Data-driven and Semantically-enabled Journal Portal for Scientometrics", 
                    "abstract": "The Semantic Web journal by IOS Press follows a unique open and transparent process during which each submitted manuscript is available online together with the full history of its successive decision statuses, assigned editors, solicited and voluntary reviewers, their full text reviews, and in many cases also the authors\u2019 response letters. Combined with a highly-customized, Drupal-based journal management system, this provides the journal with semantically rich manuscript time lines and networked data about authors, reviewers, and editors. These data are now exposed using a SPARQL endpoint, an extended Bibo ontology, and a modular Linked Data portal that provides interactive scientometrics based on established and new analysis methods. The portal can be customized for other journals as well.", 
                    "author": "Yingjie Hu, Krzysztof Janowicz, Grant Mckenzie, Kunal Sengupta, Pascal Hitzler", 
                    "author_latex": "Yingjie Hu, Krzysztof Janowicz, Grant Mckenzie, Kunal Sengupta, Pascal Hitzler", 
                    "uri": "#A_Linked-Data-driven_and_Semantically-enabled_Journal_Portal_for_Scientometrics", 
                    "authors": [
                        {
                            "name": "Yingjie Hu", 
                            "uri": "#Yingjie_Hu"
                        }, 
                        {
                            "name": "Krzysztof Janowicz", 
                            "uri": "#Krzysztof_Janowicz"
                        }, 
                        {
                            "name": "Grant Mckenzie", 
                            "uri": "#Grant_Mckenzie"
                        }, 
                        {
                            "name": "Kunal Sengupta", 
                            "uri": "#Kunal_Sengupta"
                        }, 
                        {
                            "name": "Pascal Hitzler", 
                            "uri": "#Pascal_Hitzler"
                        }
                    ], 
                    "link_local": "82190113-a-linked-data-driven-and-semantically-enabled-journal-portal-for-scientometrics.pdf", 
                    "pages": "113-128"
                }, 
                {
                    "page_start": "129", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190129-cross-language-semantic-retrieval-and-linking-of-e-gov-services.pdf", 
                    "title": "Cross-language Semantic Retrieval and Linking of E-gov Services", 
                    "abstract": "Public administrations are aware of the advantages of sharing Open Government Data in terms of transparency, development of improved services, collaboration between stakeholders, and spurring new economic activities. Initiatives for the publication and interlinking of government service catalogs as Linked Open Data (lod) support the interoperability among European administrations and improve the capability of foreign citizens to access services across Europe. However, linking service catalogs to reference lod catalogs requires a signi\ufb01cant e\ufb00ort from local administrations, preventing the uptake of interoperable solutions at a large scale. The web application presented in this paper is named CroSeR (Cross-language Service Retriever) and supports public bodies in the process of linking their own service catalogs to the lod cloud. CroSeR supports di\ufb00erent European languages and adopts a semantic representation of e-gov services based on Wikipedia. CroSeR tries to overcome problems related to the short textual descriptions associated to a service by embodying a semantic annotation algorithm that enriches service labels with emerging Wikipedia concepts related to the service. An experimental evaluation carried-out on e-gov service catalogs in \ufb01ve di\ufb00erent languages shows the e\ufb00ectiveness of our model.", 
                    "author": "Fedelucio Narducci, Matteo Palmonari, Giovanni Semeraro", 
                    "author_latex": "Fedelucio Narducci, Matteo Palmonari, Giovanni Semeraro", 
                    "uri": "#Cross-language_Semantic_Retrieval_and_Linking_of_E-gov_Services", 
                    "authors": [
                        {
                            "name": "Fedelucio Narducci", 
                            "uri": "#Fedelucio_Narducci"
                        }, 
                        {
                            "name": "Matteo Palmonari", 
                            "uri": "#Matteo_Palmonari"
                        }, 
                        {
                            "name": "Giovanni Semeraro", 
                            "uri": "#Giovanni_Semeraro"
                        }
                    ], 
                    "link_local": "82190129-cross-language-semantic-retrieval-and-linking-of-e-gov-services.pdf", 
                    "pages": "129-144"
                }, 
                {
                    "page_start": "145", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190145-using-the-past-to-explain-the-present-interlinking-current-affairs-with-archives-via-the-semantic-web.pdf", 
                    "title": "Using the past to explain the present: interlinking current affairs with archives via the Semantic Web", 
                    "abstract": "The BBC has a very large archive of programmes, covering a wide range of topics. This archive holds a signi\ufb01cant part of the BBC\u2019s institutional memory and is an important part of the cultural history of the United Kingdom and the rest of the world. These programmes, or parts of them, can help provide valuable context and background for current news events. However the BBC\u2019s archive catalogue is not a complete record of everything that was ever broadcast. For example, it excludes the BBC World Service, which has been broadcasting since 1932. This makes the discovery of content within these parts of the archive very di\ufb03cult. In this paper we describe a system based on Semantic Web technologies which helps us to quickly locate content related to current news events within those parts of the BBC\u2019s archive with little or no pre-existing metadata. This system is driven by automated interlinking of archive content with the Semantic Web, user validations of the resulting data and topic extraction from live BBC News subtitles. The resulting inter-links between live news subtitles and the BBC\u2019s archive are used in a dynamic visualisation enabling users to quickly locate relevant content. This content can then be used by journalists and editors to provide historical context, background information and supporting content around current a\ufb00airs.", 
                    "author": "Yves Raimond, Michael Smethurst, Andrew McParland, Christopher Lowis", 
                    "author_latex": "Yves Raimond, Michael Smethurst, Andrew McParland, Christopher Lowis", 
                    "uri": "#Using_the_past_to_explain_the_present%3A_interlinking_current_affairs_with_archives_via_the_Semantic_Web", 
                    "authors": [
                        {
                            "name": "Yves Raimond", 
                            "uri": "#Yves_Raimond"
                        }, 
                        {
                            "name": "Michael Smethurst", 
                            "uri": "#Michael_Smethurst"
                        }, 
                        {
                            "name": "Andrew McParland", 
                            "uri": "#Andrew_McParland"
                        }, 
                        {
                            "name": "Christopher Lowis", 
                            "uri": "#Christopher_Lowis"
                        }
                    ], 
                    "link_local": "82190145-using-the-past-to-explain-the-present-interlinking-current-affairs-with-archives-via-the-semantic-web.pdf", 
                    "pages": "145-160"
                }, 
                {
                    "page_start": "161", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190161-publishing-the-norwegian-petroleum-directorate-s-factpages-as-semantic-web-data.pdf", 
                    "title": "Publishing the Norwegian Petroleum Directorate\u2019s FactPages as Semantic Web Data", 
                    "abstract": "This paper motivates, documents and evaluates the process and results of converting the Norwegian Petroleum Directorate\u2019s Fact-Pages, a well-known and diverse set of tabular data, but with little and incomplete schema information, stepwise into other representations where in each step more semantics is added to the dataset. The di\ufb00erent representations we consider are a regular relational database, a linked open data dataset, and an ontology. For each conversion step we explain and discuss necessary design choices which are due to the speci\ufb01c shape of the dataset, but also those due to the characteristics and idiosyncrasies of the representation formats. We additionally evaluate the output, performance and cost of querying the di\ufb00erent formats using questions provided by users of the FactPages.", 
                    "author": "Martin G. Skj\u00e6veland, Espen H. Lian, Ian Horrocks", 
                    "author_latex": "Martin G. Skj\\{\\ae\\}veland, Espen H. Lian, Ian Horrocks", 
                    "uri": "#Publishing_the_Norwegian_Petroleum_Directorate%E2%80%99s_FactPages_as_Semantic_Web_Data", 
                    "authors": [
                        {
                            "name": "Martin G. Skj\u00e6veland", 
                            "uri": "#Martin_G._Skj%C3%A6veland"
                        }, 
                        {
                            "name": "Espen H. Lian", 
                            "uri": "#Espen_H._Lian"
                        }, 
                        {
                            "name": "Ian Horrocks", 
                            "uri": "#Ian_Horrocks"
                        }
                    ], 
                    "link_local": "82190161-publishing-the-norwegian-petroleum-directorate-s-factpages-as-semantic-web-data.pdf", 
                    "pages": "161-176"
                }, 
                {
                    "page_start": "177", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190177-real-time-urban-monitoring-in-dublin-using-semantic-and-stream-technologies.pdf", 
                    "title": "Real-time Urban Monitoring in Dublin using Semantic and Stream Technologies", 
                    "abstract": "Several sources of information, from people, systems, things, are already available in most modern cities. Processing these continuous \ufb02ows of information and capturing insight poses unique technical challenges that span from response time constraints to data heterogeneity, in terms of format and throughput. To tackle these problems, we focus on a novel prototype to ease real-time monitoring and decision-making processes for the City of Dublin with three main original technical aspects: (i) an extension to SPARQL to support ef\ufb01cient querying of heterogeneous streams; (ii) a query execution framework and runtime environment based on IBM InfoSphere Streams, a high-performance, industrial strength, stream processing engine; (iii) a hybrid RDFS reasoner, optimized for our stream processing execution framework. Our approach has been validated with real data collected on the \ufb01eld, as shown in our Dublin City video demonstration. Results indicate that real-time processing of city information streams based on semantic technologies is indeed not only possible, but also ef\ufb01cient, scalable and low-latency.", 
                    "author": "Simone Tallevi-Diotallevi, Spyros Kotoulas, Luca Foschini, Freddy Lecue, Antonio Corradi", 
                    "author_latex": "Simone Tallevi-Diotallevi, Spyros Kotoulas, Luca Foschini, Freddy Lecue, Antonio Corradi", 
                    "uri": "#Real-time_Urban_Monitoring_in_Dublin_using_Semantic_and_Stream_Technologies", 
                    "authors": [
                        {
                            "name": "Simone Tallevi-Diotallevi", 
                            "uri": "#Simone_Tallevi-Diotallevi"
                        }, 
                        {
                            "name": "Spyros Kotoulas", 
                            "uri": "#Spyros_Kotoulas"
                        }, 
                        {
                            "name": "Luca Foschini", 
                            "uri": "#Luca_Foschini"
                        }, 
                        {
                            "name": "Freddy Lecue", 
                            "uri": "#Freddy_Lecue"
                        }, 
                        {
                            "name": "Antonio Corradi", 
                            "uri": "#Antonio_Corradi"
                        }
                    ], 
                    "link_local": "82190177-real-time-urban-monitoring-in-dublin-using-semantic-and-stream-technologies.pdf", 
                    "pages": "177-192"
                }, 
                {
                    "page_start": "193", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190193-using-semantic-web-in-icd-11-three-years-down-the-road.pdf", 
                    "title": "Using Semantic Web in ICD-11: Three Years Down the Road", 
                    "abstract": "The World Health Organization is using Semantic Web technologies in the development of the 11th revision of the International Classi\ufb01cation of Diseases (ICD-11). Health o\ufb03cials use ICD in all United Nations member countries to compile basic health statistics, to monitor health-related spending, and to inform policy makers. In 2010, we published a paper in the ISWC In Use track reporting on our experience in the \ufb01rst six months with building and deploying iCAT, a Semantic Web platform to support the collaborative authoring of ICD-11. Three years since our original publication, 270 domain experts around the world have used iCAT to author more than 45,000 classes, to perform more than 260,000 changes, and to create more than 17,000 links to external medical terminologies. During the last three years, the collaboration processes, modeling and tooling have evolved signi\ufb01cantly, and we have learned important lessons, which we will report in this paper. We describe the bene\ufb01ts of using semantic technologies as an infrastructure, which proved to be critical in making support for this rapid evolution possible. To our knowledge, this e\ufb00ort is the only real-world pro ject supporting the collaborative authoring of ontologies at this scale, and which, at the same time, has a high visibility and impact for the health care around the world. We believe that the insights that we gained and the lessons that we learned after four years into this large-scale pro ject will be useful to others who need to support similar collaborative pro jects.", 
                    "author": "Tania Tudorache, Csongor I Nyulas, Natasha F. Noy, Mark Musen", 
                    "author_latex": "Tania Tudorache, Csongor I Nyulas, Natasha F. Noy, Mark Musen", 
                    "uri": "#Using_Semantic_Web_in_ICD-11%3A_Three_Years_Down_the_Road", 
                    "authors": [
                        {
                            "name": "Tania Tudorache", 
                            "uri": "#Tania_Tudorache"
                        }, 
                        {
                            "name": "Csongor I Nyulas", 
                            "uri": "#Csongor_I_Nyulas"
                        }, 
                        {
                            "name": "Natasha F. Noy", 
                            "uri": "#Natasha_F._Noy"
                        }, 
                        {
                            "name": "Mark Musen", 
                            "uri": "#Mark_Musen"
                        }
                    ], 
                    "link_local": "82190193-using-semantic-web-in-icd-11-three-years-down-the-road.pdf", 
                    "pages": "193-208"
                }, 
                {
                    "page_start": "209", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190209-semantic-data-and-models-sharing-in-systems-biology-the-just-enough-results-model-and-the-seek-platform.pdf", 
                    "title": "Semantic Data and Models Sharing in systems Biology: The Just Enough Results Model and the SEEK Platform", 
                    "abstract": "Research in Systems Biology involves integrating data and knowledge about the dynamic processes in biological systems in order to understand and model them. Semantic web technologies should be ideal for exploring the complex networks of genes, proteins and metabolites that interact, but much of this data is not natively available to the semantic web. Data is typically collected and stored with free-text annotations in spreadsheets, many of which do not conform to existing metadata standards and are often not publica lly released. Along with initiatives to promote more data sharing, one of the main challenges is therefore to semantically annotate and extract this data so that it is available to the research community. Data annotation and curation are expensive and undervalued tasks that have enormous benefits to the discipline as a whole, but fewer benefits to the individual data producers. By embedding semantic annotation into spreadsheets, however, and automat ically extracting this data into RDF at the time of repository submission, the process of producing standards-compliant data, that is available for semantic web querying, can be achieved without adding additional overheads to laboratory data management. This paper describes these strategies in the context of semantic data management in the SEEK. The SEEK is a web-based resource for sharing and exchanging Systems Biology data and models that is underpinned by the JERM ontology (Just Enough Results Model), which describes the relationships between data, models, protocols and experiments. The SEEK was originally developed for SysMO, a large European Systems Biology consortium studying micro-organisms, but it has since had widespread adoption across European Systems Biology.", 
                    "author": "Katherine Wolstencroft, Stuart Owen, Olga Krebs, Quyen Ngyuen, Jacky. L. Snoep, Wolfgang Mueller, Carole Goble", 
                    "author_latex": "Katherine Wolstencroft, Stuart Owen, Olga Krebs, Quyen Ngyuen, Jacky. L. Snoep, Wolfgang Mueller, Carole Goble", 
                    "uri": "#Semantic_Data_and_Models_Sharing_in_systems_Biology%3A_The_Just_Enough_Results_Model_and_the_SEEK_Platform", 
                    "authors": [
                        {
                            "name": "Katherine Wolstencroft", 
                            "uri": "#Katherine_Wolstencroft"
                        }, 
                        {
                            "name": "Stuart Owen", 
                            "uri": "#Stuart_Owen"
                        }, 
                        {
                            "name": "Olga Krebs", 
                            "uri": "#Olga_Krebs"
                        }, 
                        {
                            "name": "Quyen Ngyuen", 
                            "uri": "#Quyen_Ngyuen"
                        }, 
                        {
                            "name": "Jacky. L. Snoep", 
                            "uri": "#Jacky._L._Snoep"
                        }, 
                        {
                            "name": "Wolfgang Mueller", 
                            "uri": "#Wolfgang_Mueller"
                        }, 
                        {
                            "name": "Carole Goble", 
                            "uri": "#Carole_Goble"
                        }
                    ], 
                    "link_local": "82190209-semantic-data-and-models-sharing-in-systems-biology-the-just-enough-results-model-and-the-seek-platform.pdf", 
                    "pages": "209-224"
                }, 
                {
                    "page_start": "225", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190225-reasoning-on-crowd-sourced-semantic-annotations-to-facilitate-cataloguing-of-3d-artefacts-in-the-cultural-heritage-domain.pdf", 
                    "title": "Reasoning on crowd-sourced semantic annotations to facilitate cataloguing of 3D artefacts in the cultural heritage domain", 
                    "abstract": "The 3D Semantic Annotation (3DSA) system expedites the classi\ufb01cation of 3D digital surrogates from the cultural heritage domain, by leveraging crowd-sourced semantic annotations. More speci\ufb01cally, the 3DSA system generates high-level classi\ufb01cations of 3D ob jects by applying rule-based reasoning across community-generated annotations and low-level shape and size attributes. This paper describes a particular use of the 3DSA system \u2013 cataloguing Greek pottery. It also describes our novel approach to rule-based reasoning that is modelled on concepts inspired from Markov logic networks. Our evaluation of this approach demonstrates its e\ufb03ciency, accuracy and versatility, compared to classical rule-based reasoning.", 
                    "author": "Chih-Hao Yu, Tudor Groza, Jane Hunter", 
                    "author_latex": "Chih-Hao Yu, Tudor Groza, Jane Hunter", 
                    "uri": "#Reasoning_on_crowd-sourced_semantic_annotations_to_facilitate_cataloguing_of_3D_artefacts_in_the_cultural_heritage_domain", 
                    "authors": [
                        {
                            "name": "Chih-Hao Yu", 
                            "uri": "#Chih-Hao_Yu"
                        }, 
                        {
                            "name": "Tudor Groza", 
                            "uri": "#Tudor_Groza"
                        }, 
                        {
                            "name": "Jane Hunter", 
                            "uri": "#Jane_Hunter"
                        }
                    ], 
                    "link_local": "82190225-reasoning-on-crowd-sourced-semantic-annotations-to-facilitate-cataloguing-of-3d-artefacts-in-the-cultural-heritage-domain.pdf", 
                    "pages": "225-240"
                }, 
                {
                    "page_start": "241", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190241-using-linked-data-to-evaluate-the-impact-of-research-and-development-in-europe-a-structural-equation-model.pdf", 
                    "title": "Using Linked Data to evaluate the impact of Research and Development in Europe: a Structural Equation Model", 
                    "abstract": "Europe has a high impact on the global biomedical literature, having contributed with a growing number of research articles and a signi\ufb01cant citation impact. However, the impact of research and development generated by European countries on economic, educational and healthcare performance is poorly understood. The recent Linking Open Data (LOD) project has made a lot of data sources publicly available and in human-readable formats. In this paper, we demonstrate the utility of LOD in assessing the impact of Research and Development (R&D) on the economic, education and healthcare performance in Europe. We extract relevant variables from two LOD datasets, namely World Bank and Eurostat. We analyze the data for 20 out of the 27 European countries over a span of 10 years (1999 to 2009). We use a Structural Equation Modeling (SEM) approach to quantify the impact of R&D on the different measures. We perform different exploratory and con\ufb01rmatory factorial analysis evaluations which gives rise to four latent variables that are included in the model: (i) Research and Development (R&D), (ii) Economic Performance (EcoP), (iii) Educational Performance (EduP), (iv) Healthcare performance (HcareP) of the European countries. Our results indicate the importance of R&D to the overall development of the European educational and healthcare performance (directly) and economic performance (indirectly). The results also shows the practical applicability of LOD to estimate this impact.", 
                    "author": "Amrapali Zaveri, Joao Ricardo Nickenig Vissoci, Cinzia Daraio, Ricardo Pietrobon", 
                    "author_latex": "Amrapali Zaveri, Joao Ricardo Nickenig Vissoci, Cinzia Daraio, Ricardo Pietrobon", 
                    "uri": "#Using_Linked_Data_to_evaluate_the_impact_of_Research_and_Development_in_Europe%3A_a_Structural_Equation_Model", 
                    "authors": [
                        {
                            "name": "Amrapali Zaveri", 
                            "uri": "#Amrapali_Zaveri"
                        }, 
                        {
                            "name": "Joao Ricardo Nickenig Vissoci", 
                            "uri": "#Joao_Ricardo_Nickenig_Vissoci"
                        }, 
                        {
                            "name": "Cinzia Daraio", 
                            "uri": "#Cinzia_Daraio"
                        }, 
                        {
                            "name": "Ricardo Pietrobon", 
                            "uri": "#Ricardo_Pietrobon"
                        }
                    ], 
                    "link_local": "82190241-using-linked-data-to-evaluate-the-impact-of-research-and-development-in-europe-a-structural-equation-model.pdf", 
                    "pages": "241-256"
                }
            ], 
            "label": "Semantic Web In Use Track Paper"
        }, 
        {
            "papers": [
                {
                    "page_start": "257", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190257-crowdsourcing-linked-data-quality-assessment.pdf", 
                    "title": "Crowdsourcing Linked Data Quality Assessment", 
                    "abstract": "In this paper we look into the use of crowdsourcing as a means to handle Linked Data quality problems that are challenging to be solved automatically. We analyzed the most common errors encountered in Linked Data sources and classi\ufb01ed them according to the extent to which they are likely to be amenable to a speci\ufb01c form of crowdsourcing. Based on this analysis, we implemented a quality assessment methodology for Linked Data that leverages the wisdom of the crowds in different ways: (i) a contest targeting an expert crowd of researchers and Linked Data enthusiasts; complemented by (ii) paid microtasks published on Amazon Mechanical Turk. We empirically evaluated how this methodology could ef\ufb01ciently spot quality issues in DBpedia. We also investigated how the contributions of the two types of crowds could be optimally integrated into Linked Data curation processes. The results show that the two styles of crowdsourcing are complementary and that crowdsourcing-enabled quality assessment is a promising and affordable way to enhance the quality of Linked Data.", 
                    "author": "Maribel Acosta, Amrapali Zaveri, Elena Simperl, Dimitris Kontokostas, S\u00f6ren Auer, Jens Lehmann", 
                    "author_latex": "Maribel Acosta, Amrapali Zaveri, Elena Simperl, Dimitris Kontokostas, S\\\"oren Auer, Jens Lehmann", 
                    "uri": "#Crowdsourcing_Linked_Data_Quality_Assessment", 
                    "authors": [
                        {
                            "name": "Maribel Acosta", 
                            "uri": "#Maribel_Acosta"
                        }, 
                        {
                            "name": "Amrapali Zaveri", 
                            "uri": "#Amrapali_Zaveri"
                        }, 
                        {
                            "name": "Elena Simperl", 
                            "uri": "#Elena_Simperl"
                        }, 
                        {
                            "name": "Dimitris Kontokostas", 
                            "uri": "#Dimitris_Kontokostas"
                        }, 
                        {
                            "name": "S\u00f6ren Auer", 
                            "uri": "#S%C3%B6ren_Auer"
                        }, 
                        {
                            "name": "Jens Lehmann", 
                            "uri": "#Jens_Lehmann"
                        }
                    ], 
                    "link_local": "82190257-crowdsourcing-linked-data-quality-assessment.pdf", 
                    "pages": "257-272"
                }, 
                {
                    "page_start": "273", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190273-sparql-web-querying-infrastructure-ready-for-action-.pdf", 
                    "title": "SPARQL Web-Querying Infrastructure: Ready for Action?", 
                    "abstract": "Hundreds of public SPARQL endpoints have been deployed on the Web, forming a novel decentralised infrastructure for querying billions of structured facts from a variety of sources on a plethora of topics. But is this infrastructure mature enough to support applications? For 427 public SPARQL endpoints registered on the DataHub, we conduct various experiments to test their maturity. Regarding discoverability, we nd that only one-third of endpoints make descriptive meta-data available, making it dicult to locate or learn about their content and capabilities. Regarding interoperability, we nd patchy support for established SPARQL features like ORDER BY as well as (understandably) for new SPARQL 1.1 features. Regarding eciency, we show that the performance of endpoints for generic queries can vary by up to 34 orders of magnitude. Regarding availability, based on a 27-month long monitoring experiment, we show that only 32.2% of public endpoints can be expected to have (monthly) two-nines uptimes of 99100%.", 
                    "author": "Carlos Buil-Aranda, Aidan Hogan, J\u00fcrgen Umbrich, Pierre-Yves Vandenbussche", 
                    "author_latex": "Carlos Buil-Aranda, Aidan Hogan, J\\\"urgen Umbrich, Pierre-Yves Vandenbussche", 
                    "uri": "#SPARQL_Web-Querying_Infrastructure%3A_Ready_for_Action%3F", 
                    "authors": [
                        {
                            "name": "Carlos Buil-Aranda", 
                            "uri": "#Carlos_Buil-Aranda"
                        }, 
                        {
                            "name": "Aidan Hogan", 
                            "uri": "#Aidan_Hogan"
                        }, 
                        {
                            "name": "J\u00fcrgen Umbrich", 
                            "uri": "#J%C3%BCrgen_Umbrich"
                        }, 
                        {
                            "name": "Pierre-Yves Vandenbussche", 
                            "uri": "#Pierre-Yves_Vandenbussche"
                        }
                    ], 
                    "link_local": "82190273-sparql-web-querying-infrastructure-ready-for-action-.pdf", 
                    "pages": "273-288"
                }, 
                {
                    "page_start": "289", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190289-string-similarity-metrics-for-ontology-alignment.pdf", 
                    "title": "String Similarity Metrics for Ontology Alignment", 
                    "abstract": "Ontology alignment is an important part of enabling the semantic web to reach its full potential. The vast ma jority of ontology alignment systems use one or more string similarity metrics, but often the choice of which metrics to use is not given much attention. In this work we evaluate a wide range of such metrics, along with string pre-processing strategies such as removing stop words and considering synonyms, on di\ufb00erent types of ontologies. We also present a set of guidelines on when to use which metric. We furthermore show that if optimal string similarity metrics are chosen, those alone can produce alignments that are competitive with the state of the art in ontology alignment systems. Finally, we examine the improvements possible to an existing ontology alignment system using an automated string metric selection strategy based upon the characteristics of the ontologies to be aligned.", 
                    "author": "Michelle Cheatham, Pascal Hitzler", 
                    "author_latex": "Michelle Cheatham, Pascal Hitzler", 
                    "uri": "#String_Similarity_Metrics_for_Ontology_Alignment", 
                    "authors": [
                        {
                            "name": "Michelle Cheatham", 
                            "uri": "#Michelle_Cheatham"
                        }, 
                        {
                            "name": "Pascal Hitzler", 
                            "uri": "#Pascal_Hitzler"
                        }
                    ], 
                    "link_local": "82190289-string-similarity-metrics-for-ontology-alignment.pdf", 
                    "pages": "289-304"
                }, 
                {
                    "page_start": "305", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190305-nosql-databases-for-rdf-an-empirical-evaluation.pdf", 
                    "title": "NoSQL Databases for RDF: An Empirical Evaluation", 
                    "abstract": "Processing large volumes of RDF data requires sophisticated tools. In recent years, much e\ufb00ort was spent on optimizing native RDF stores and on repurposing relational query engines for large-scale RDF processing. Concurrently, a number of new data management systems\u2014 regrouped under the NoSQL (for \u201cnot only SQL\u201d) umbrella\u2014rapidly rose to prominence and represent today a popular alternative to classical databases. Though NoSQL systems are increasingly used to manage RDF data, it is still di\ufb03cult to grasp their key advantages and drawbacks in this context. This work is, to the best of our knowledge, the \ufb01rst systematic attempt at characterizing and comparing NoSQL stores for RDF processing. In the following, we describe four di\ufb00erent NoSQL stores and compare their key characteristics when running standard RDF benchmarks on a popular cloud infrastructure using both single-machine and distributed deployments.", 
                    "author": "Philippe Cudr\u00e9-Mauroux, Iliya Enchev, Sever Fundatureanu, Paul Groth, Albert Haque, Andreas Harth, Felix Leif Keppmann, Daniel Miranker, Juan Sequeda, Marcin Wylot", 
                    "author_latex": "Philippe Cudr\\'e-Mauroux, Iliya Enchev, Sever Fundatureanu, Paul Groth, Albert Haque, Andreas Harth, Felix Leif Keppmann, Daniel Miranker, Juan Sequeda, Marcin Wylot", 
                    "uri": "#NoSQL_Databases_for_RDF%3A_An_Empirical_Evaluation", 
                    "authors": [
                        {
                            "name": "Philippe Cudr\u00e9-Mauroux", 
                            "uri": "#Philippe_Cudr%C3%A9-Mauroux"
                        }, 
                        {
                            "name": "Iliya Enchev", 
                            "uri": "#Iliya_Enchev"
                        }, 
                        {
                            "name": "Sever Fundatureanu", 
                            "uri": "#Sever_Fundatureanu"
                        }, 
                        {
                            "name": "Paul Groth", 
                            "uri": "#Paul_Groth"
                        }, 
                        {
                            "name": "Albert Haque", 
                            "uri": "#Albert_Haque"
                        }, 
                        {
                            "name": "Andreas Harth", 
                            "uri": "#Andreas_Harth"
                        }, 
                        {
                            "name": "Felix Leif Keppmann", 
                            "uri": "#Felix_Leif_Keppmann"
                        }, 
                        {
                            "name": "Daniel Miranker", 
                            "uri": "#Daniel_Miranker"
                        }, 
                        {
                            "name": "Juan Sequeda", 
                            "uri": "#Juan_Sequeda"
                        }, 
                        {
                            "name": "Marcin Wylot", 
                            "uri": "#Marcin_Wylot"
                        }
                    ], 
                    "link_local": "82190305-nosql-databases-for-rdf-an-empirical-evaluation.pdf", 
                    "pages": "305-320"
                }, 
                {
                    "page_start": "321", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190321-on-correctness-in-rdf-stream-processor-benchmarking.pdf", 
                    "title": "On Correctness in RDF stream processor benchmarking", 
                    "abstract": "Two complementary benchmarks have been proposed so far for the evaluation and continuous improvement of RDF stream processors: SRBench and LSBench. They put a special focus on di\ufb00erent features of the evaluated systems, including coverage of the streaming extensions of SPARQL supported by each processor, query processing throughput, and an early analysis of query evaluation correctness, based on comparing the results obtained by di\ufb00erent processors for a set of queries. However, none of them has analysed the operational semantics of these processors in order to assess the correctness of query evaluation results. In this paper, we propose a characterization of the operational semantics of RDF stream processors, adapting well-known models used in the stream processing engine community: CQL and SECRET. Through this formalization, we address correctness in RDF stream processor benchmarks, allowing to determine the multiple answers that systems should provide. Finally, we present CSRBench, an extension of SRBench to address query result correctness veri\ufb01cation using an automatic method.", 
                    "author": "Daniele Dell'Aglio, Jean-Paul Calbimonte, Marco Balduini, Oscar Corcho, Emanuele Della Valle", 
                    "author_latex": "Daniele Dell'Aglio, Jean-Paul Calbimonte, Marco Balduini, Oscar Corcho, Emanuele Della Valle", 
                    "uri": "#On_Correctness_in_RDF_stream_processor_benchmarking", 
                    "authors": [
                        {
                            "name": "Daniele Dell'Aglio", 
                            "uri": "#Daniele_Dell%27Aglio"
                        }, 
                        {
                            "name": "Jean-Paul Calbimonte", 
                            "uri": "#Jean-Paul_Calbimonte"
                        }, 
                        {
                            "name": "Marco Balduini", 
                            "uri": "#Marco_Balduini"
                        }, 
                        {
                            "name": "Oscar Corcho", 
                            "uri": "#Oscar_Corcho"
                        }, 
                        {
                            "name": "Emanuele Della Valle", 
                            "uri": "#Emanuele_Della_Valle"
                        }
                    ], 
                    "link_local": "82190321-on-correctness-in-rdf-stream-processor-benchmarking.pdf", 
                    "pages": "321-336"
                }, 
                {
                    "page_start": "337", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190337-geographica-a-benchmark-for-geospatial-rdf-stores.pdf", 
                    "title": "Geographica: A Benchmark for Geospatial RDF Stores", 
                    "abstract": "Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently been de\ufb01ned and corresponding geospatial RDF stores have been implemented. However, there is no widely used benchmark for evaluating geospatial RDF stores which takes into account recent advances to the state of the art in this area. In this paper, we develop a benchmark, called Geographica, which uses both real-world and synthetic data to test the o\ufb00ered functionality and the performance of some prominent geospatial RDF stores.", 
                    "author": "George Garbis, Kostis Kyzirakos, Manolis Koubarakis", 
                    "author_latex": "George Garbis, Kostis Kyzirakos, Manolis Koubarakis", 
                    "uri": "#Geographica%3A_A_Benchmark_for_Geospatial_RDF_Stores", 
                    "authors": [
                        {
                            "name": "George Garbis", 
                            "uri": "#George_Garbis"
                        }, 
                        {
                            "name": "Kostis Kyzirakos", 
                            "uri": "#Kostis_Kyzirakos"
                        }, 
                        {
                            "name": "Manolis Koubarakis", 
                            "uri": "#Manolis_Koubarakis"
                        }
                    ], 
                    "link_local": "82190337-geographica-a-benchmark-for-geospatial-rdf-stores.pdf", 
                    "pages": "337-352"
                }, 
                {
                    "page_start": "353", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190353-introducing-statistical-design-of-experiments-to-sparql-endpoint-evaluation.pdf", 
                    "title": "Introducing Statistical Design of Experiments to SPARQL Endpoint Evaluation", 
                    "abstract": "This paper argues that the common practice of benchmarking is inadequate as a scienti\ufb01c evaluation methodology. It further attempts to introduce the empirical tradition of the physical sciences by using techniques from Statistical Design of Experiments applied to the example of SPARQL endpoint performance evaluation. It does so by studying full as well as fractional factorial experiments designed to evaluate an assertion that some change introduced in a system has improved performance. This paper does not present a \ufb01nished experimental design, rather its main focus is didactical, to shift the focus of the community away from benchmarking towards higher scienti\ufb01c rigor.", 
                    "author": "Kjetil Kjernsmo, John S. Tyssedal", 
                    "author_latex": "Kjetil Kjernsmo, John S. Tyssedal", 
                    "uri": "#Introducing_Statistical_Design_of_Experiments_to_SPARQL_Endpoint_Evaluation", 
                    "authors": [
                        {
                            "name": "Kjetil Kjernsmo", 
                            "uri": "#Kjetil_Kjernsmo"
                        }, 
                        {
                            "name": "John S. Tyssedal", 
                            "uri": "#John_S._Tyssedal"
                        }
                    ], 
                    "link_local": "82190353-introducing-statistical-design-of-experiments-to-sparql-endpoint-evaluation.pdf", 
                    "pages": "353-368"
                }, 
                {
                    "page_start": "369", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190369-towards-a-systematic-benchmarking-of-ontology-based-query-rewriting-systems.pdf", 
                    "title": "Towards a systematic benchmarking of ontology-based query rewriting systems", 
                    "abstract": "Query rewriting is one of the fundamental steps in ontologybased data access (OBDA) approaches. It takes as inputs an ontology and a query written according to that ontology, and produces as an output a set of queries that should be evaluated to account for the inferences that should be considered for that query and ontology. Di\ufb00erent query rewriting systems give support to di\ufb00erent ontology languages with varying expressiveness, and the rewritten queries obtained as an output do also vary in expressiveness. This heterogeneity has traditionally made it di\ufb03cult to compare di\ufb00erent approaches, and the area lacks in general commonly agreed benchmarks that could be used not only for such comparisons but also for improving OBDA support. In this paper we compile data, dimensions and measurements that have been used to evaluate some of the most recent systems, we analyse and characterise these assets, and provide a uni\ufb01ed set of them that could be used as a starting point towards a more systematic benchmarking process for such systems. Finally, we apply this initial benchmark with some of the most relevant OBDA approaches in the state of the art.", 
                    "author": "Jose Mora, Oscar Corcho", 
                    "author_latex": "Jose Mora, Oscar Corcho", 
                    "uri": "#Towards_a_systematic_benchmarking_of_ontology-based_query_rewriting_systems", 
                    "authors": [
                        {
                            "name": "Jose Mora", 
                            "uri": "#Jose_Mora"
                        }, 
                        {
                            "name": "Oscar Corcho", 
                            "uri": "#Oscar_Corcho"
                        }
                    ], 
                    "link_local": "82190369-towards-a-systematic-benchmarking-of-ontology-based-query-rewriting-systems.pdf", 
                    "pages": "369-384"
                }, 
                {
                    "page_start": "385", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190385-evaluation-measures-for-ontology-matchers-in-supervised-matching-scenarios.pdf", 
                    "title": "Evaluation measures for ontology matchers in supervised matching scenarios", 
                    "abstract": "Precision and Recall, as well as their combination in terms of F-Measure, are widely used measures in computer science and generally applied to evaluate the overall performance of ontology matchers in fully automatic, unsupervised scenarios. In this paper, we investigate the case of supervised matching, where automatically created ontology alignments are veri\ufb01ed by an expert. We motivate and describe this use case and its characteristics and discuss why traditional, F-measure based evaluation measures are not suitable for this use case. Therefore, we investigate several alternative evaluation measures and propose the use of Precision@N curves as a mean to assess different matching systems for supervised matching. We compare the ranking of several state of the art matchers using Precision@N curves to the traditional F-measure based ranking, and discuss means to combine matchers in a way that optimizes the user support in supervised ontology matching.", 
                    "author": "Dominique Ritze, Heiko Paulheim, Kai Eckert", 
                    "author_latex": "Dominique Ritze, Heiko Paulheim, Kai Eckert", 
                    "uri": "#Evaluation_measures_for_ontology_matchers_in_supervised_matching_scenarios", 
                    "authors": [
                        {
                            "name": "Dominique Ritze", 
                            "uri": "#Dominique_Ritze"
                        }, 
                        {
                            "name": "Heiko Paulheim", 
                            "uri": "#Heiko_Paulheim"
                        }, 
                        {
                            "name": "Kai Eckert", 
                            "uri": "#Kai_Eckert"
                        }
                    ], 
                    "link_local": "82190385-evaluation-measures-for-ontology-matchers-in-supervised-matching-scenarios.pdf", 
                    "pages": "385-400"
                }, 
                {
                    "page_start": "401", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190401-evaluating-and-benchmarking-sparql-query-containment-solvers.pdf", 
                    "title": "Evaluating and benchmarking SPARQL query containment solvers", 
                    "abstract": "Query containment is the problem of deciding if the answers to a query are included in those of another query for any queried database. This problem is very important for query optimization purposes. In the SPARQL context, it can be equally useful. This problem has recently been investigated theoretically and some query containment solvers are available. Yet, there were no benchmarks to compare theses systems and foster their improvement. In order to experimentally assess implementation strengths and limitations, we provide a rst SPARQL containment test benchmark. It has been designed with respect to both the capabilities of existing solvers and the study of typical queries. Some solvers support optional constructs and cycles, while other solvers support pro jection, union of conjunctive queries and RDF Schemas. No solver currently supports all these features or OWL entailment regimes. The study of query demographics on DBPedia logs shows that the vast ma jority of queries are acyclic and a signicant part of them uses UNION or pro jection. We thus test available solvers on their domain of applicability on three dierent benchmark suites. These experiments show that (i) tested solutions are overall functionally correct, (ii) in spite of its complexity, SPARQL query containment is practicable for acyclic queries, (iii) state-of-the-art solvers are at an early stage both in terms of capability and implementation.", 
                    "author": "Melisachew Wudage Chekol, J\u00e9r\u00f4me Euzenat, Pierre Genev\u00e8s, Nabil Laya\u00efda", 
                    "author_latex": "Melisachew Wudage Chekol, J\\'er\\^ome Euzenat, Pierre Genev\\`es, Nabil Laya\\\"\\ida", 
                    "uri": "#Evaluating_and_benchmarking_SPARQL_query_containment_solvers", 
                    "authors": [
                        {
                            "name": "Melisachew Wudage Chekol", 
                            "uri": "#Melisachew_Wudage_Chekol"
                        }, 
                        {
                            "name": "J\u00e9r\u00f4me Euzenat", 
                            "uri": "#J%C3%A9r%C3%B4me_Euzenat"
                        }, 
                        {
                            "name": "Pierre Genev\u00e8s", 
                            "uri": "#Pierre_Genev%C3%A8s"
                        }, 
                        {
                            "name": "Nabil Laya\u00efda", 
                            "uri": "#Nabil_Laya%C3%AFda"
                        }
                    ], 
                    "link_local": "82190401-evaluating-and-benchmarking-sparql-query-containment-solvers.pdf", 
                    "pages": "401-416"
                }
            ], 
            "label": "Evaluation Track Paper"
        }, 
        {
            "papers": [
                {
                    "page_start": "417", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190417-assessing-content-value-for-digital-publishing-through-relevance-and-provenance-based-trust.pdf", 
                    "title": "Assessing Content Value for Digital Publishing through Relevance and Provenance-based Trust", 
                    "abstract": "Due to the abundance of content on the Web, content authors and publishers have a pressing need for systems that select content that is valuable for them, is trustworthy and is related to their own work. Additionally, the value of their own work needs to be assessed before it is published, to guarantee high value for the consumer. In this doctoral research, we investigate how to use Semantic Web technologies to automatically assess the value of content that is \u2013 or is about to be \u2013 digitally published. To achieve this, we propose methods to assess the relevance of content to existing publications, retrieve or reconstruct its provenance, and derive a trust assessment from this provenance. We discuss our evaluation methods, and present some preliminary results.", 
                    "author": "Tom De Nies", 
                    "author_latex": "Tom De Nies", 
                    "uri": "#Assessing_Content_Value_for_Digital_Publishing_through_Relevance_and_Provenance-based_Trust", 
                    "authors": [
                        {
                            "name": "Tom De Nies", 
                            "uri": "#Tom_De_Nies"
                        }
                    ], 
                    "link_local": "82190417-assessing-content-value-for-digital-publishing-through-relevance-and-provenance-based-trust.pdf", 
                    "pages": "417-424"
                }, 
                {
                    "page_start": "425", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190425-the-effects-of-licensing-on-open-data-computing-a-measure-of-health-for-our-scholarly-record.pdf", 
                    "title": "The effects of Licensing on Open Data: Computing a measure of health for our Scholarly Record", 
                    "abstract": "As data collections become established in key disciplines, some of the longstanding barriers to data sharing become to dissolve; yet others remain. While metadata and ontologies help overcome the problems of finding and interpreting data, the lack of clarity over licensing remains a real impediment to data reuse. Freedom from legal restriction and uncertainty is essential for the effective sharing, combining and deriving of data from these distributed collections. Reuse and recombination of data will be greatly facilitated by expanding the definition of the semantic web to include the semantics of data licensing. We aim to express licensing terms in a computable manner, within the context of research practice, enabling us to infer the resulting state of rights, obligations and conditions that are inherited by derived and recombined datasets, using a mixed bag of licenses. Building off this we aim to simulate the effects of varying licensing practices within communities, proposing a measure of health of our scholarly record based on compatibility and restrictiveness of the licenses contained therein.", 
                    "author": "Richard Hosking, Mark Gahegan", 
                    "author_latex": "Richard Hosking, Mark Gahegan", 
                    "uri": "#The_effects_of_Licensing_on_Open_Data%3A_Computing_a_measure_of_health_for_our_Scholarly_Record", 
                    "authors": [
                        {
                            "name": "Richard Hosking", 
                            "uri": "#Richard_Hosking"
                        }, 
                        {
                            "name": "Mark Gahegan", 
                            "uri": "#Mark_Gahegan"
                        }
                    ], 
                    "link_local": "82190425-the-effects-of-licensing-on-open-data-computing-a-measure-of-health-for-our-scholarly-record.pdf", 
                    "pages": "425-432"
                }, 
                {
                    "page_start": "433", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190433-utilising-provenance-to-enhance-social-computation.pdf", 
                    "title": "Utilising Provenance to Enhance Social Computation", 
                    "abstract": "Many online platforms employ networks of human workers to perform computational tasks that can be di\ufb03cult for a machine (e.g. reporting travel disruption). Such systems have to make a range of decisions, for example, selection of suitable workers for a task. In this paper we present an approach that utilises Semantic Web technologies and provenance to support such decision-making processes.", 
                    "author": "Milan Markovic", 
                    "author_latex": "Milan Markovic", 
                    "uri": "#Utilising_Provenance_to_Enhance_Social_Computation", 
                    "authors": [
                        {
                            "name": "Milan Markovic", 
                            "uri": "#Milan_Markovic"
                        }
                    ], 
                    "link_local": "82190433-utilising-provenance-to-enhance-social-computation.pdf", 
                    "pages": "433-440"
                }, 
                {
                    "page_start": "441", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190441-crowdsourcing-ontology-verification.pdf", 
                    "title": "Crowdsourcing Ontology Verification", 
                    "abstract": "As the scale and complexity of ontologies increases, so too do errors and engineering challenges. It is frequently unclear, however, to what degree extralogical ontology errors negatively affect the application that the ontology underpins. For example, \u201cShoe SubClassOf Foot\u201d may be correct logically, but not in a human interpretation. Indeed, such errors, not caught by reasoning, are likely to be domain-speci\ufb01c, and thus identifying salient ontology errors requires consideration of the domain. There are both automated and manual methods that provide ontology quality assurance. Nevertheless, these methods do not readily scale as ontology size increases, and do not necessarily identify the most salient extralogical errors. Recently, crowdsourcing has enabled solutions to complex problems that computers alone cannot solve. For instance, human workers can quickly and more accurately identify objects in images at scale. Crowdsourcing presents an opportunity to develop methods for ontology quality assurance that overcome the current limitations of scalability and applicability. In this work, I aim (1) to determine the effect of extralogical ontology errors in an example domain, (2) to develop a scalable framework for crowdsourcing ontology veri\ufb01cation that overcomes current ontology Q/A method limitations, and (3) to apply this framework to ontologies in use. I will then evaluate the method itself and also its effect in the context of a speci\ufb01c domain. As an example domain, I will use biomedicine, which applies many large-scale ontologies. Thus, this work will enable scalable quality assurance for extralogical errors in biomedical ontologies.", 
                    "author": "Jonathan Mortensen", 
                    "author_latex": "Jonathan Mortensen", 
                    "uri": "#Crowdsourcing_Ontology_Verification", 
                    "authors": [
                        {
                            "name": "Jonathan Mortensen", 
                            "uri": "#Jonathan_Mortensen"
                        }
                    ], 
                    "link_local": "82190441-crowdsourcing-ontology-verification.pdf", 
                    "pages": "441-448"
                }, 
                {
                    "page_start": "449", 
                    "link_open_access": "https://github.com/lidingpku/iswc-archive/raw/master/paper/iswc-2013/82190449-interactive-pay-as-you-go-relational-to-ontology-mapping.pdf", 
                    "title": "Interactive Pay as you go Relational-to-Ontology Mapping", 
                    "abstract": "Ontology Based Data Access (OBDA) enables access to relational data with a complex structure through ontologies as conceptual domain models. To this end, mappings are required. A key aim of OBDA is to facilitate access to data with a complex structure. Ironically, though, in today\u2019s existing OBDA systems mappings typically need to be compiled by hand, which is a complex and labor intensive task. Additionally, existing semi-automatic mapping approaches su\ufb00er from high human e\ufb00ort for cleaning up results. Fully automatic approaches, on the other side, su\ufb00er from a lack of precision and/or recall. In setups where the correctness of query results is crucial but the initial human e\ufb00ort must still be kept be small as possible, neither approach is acceptable. This situation calls for a guided, pay as you go feedback process for human mapping validation. We envision a comprehensive suite of methods and techniques that work well with one another in a seamless mapping process and support mapping construction in the context of OBDA. This suite will in part consist of a recombination and adaptation of various existing methods, but will also comprise newly devised algorithms and techniques.", 
                    "author": "Christoph Pinkel", 
                    "author_latex": "Christoph Pinkel", 
                    "uri": "#Interactive_Pay_as_you_go_Relational-to-Ontology_Mapping", 
                    "authors": [
                        {
                            "name": "Christoph Pinkel", 
                            "uri": "#Christoph_Pinkel"
                        }
                    ], 
                    "link_local": "82190449-interactive-pay-as-you-go-relational-to-ontology-mapping.pdf", 
                    "pages": "449-456"
                }
            ], 
            "label": "Doctoral Consortium Paper"
        }
    ]
}